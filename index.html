<!DOCTYPE html>
<html lang="zh-CN,en,default">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 7.0.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"humble2967738843.github.io","root":"/","scheme":"Gemini","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":true},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta property="og:type" content="website">
<meta property="og:title" content="院龙">
<meta property="og:url" content="http://humble2967738843.github.io/index.html">
<meta property="og:site_name" content="院龙">
<meta property="og:locale" content="zh_CN">
<meta property="article:author" content="院龙">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="http://humble2967738843.github.io/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : true,
    isPost : false,
    lang   : 'zh-CN'
  };
</script>

  <title>院龙</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

<!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">
<!-- hexo injector head_end end --><link rel="alternate" href="/atom.xml" title="院龙" type="application/atom+xml">
</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">院龙</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content index posts-expand">
            
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN,en,default">
    <link itemprop="mainEntityOfPage" href="http://humble2967738843.github.io/2024/11/30/gbdt/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="院龙">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="院龙">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2024/11/30/gbdt/" class="post-title-link" itemprop="url">GBDT</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2024-11-30 15:45:56 / 修改时间：17:25:41" itemprop="dateCreated datePublished" datetime="2024-11-30T15:45:56+08:00">2024-11-30</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>​        GBDT为梯度提升决策树（Gradient  Boosting Decision Tree），是一种以回归决策树为 弱学习器的集成学习模型。GBDT集成学习模型通常使用CART决 策树（回归树）模型作为弱学习器。</p>
<p>​        提升树算法(Boosting Decision Tree)：</p>
<ul>
<li>提升树是迭代多棵回归树来共同决策。当采用平方 误差损失函数时，每一棵回归树学习的是之前所有 树的结论和残差，拟合得到一个当前的残差回归树。</li>
<li>残差 = 真实值- 预测值。</li>
<li>提升树即是整个迭代过程生成的回归树的累加。</li>
</ul>
<p>例：训练一个提升树模型来预测年龄 训练集只有4个人，A,B,C,D，他们的年龄分别是 14,16,24,26。其中A、B分别是高一和高三学生；C,D 分别是应届毕业生和工作两年的员工。样本中有购物 金额、上网时长、经常到百度知道提问等特征。</p>
<p><img src="https://s2.loli.net/2024/11/30/YNhABjfvzogsKdy.png" alt="image-20241130163828685"></p>
<p>提升树过程如下：由于数据太少，限定叶子节点最多有两个，即每棵树 都只有一个分枝，并且限定只学习两棵树。第一棵树，由于A,B年龄较为 相近，C,D年龄较为相近，4人被分为两拨，每拨用<strong>平均年龄作为预测值</strong> 。<strong>拿残差替代A,B,C,D的原值</strong>，到第二棵树去学习，如果预测值和它们的残差相等，则只需把第二棵树的结论累加到第一棵树上就能得到真实年龄了。第二棵树只有两个值1和-1，直接分成两个节点。此时所有人的残 差都是0，即每个人都得到了真实的预测值。</p>
<p><img src="https://s2.loli.net/2024/11/30/fQz65S8XenPcL2Z.png" alt="image-20241130163856863"></p>
<p>现在A,B,C,D的预测值都和真实年龄一致</p>
<p>A: 14岁高一学生，购物较少，经常问学长问题； 预测年龄A = 15 – 1 = 14 </p>
<p>B: 16岁高三学生；购物较少，经常被学弟问问题； 预测年龄B = 15 + 1 = 16 </p>
<p>C: 24岁应届毕业生；购物较多，经常问师兄问题； 预测年龄C = 25 – 1 = 24 </p>
<p>D: 26岁工作两年员工；购物较多，经常被师弟问问题； 预测年龄D = 25 + 1 = 26</p>
<p><img src="https://s2.loli.net/2024/11/30/B6JROQlmMohCjTw.png" alt="image-20241130163925661"></p>
<p>从例子很直观看到，预测值等于所有树值得累加，如A的预测值 = 树1左节点值(15)+树2左节点(-1)=14。</p>
<p>因此，给定当前模型fm-1(x)，只需要简单的拟合当前模型的残差。 累加每棵回归树的结论，得出最终的预测值.现将回归问题的提升 树算法叙述如下：</p>
<p><img src="https://s2.loli.net/2024/11/30/3MygRhSbrj12QVs.png" alt="image-20241130163946424"></p>
<p><strong>梯度的大小反映了当前预测值与目标值之间的距离。因 此，除第一棵决策树使用原始预测指标建树，之后的每一棵 决策树都用前一棵决策树的预测值与目标值计算出来的负梯 度来建树。相当于给分错的样本加权多次分类，使样本最终 残差趋近于0。</strong></p>
<p>由于是对目标残差或增量进行建模预测，因此 <strong>GBDT模型只需把过程中每一棵决策树的输出结果累加，便 可得到最终的预测输出。</strong></p>
<p>设某个回归任务的训练样本数据集为$D={(X_1, y_1), (X_2, y_2),…,(X_n, y_n)}$,根据样本集$D$构造第一 个弱学习器的初始回归决策树$L_0$，对于$D$中任意给定的一个训练样本$X$，决策树$L_0$对$X$的预测输出与其标记值$y$之间的误差为：</p>
<script type="math/tex; mode=display">
e(L_0(X))=\frac{1}{2}[L_0(X)-y]^2</script><p>使用上述函数作为优化的目标函数改进模型$L_0$，使用梯度下 降法实现对上述优化问题的求解，则对上式求导可求得如下梯度</p>
<script type="math/tex; mode=display">
\frac{\partial e(L_0(X))}{\partial L_0(X)}=L_0(X)-y</script><p>梯度的反方向为$y-L_0(X)$，应对模型$L_0(X)$往该方向进行调整。</p>
<p>由于模型$L_0(X)$ 的更新方向$y-L_0(x)$为训练样本标 记值与该模型预测结果之差，即模型$L_0(X)$的的预测误差 ，可构造一个新的模型$L_1(X)$进行拟合。</p>
<p>在对样本$X$进行预测时，由于$L_1$对于$X$的输出是对$L_0$的输出的某个校正量，且校正方向一定是误差$e$减小的 方向，故这两个模型的输出之和$L_0(X)+L_1(X)$一定比$L_0(X)$更加接近样本真实值$y$。</p>
<p>GBDT集成学习算法正是根据上述思路通过迭代方 式逐步构造多个弱学习器，根据训练样本数据集$D$构造 一个新的数据集$T_1$，并使用$T_1$1构造一个新的回归决策树模型$L_1(X)$作为GBDT，集成学习模型的一个新增弱学习器。$T_1$的具体形式如下：</p>
<script type="math/tex; mode=display">
T_1={(X_1, \nabla_1), (X_2, \nabla_2),...,(X_n, \nabla_n)}</script><p>其中$\nabla_i = y_i-L_0(X_i)$</p>
<p>GBDT集成学习算法的基本步骤如下：</p>
<p>（1）构造初始学习器$L^0(X)$。令$t=0$，根据下式构建初始回归树$L^0(X)=L_0(X)$</p>
<script type="math/tex; mode=display">
L_0(X)=arg_{c}min\sum_{X_i, y_i\in D}J(y_i,c)</script><p>其中$L_0(X)$为只有一个根节点的初始回归决策树，$c$为使得目标 函数最小化的模型参数，$J(y_i,c)$为损失函数。</p>
<p>这里采用平方误差损失函数，即有：</p>
<script type="math/tex; mode=display">
J(y,g(x))=\frac{1}{2}(y-g(x))^2</script><p>其中$y$为为样本真实值或标注值，$g(x)$为单个回归决策树模型的预测。</p>
<p>（2）令$t=t+1$，并计算数据集$D$中每个训练样本的负梯度$\nabla_i$：</p>
<script type="math/tex; mode=display">
\nabla_i=-[\frac{\partial J(y,L(X_i)}{\partial L(X_i)}]_{L(X)=L^t(X)}</script><p>（3）<strong>构建新的训练样本集$T_t$</strong>：</p>
<script type="math/tex; mode=display">
T_1={(X_1, \nabla_1), (X_2, \nabla_2),...,(X_n, \nabla_n)}</script><p>使用$T_t$作为训练样本集构造一棵回归树，并使用该回归树作为第$t+1$个弱学习器$L_t(X)$，该决策树中第$j$个叶子的输出值为：</p>
<script type="math/tex; mode=display">
C_{t,j}=arg_{c} min\sum_{(X_i,\nabla_i)\in T_{t}^j}J(y_i,L^t(X_i)+c)</script><p>其中$T_t^j$表示第$t+1$个弱学习器的第$j$个叶子节点所对应 的数据集合。</p>
<p>上式表明弱学习器$L_t(X)$中每个叶节点的输出均使得上轮迭代所得集成模型$L^(t-1)(X)$的预测误差达到最小</p>
<p>可将回归决策树$L_t(X)$表示为：</p>
<script type="math/tex; mode=display">
L_t(X)=\sum C_{t,j}[(X_i,\nabla_i)\in T_{t}^j]</script><p>其中：</p>
<script type="math/tex; mode=display">
I[(X_i, \nabla_i)\in T_{t}^j]=\left\{  
             \begin{array}{**lr**}  
             1, & (X_i,\nabla_i)\in T_t^j\\  
             0, & (X_i,\nabla_i)\notin T_t^j
             \end{array}  
\right.</script><p>（4）更新集成模型为：</p>
<script type="math/tex; mode=display">
L^t(X)=L^(t-1)(X)+L_t(X)</script><p>若未满足算法终止条件，则返回步骤（2），否 则算法结束。</p>
<p>【例题】现有某个公司四位员工的考评信息即月薪如表5-9 所示，试根据该数据集和GBDT学习算法构造包含两个个体学 习器的集成模型，并使用该集成模型预测工龄为25年，绩效得 分为65分的员工的月薪。</p>
<p><img src="https://s2.loli.net/2024/11/30/suWvA476q1gXEbB.png" alt="image-20241130172531954"></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN,en,default">
    <link itemprop="mainEntityOfPage" href="http://humble2967738843.github.io/2024/05/06/ji-yu-lian-shi-tui-li-de-wen-dang-ji-shi-jian-lun-yuan-ti-qu/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="院龙">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="院龙">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2024/05/06/ji-yu-lian-shi-tui-li-de-wen-dang-ji-shi-jian-lun-yuan-ti-qu/" class="post-title-link" itemprop="url">基于链式推理的文档级事件论元提取</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2024-05-06 12:21:08" itemprop="dateCreated datePublished" datetime="2024-05-06T12:21:08+08:00">2024-05-06</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2024-05-26 19:14:12" itemprop="dateModified" datetime="2024-05-26T19:14:12+08:00">2024-05-26</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E4%BA%8B%E4%BB%B6%E5%8F%82%E6%95%B0%E6%8A%BD%E5%8F%96/" itemprop="url" rel="index"><span itemprop="name">事件参数抽取</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>Document-Level Event Argument Extraction With A Chain Reasoning Paradigm</p>
<p>文档级事件参数提取旨在识别句子级别之外的事件参数，其中一个重大挑战是对远程依赖关系进行建模。针对这一挑战，我们为该任务<span style="background-color: #ff666680">提出了一种新的链式推理范式，它可以生成可分解的一阶逻辑规则进行推理。由于链的组合性质，这种范式自然地捕获了远程相互依赖，这也通过显式地建模推理过程来提高可解释性</span>。我们<span style="background-color: #ff666680">引入 T 范数模糊逻辑进行优化，它允许端到端学习，并有望将逻辑推理的表达能力与神经网络的泛化相结合</span>。在实验中，我们表明我们的方法在两个标准基准上明显优于以前的方法（F1 中超过 6 个点）。此外，它在资源匮乏的情况下具有数据效率，并且足够强大以防御对抗性攻击。</p>
<p>1.</p>
<p>识别事件参数（即事件的参与者）是文档级事件理解的一项关键任务（Ebner et al., 2020；Li et al., 2021）。在此任务中，主要挑战是对事件触发器和参数之间的远程依赖关系进行建模，因为事件表达式可以跨越多个句子（Ebner 等人，2020；Liu 等人，2021；Li 等人，2021）。考虑图 1 中由触发引爆（类型 = 攻击）表示的事件。为了定位其参数 Tartus（语义角色 = 地点），模型应捕获包含三个句子和 178 个单词的大型上下文窗口，以支持推理过程。</p>
<p>目前，有效捕获此类依赖关系仍然是一个悬而未决的问题（Liu et al., 2021, 2022c）。先前的研究提出建模通过结合分层编码机制（Du and Cardie，2020a）、生成范式（Li et al.，2021；Ma et al.，2022；Du et al.，2022）和文档级归纳偏差（Wei等人，2021；Pouran Ben Veyseh 等人，2022；Liu 等人，2022b)。然而，此类方法并未明确表征文档上下文背后的推理模式，这可能会导致性能不佳。此外，大多数以前的方法都是不可解释的，因为它们依赖于黑盒神经网络。</p>
<p><img src="attachments/5WFMU28T.png" alt="\&lt;img alt=&quot;&quot; data-attachment-key=&quot;5WFMU28T&quot; width=&quot;676&quot; height=&quot;697&quot; src=&quot;attachments/5WFMU28T.png&quot; ztype=&quot;zimage&quot;&gt;"></p>
<p>在本文中，我们提出了一种新的推理链范式来解决文档级事件参数提取（EAE）。如图 1 底部所示，<span style="background-color: #ff666680">我们的方法试图通过一系列局部推理步骤来描述全局参数查找过程。</span>例如，我们可以使用以下链来定位塔尔图斯：<span style="background-color: #ff666680">引爆目标→Arzunah Bridge locatedIn→塔尔图斯。</span>与以前的方法相比，这种推理链范式具有三个明显的好处：<span style="background-color: #ff666680">首先，由于推理链的组合结构，它自然地捕获了长距离依赖关系</span>。其次，<span style="background-color: #ff666680">它只涉及局部推理，这在概念上比直接执行全局推理更容易</span>。第三，<span style="background-color: #ff666680">它提高了可解释性，因为推理过程是可见的</span>。</p>
<p>我们的方法将推理链形式化为一阶逻辑（FOL）规则（Cresswell 和 Hughes，1996）。具体来说，<span style="background-color: #ff666680">令 RL(T , ?) 为对事件参数的查询，该事件参数满足关于事件触发器 T 的语义角色 RL （例如，地点）</span>。我们将查询形式化为以下 FOL 规则：</p>
<p><img src="attachments/T4XFLP62.png" alt="\&lt;img alt=&quot;&quot; data-attachment-key=&quot;T4XFLP62&quot; width=&quot;513&quot; height=&quot;69&quot; src=&quot;attachments/T4XFLP62.png&quot; ztype=&quot;zimage&quot;&gt;"></p>
<p>其中<span style="background-color: #ff666680">规则主体（右侧）由具有低级谓词 {ri}1n 和中间线索实体 {Bi}n−1 1 的合取命题组成</span>。我们<span style="background-color: #ff666680">构建一个模型，根据文档上下文自动生成规则，然后将规则转换为推理链来定位事件参数</span>。然而，由于 FOL 规则的离散性质，使用 FOL 规则进行优化通常具有挑战性（Qu 等人，2021a）。受到使用 FOL 增强神经网络的工作的启发（Li 和 Srikumar，2019；Ahmed 等人，2022），我们<span style="background-color: #ff666680">提出了用于松弛的 T-Norm 模糊逻辑（Hajek，1998），这导致了端到端的训练机制。</span></p>
<p>我们在两个基准上验证了我们的方法的有效性（Ebner 等人，2020；Li 等人，2021）。根据结果​​，我们的方法通过这种链式推理范式提供了有希望的结果，例如与使用大规模外部资源训练的模型相比，F1 提高了 6 个点（第 6.1 节）。有趣的是，除了性能提升之外，我们的方法还表现出良好的鲁棒性，特别是在资源匮乏的情况下和防御对抗性噪音（第 7.2 节）。最后，我们使用彻底的案例研究来评估我们方法的可解释性（第 7.3 节）。</p>
<p>2 相关工作</p>
<p><strong>使用 FOL 规则进行推理</strong>。一阶逻辑（FOL）规则可以对声明性知识进行编码，并在符号推理中发挥至关重要的作用（Cresswell 和 Hughes，1996）。在深度学习时代，多项研究探讨了 <span style="background-color: #2ea8e580">FOL 规则与神经网络的集成以进行推理（称为神经符号方法），以及在知识库推理（Qu 等人，2021b）、文本蕴涵（Li 和 Srikumar）中的应用，2019）、问答（Wang 和 Pan，2022）以及其他（Medina 等人，2021；Ahmed 等人，2022）</span>。我们的方法<span style="background-color: #ff666680">受到知识库推理工作的启发，据我们所知，这是在文档级 EAE 背景下合并 FOL 规则进行推理的首次尝试</span>。与其他方法相比，我们研究了使用神经网络自动生成规则，而不是像（Li 和 Srikumar，2019；Wang 和 Pan，2022）那样采用专家编写的规则。此外，与基于强化学习的方法不同（Qu et al., 2021b），我们<span style="background-color: #ff666680">使用 T 范数进行规则松弛，从而形成具有更稳定学习过程的端到端训练范例。</span></p>
<p>3 方法</p>
<p>图 2 概述了我们的方法，并提供了一个提取事件引爆的 Place 角色参数的示例。令 D = {w1, · · · , T, · · · , wN } 为包含 N 个单词和事件触发器 T 的文档，并令 RL(T , ?) 为对语义角色 RL 的事件参数的查询。我们的方法不是直接执行可能涉及高级过程的推理，而是将查询表示为具有连接命题和低级谓词的 FOL 规则{ri}1n：</p>
<p><img src="attachments/EALEF5M3.png" alt="\&lt;img alt=&quot;&quot; data-attachment-key=&quot;EALEF5M3&quot; width=&quot;585&quot; height=&quot;67&quot; src=&quot;attachments/EALEF5M3.png&quot; ztype=&quot;zimage&quot;&gt;"></p>
<p>这样，规则的主体就暗示了一个推理链：T r1 → B1 r2 → · · · Bn−1 rn → ?。我们使用双谓词公式，特别是RL(T, ?) ← r1(T, B) ∧ r2(B, ?)为了解释我们的方法，我们在第 4 节中描述了一般情况。</p>
<p><img src="attachments/3Y9VQ2IL.png" alt="\&lt;img alt=&quot;&quot; data-attachment-key=&quot;3Y9VQ2IL&quot; width=&quot;1379&quot; height=&quot;627&quot; src=&quot;attachments/3Y9VQ2IL.png&quot; ztype=&quot;zimage&quot;&gt;"></p>
<p>3.1</p>
<p>在我们方法的第一步中，我们创建一组实体，可以从中选择一个实体作为中间线索实体以形成推理链（关于我们的双谓词结构）。<span style="background-color: #ff666680">我们扩大了“实体”的概念，以包含文档中的任何单个单词，以合并基于动词的提示</span>。<span style="background-color: #ff666680">为了限制集合的大小，我们给每个单词一个分数源自 BERT 表示</span>（Devlin 等人，2019）。例如，wi 的得分为：</p>
<p><img src="attachments/46BYK6KL.png" alt="\&lt;img alt=&quot;&quot; data-attachment-key=&quot;46BYK6KL&quot; width=&quot;562&quot; height=&quot;106&quot; src=&quot;attachments/46BYK6KL.png&quot; ztype=&quot;zimage&quot;&gt;"></p>
<p>其中h<sub>wi</sub>是wi的表示，w<sub>s</sub>和b<sub>s</sub>是模型参数。我们根据分数对所有单词进行排序，并选择分数最高的 K 个单词组成集合，表示为 B = {b<sub>i</sub>}<sub>i</sub><sup>K=1</sup>。</p>
<p>为了促进训练和测试，我们还生成了一个参数候选集。在这种情况下，<span style="background-color: #2ea8e580">我们不使用实体的广义定义，因为事件参数被定义为名词实体</span>（Walker and Consortium，2005；Ahn，2006）。当真实实体可用时（例如在 WikiEvents (Li et al., 2021) 中），我们将候选集视为真实实体集；否则，我们使用外部工具包2来识别实体。我们用 A = {ai}iL=1 表示参数候选集。</p>
<p>3.2</p>
<p><span style="background-color: #ff666680">给定实体候选集B和参数候选集A，下一步是生成两个谓词并选择集合中的相关候选者以形成规则。</span>在这里，我们解释了生成关于特定实体参数对（B ∈ B，A ∈ A）的谓词的方法，并且我们在第 4 节中展示了对不同候选对生成的规则进行排名的度量。</p>
<p><strong>谓词表示。</strong>在我们的方法中，我们假设有 M 个原子谓词不可分解的语义，由谓词集 R = {R<sub>i</sub>}<sup>M</sup><sub>i=1</sub> 表示。我们给每个谓词一个 d 维向量化表示，并导出 R 的矩阵表示 U ∈ RM×d。对于语义角色 RL，我们还给它一个 d 维表示，由 r<sub>RL</sub> ∈ Rd 表示。</p>
<p><strong>学习角色-谓词关联</strong>。给定这些表示，我们首先学习角色-顶级谓词关联，<span style="background-color: #ff666680">该关联指示哪些谓词可能仅基于角色而不考虑上下文而生成</span>。我们采用自回归学习并生成概率向量 a(1) RL ∈ RM 指示第一个谓词 r1 在谓词集 R 上的分布：</p>
<p><img src="attachments/2DJDB86S.png" alt="\&lt;img alt=&quot;&quot; data-attachment-key=&quot;2DJDB86S&quot; width=&quot;487&quot; height=&quot;73&quot; src=&quot;attachments/2DJDB86S.png&quot; ztype=&quot;zimage&quot;&gt;"></p>
<p>其中 W<sup> (1) </sup><sub>s</sub> ∈ R<sup>d×d</sup> 是一个参数。为了了解第二个谓词 r2 的分布，我们首先通过整合第一个谓词的影响来更新角色的表示：</p>
<p><img src="attachments/WLCE8QIE.png" alt="\&lt;img alt=&quot;&quot; data-attachment-key=&quot;WLCE8QIE&quot; width=&quot;469&quot; height=&quot;76&quot; src=&quot;attachments/WLCE8QIE.png&quot; ztype=&quot;zimage&quot;&gt;"></p>
<p>然后计算概率向量 a<sup>(2)</sup> <sub>RL</sub> ∈ R<sup>M </sup>：</p>
<p><img src="attachments/IKPRLVPR.png" alt="\&lt;img alt=&quot;&quot; data-attachment-key=&quot;IKPRLVPR&quot; width=&quot;497&quot; height=&quot;78&quot; src=&quot;attachments/IKPRLVPR.png&quot; ztype=&quot;zimage&quot;&gt;"></p>
<p>其中 W<sup> (1) </sup><sub>a</sub> ∈ R<sup>M×d</sup> 和 W<sup> (2)</sup><sub> s</sub> ∈ R<sup>d×d </sup>是要学习的参数。我们可以将 r1 和 r2 分别设置为 a<sup>(1) </sup><sub>RL</sub> 和 a<sup>(2) </sup><sub>RL</sub> 中概率最高的谓词。<span style="background-color: #ff666680">然而，这种方法总是为语义角色生成相同的谓词，并且性能相当差（7.1）</span>。作为解决方案，我们引入了一种根据上下文对谓词重新排序的机制。</p>
<p><strong>上下文相关谓词生成</strong>。设 X 和 Y 为两个实体。我们首先计算一个概率向量 v(X,Y ) ∈ R<sup>M</sup> 表示 (X,Y ) 与每个谓词 R ∈ R 的兼容性，以形成命题 R(X, Y )：</p>
<p><img src="attachments/CUYULVTV.png" alt="\&lt;img alt=&quot;&quot; data-attachment-key=&quot;CUYULVTV&quot; width=&quot;520&quot; height=&quot;68&quot; src=&quot;attachments/CUYULVTV.png&quot; ztype=&quot;zimage&quot;&gt;"></p>
<p>其中 h<sub>X</sub> ​​和 h<sub>Y</sub> 是 X 和 Y 的表示，⊕ 是串联运算符，W ∈ R<sup>m×2d</sup> 是模型参数。我<span style="background-color: #ff666680">们将兼容性概率与角色谓词关联概率相结合，以生成最终谓词</span>。具体来说，对于事件触发器 T ，一定实体 B ∈ B 和参数候选 A ∈ A，我们生成以下两个谓词：</p>
<p><img src="attachments/RCETCNC6.png" alt="\&lt;img alt=&quot;&quot; data-attachment-key=&quot;RCETCNC6&quot; width=&quot;601&quot; height=&quot;122&quot; src=&quot;attachments/RCETCNC6.png&quot; ztype=&quot;zimage&quot;&gt;"></p>
<p>其中是逐元素乘法运算符，s<sub>X</sub>表示被选为候选线索实体集 B 中的实体 X 的得分（式（1））。这样，生成的 FOL 规则为 RL(T, A) ← r1(T, B) ∧ r2(B, A)，暗示到达事件参数 A 的推理路径： T r1 −→ B r2 −→ A 。\<br>4 优化和泛化</p>
<p>由于 FOL 规则的离散性，使用 FOL 规则进行优化通常具有挑战性（Qu 等人，2021a）。在这里，我们提出了用于松弛的 T-Norm 模糊逻辑，它产生了端到端的学习过程。</p>
<p><strong>用于松弛的 T 范数模糊逻辑</strong>。 T-Norm 模糊逻辑通过承认 1（真值）和 0（假值）之间的中间真值来概括经典的二值逻辑。对于我们生成的 FOL 规则 RL(T, A) ← r1(T, B) ∧ r2(B, A)，我们将 r1(T, B) 和 r2(B, A) 的真值设置为相应的分数式（6）和式（7）中，分别记为p1和p2。然后，遵循 Łukasiewicz T-Norm 逻辑，两个命题的合取对应于：</p>
<p><img src="attachments/3VGXZBE4.png" alt="\&lt;img alt=&quot;&quot; data-attachment-key=&quot;3VGXZBE4&quot; width=&quot;572&quot; height=&quot;55&quot; src=&quot;attachments/3VGXZBE4.png&quot; ztype=&quot;zimage&quot;&gt;"></p>
<p>我们将其重写为度量4：M (T, B, A) = p(r1(T, B) ∧ r2(B, A)) 并将其用于规则排序和优化。特别地，我们枚举每个实体参数对 (B, A) ∈ B × A，并用 ( ˆ B, ˆ A) 表示得分最高的一个。然后我们得出以下优化损失：</p>
<p><img src="attachments/B45E6KDW.png" alt="\&lt;img alt=&quot;&quot; data-attachment-key=&quot;B45E6KDW&quot; width=&quot;588&quot; height=&quot;131&quot; src=&quot;attachments/B45E6KDW.png&quot; ztype=&quot;zimage&quot;&gt;"></p>
<p>其中θ表示整体参数集（在训练时，ground-truth参数是已知的，我们可以直接将最优参数设置为ground-truth）。尽管我们的方法考虑了每个候选实体和参数，但我们通过并行张量运算表明，我们的方法与先前的方法一样有效地进行竞争（参见附录 A.1）。</p>
<p>概括为一般情况。我们使用结构二谓词结构来解释我们的方法，但是很容易将其适用于具有任意数量谓词的一般情况。现在假设一个 n 谓词结构。我们首先使用类似于等式 1 的自回归机制来学习一系列角色预测关联向量 a(1) RL 、 a(2) RL 、…、a(n) RL 。 （3）和（4）。然后，我们重新排序并生成谓词r1，r2，····，rn以形成逻辑规则。为了优化，我们驱动以下度量 p(r1∧r2∧···∧rn) = min(p1, p2,···, pn)，这类似于等式： (8)进行规则排序和模型训练。</p>
<p>5.</p>
<p>基准和评估。我们使用两个文档级 EAE 基准进行实验：RAMS（Ebner 等人，2020）和 WikiEvents（Li 等人，2021）。 RAMS基准定义了139种事件类型和59种语义角色，并给出了7,329个带注释的文档； WikiEvents 基准定义了 50 个事件类型和 59 个语义角色，并提供了 246 个带注释的文档。详细的数据统计如表1所示。接下来（Ebner et al., 2020; Liu et al., 2021），我们采用类型约束解码（TCD）设置进行评估，假设事件触发器及其类型已知。我们在 RAMS 上使用 Span-F1，在 WikiEvents 上使用 Head-F1 和 Coref-F1 作为评估指标，其中 Head-F1 仅检查参数中的中心词，Coref-F1 还考虑参数之间的共指链接（Du 和Cardie 等人，2020a；Li 等人，2021；Ma 等人，2022）。</p>
<p>实施。在我们的方法中，我们使用 BERTbase 来学习上下文单词表示（Devlin 等人，2019）。使用开发集调整超参数。最后，实体候选集K的大小设置为40，从范围[20,30,40,50]中选择，而参数候选集的大小由外部实体识别器自动确定。谓词数量 M 设置为 [10, 15, 20, 25] 选项中的 20 个。为了优化，我们使用 Adam 优化器（Kingma 和 Ba，2015），批量大小为 10（来自 [5, 10, 15, 20]），学习率为 1e-4（来自 [1e-3, 1e-4, 1e]） -5]。</p>
<p>基线。为了进行比较，我们考虑以下四类方法：1）传统方法，例如 BIOLabel（Shi 和 Lin，2019），它将任务视为顺序标记问题。 2）全局编码方法，例如 QAEE（Du 和 Cardie，2020b）和 DocMRC（Liu 等人，2021），它们将任务形成为基于文档的问答问题，以及 MemNet（Du 等人，2022） ），它使用内存来存储全局事件信息。 3）生成方法，例如BART-Gen（Li et al., 2021），它提出了用于参数提取的序列到序列范式，以及PAIE（Ma et al., 2022），它采用集合生成公式。 4）使用额外监督的方法，例如采用框架相关知识的FEAE（Wei et al., 2021）和利用抽象意义表示（AMR）资源的TSAR（Xu et al., 2022）。</p>
<p>6.</p>
<p>在本节中，我们将介绍关键结果，按整体性能和捕获远程依赖项的结果分开。</p>
<p>6.1</p>
<p>表 2 和表 3 分别显示了不同模型在 RAMS 和 WikiEvents 上的性能。通过采用推理链范式，我们的方法显着优于以前的方法，并实现了最先进的性能——RAMS 上的 F1 为 56.1%，WikiEvents 上的 Head-F1 和 Coref-F1 为 72.3%。值得注意的是，我们的模型不使用外部资源进行训练，但它比以前使用大量外部资源训练的模型在 RAMS 上的 F1 中优于 6%，在 WikiEvents 上的 Head-F1 中优于 4%（在 Coref-F1 中为 7%）。此外，我们发现主要的改进来自于召回率的提高，这表明学习推理逻辑规则有助于定位以前的全局推理方法难以找到的论点。</p>
<p>6.2</p>
<p>然后，我们评估不同模型处理远程依赖关系的能力，这对于文档级任务至关重要。表 4 和表 5 相应地显示了不同参数触发距离 d 的结果，我们的模型在解决远程依赖性方面取得了显着的性能，例如，当 d=-1 时，F1 的绝对改进为 10.9%、15.7% 和 6.7%， RAMS 上分别为 d=1 和 d=2。有效性背后的见解是，通过采用推理链范式，我们的方法可以利用线索实体来缩短触发器和论点之间的距离，从而促进长上下文学习。尽管如此，我们还注意到，当参数是触发器之前的两个句子时（d=-2），我们的方法产生相对较差的性能。一个可能的原因是我们的推理链总是从触发器开始，而我们没有定义反向谓词5，这可能会限制其灵活性。我们将解决这些问题以供进一步的工作。</p>
<p>7.</p>
<p>我们进行了一系列详细的研究，以进一步验证我们模型的有效性。为了方便讨论，我们使用 RAMS 基准测试作为案例。</p>
<p>7.1</p>
<p>我们进行消融研究来分析不同成分的影响。</p>
<p>谓词生成的影响。表6将我们的方法与采用各种谓词生成策略的方法进行了对比：1）“w/o Predicate Generation”，直接生成推理路径而不生成谓词（换句话说，它只关心两个变量之间是否存在关系） ，但没有具体关系）。 2）“w/o Role Association”，消除了角色-谓词关联学习过程，其中谓词纯粹由两个变量确定。 3）“无 CTX 重新排名”，其中省略了上下文相关的谓词重新排序过程，其中谓词完全由角色生成。<span style="background-color: #ff666680">结果表明，谓词生成对于推理至关重要；如果没有它，性能会显着下降（F1 中为 23.9%）。</span>此外，<span style="background-color: #ff666680">角色的语义对于谓词生成至关重要；如果没有它，F1 的性能会下降 15.7%。</span>最后，<span style="background-color: #ff666680">学习上下文相关谓词重新排序是有利的，导致 F1 绝对提高 3.9%。</span></p>
<p>规则长度的消融。表7检查了LOC规则中谓词计数的影响，其中N（严格）表示我们精确地采用具有N个谓词的规则，N（自适应）表示我们采用最多具有N个谓词的规则并考虑预测自适应地获得最大分数，N（Ensemble）表示我们通过对参数的最终分数求和来集成结果。<span style="background-color: #ff666680">结果表明，指定固定数量的谓词会导致性能较差，而提供选择不同数量的谓词的选项会带来出色的性能</span>。这也<span style="background-color: #5fb23680">意味着论证寻找过程确实涉及不同的推理模式。</span>此外，我们没有注意到 N（自适应）相对于N（Ensemble），表明FOL规则可能不利于集成。</p>
<p>谓词数量的消融。图3考察了基于RAMS开发集的谓词数量对最终性能的影响，以及它们与规则长度的联合效应（我们使用Adaptive设置）。根据结果​​，<span style="background-color: #ff666680">我们的方法对谓词数量不敏感，并且当谓词数量超过 15 时始终保持高性能</span>。此外，<span style="background-color: #ff666680">我们证明当规则长度增加时可以减少谓词数量（例如，从两个到三个）。这是有道理的，因为更长的规则意味着更长的推理链，而推理链已经具有高度的内在表达性。相反，对于长度为 1 的 FOL 规则，即使我们增加谓词数量以增加其多样性，性能也始终不能令人满意。<br>7.2</span></p>
<p>鉴于我们的方法使用 FOL 规则来捕获基本推理模式，它可能比以前的推理方法更强大。我们通过分析其在低资源场景下的性能和防御对抗性攻击的性能来验证这一假设（Jia和Liang，2017）。</p>
<p>资源匮乏场景中的性能。图 4 比较了资源匮乏条件下的不同模型，其中显示模型仅在部分训练数据上进行训练（我们报告 5 次运行平均值以对抗随机性）。显然，我们的方法始终优于其他方法，并且值得注意的是，在极低的资源设置（少于 5% 的训练数据）下，它优于基于大型预训练语言模型和基于外部资源的 TSAR 提示的 PAIE，这表明了其有效性和学习 FOL 推理规则的普遍性。随着更多训练数据的可用，性能会提高。</p>
<p>防御对抗性攻击。图 5 显示了通过在测试示例中注入三种形式的噪声来防御对抗性攻击的结果。 ATK1：我们随机将句子中包含触发器的单词替换为槽符号[BLANK]； ATK2：我们将损坏的句子“答案是[空白]”放在包含触发器的句子后面。ATK3：我们在包含触发器的句子后面插入句子“[角色]的参数是[空白]”，其中 [ROLE] 被我们关注的语义角色所取代，考虑了两种设置： 攻击（随机），其中槽位填充有参数。在其他情况下发挥相同的作用。攻击（金色），其中槽填充了真实参数，但如果模型预测槽中的参数是答案，我们认为这是一个错误，因为注入的句子与上下文无关。结果表明，我们的方法在防御对抗性攻击方面表现出色，尤其是在“攻击（随机）”设置下（见图 5(a)）。原因之一是我们的方法强制预测与文档上下文中其他实体具有语义关系的参数，因此它受隔离注入参数的影响较小。使用真实参数来防御攻击更具挑战性（图 5(b)），但我们的方法仍然实现了最佳的整体性能。</p>
<p>7.3</p>
<p>表 8 通过案例研究检验了我们方法的可解释性。通过分析案例 1)、2) 和 3)，我们建议我们的方法可以为相同的语义角色生成特定的且依赖于上下文的推理规则。此外，情况 2) 和 3) 的推理模式类似，其中 r2 可以解释为 Attacker 谓词，r4 可以解释为 LocationIn 谓词。情况 4) 生成与情况 2) 和 3) 相同的谓词 r2，可以将其解释为支付事件的 Committer 谓词；它与情况 2) 和 3) 中攻击事件的攻击者共享相似的语义。案例 5) 表明我们的方法可以捕获极远的依赖关系。</p>
<p>8.</p>
<p>总之，我们提出了一种新的文档级 EAE 链推理范式，展示了捕获远程依赖项和提高可解释性的明显好处。我们的方法构建一阶逻辑规则来表示参数查询，并使用 T-Norm 模糊逻辑进行端到端学习。通过这种机制，我们的方法在两个基准测试中实现了最先进的性能，并在解决资源匮乏场景和防御对抗性攻击方面表现出良好的鲁棒性。在未来的工作中，我们寻求将我们的方法扩展到需要远程依赖关系建模的其他任务，例如文档级关系提取。</p>
<p>9.</p>
<p>我们方法的一个限制是，当存在不同长度的规则时，最终结果是由集成决定的，而不是通过构建模型来生成具有最佳长度的单个规则。第二种方式更加自然和重要，因为计算出规则的长度也是符号推理的关键部分。然而，它需要更多的参数化（例如，规则的长度可以是参数）和更高级的优化方法。上述方法的研究留待以后的工作。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN,en,default">
    <link itemprop="mainEntityOfPage" href="http://humble2967738843.github.io/2024/04/10/tui-jian-xi-tong/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="院龙">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="院龙">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2024/04/10/tui-jian-xi-tong/" class="post-title-link" itemprop="url">推荐系统</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2024-04-10 12:09:32" itemprop="dateCreated datePublished" datetime="2024-04-10T12:09:32+08:00">2024-04-10</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2024-06-27 07:34:34" itemprop="dateModified" datetime="2024-06-27T07:34:34+08:00">2024-06-27</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%90%9C%E5%B9%BF%E6%8E%A8/" itemprop="url" rel="index"><span itemprop="name">搜广推</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="第一章-推荐系统概述"><a href="#第一章-推荐系统概述" class="headerlink" title="第一章 推荐系统概述"></a>第一章 推荐系统概述</h1><h2 id="1-1推荐系统的意义"><a href="#1-1推荐系统的意义" class="headerlink" title="1.1推荐系统的意义"></a>1.1推荐系统的意义</h2><p>推荐系统就是一个将信息生产者和信息消费者连接起来的桥梁。平台往往会作为推荐系统的载体，实现信息生产者和消费者之间信息的匹配。上述提到的平台方、信息生产者和消费者可以分别用平台方（如：腾讯视频、淘宝、网易云音乐等）、物品（如：视频、商品、音乐等）和用户和来指代。下面分别从这三方需求出发，介绍推荐系统的存在的意义。</p>
<h3 id="平台方"><a href="#平台方" class="headerlink" title="平台方"></a>平台方</h3><p>平台方一般是为信息生产者提供物品展示的位置，然后通过不同的方式吸引用户来到平台上寻找他们感兴趣的物品。平台通过商家对物品的展示以及用户的浏览、观看或下单等行为，就产生了所谓的”流量”。</p>
<p>对平台方而言，流量的高效利用是推荐系统存在的重要原因。以典型的电商网站一般具有如图所示的树状拓扑结构，树状结构在连通性方面有着天然的劣势，阻碍这流量的高效流通。推荐系统的出现使得原本的树状结构变成网络拓扑结构，大大增强了整个网络的连通性。推荐模块不仅使用户在当前页面有了更好的选择路径，同时也给了每个商品增加入口和展示机会，进而提高了成交概率。而推荐质量的好坏，直接决定了用户选择这条路径的可能性，进而影响着流量的利用效率。</p>
<h3 id="推荐和搜索的区别"><a href="#推荐和搜索的区别" class="headerlink" title="推荐和搜索的区别"></a>推荐和搜索的区别</h3><p>搜索和推荐都是解决互联网大数据时代信息过载的手段，但是它们也存在着许多的不同：</p>
<ol>
<li><strong>用户意图</strong>：搜索时的用户意图是非常明确的，用户通过查询的关键词主动发起搜索请求。对于推荐而言，用户的需求是不明确的，推荐系统在通过对用户历史兴趣的分析给用户推荐他们可能感兴趣的内容。</li>
<li><strong>个性化程度</strong>：对于搜索而言，由于限定的了搜索词，所以展示的内容对于用户来说是有标准答案的，所以搜索的个性化程度较低。而对于推荐来说，推荐的内容本身就是没有标准答案的，每个人都有不同的兴趣，所以每个人展示的内容，个性化程度比较强。</li>
<li><strong>优化目标</strong>：对于搜索系统而言，更希望可以快速地、准确地定位到标准答案，所以希望搜索结果中答案越靠前越好，通常评价指标有：归一化折损累计收益（NDCG）、精确率（Precision）和召回率（Recall）。对于推荐系统而言，因为没有标准的答案，所以优化目标可能会更宽泛。例如用户停留时长、点击、多样性，评分等。不同的优化目标又可以拆解成具体的不同的评价指标。</li>
<li><strong>马太效应和长尾理论</strong>：对于搜索系统来说，用户的点击基本都集中在排列靠前的内容上，对于排列靠后的很少会被关注，这就是马太效应。而对于推荐系统来说，热门物品被用户关注更多，冷门物品不怎么被关注的现象也是存在的，所以也存在马太效应。此外，在推荐系统中，冷门物品的数量远远高于热门物品的数量，所以物品的长尾性非常明显。</li>
</ol>
<h2 id="1-2推荐系统的架构"><a href="#1-2推荐系统的架构" class="headerlink" title="1.2推荐系统的架构"></a>1.2推荐系统的架构</h2><p>推荐和搜索系统核心的的任务是从海量物品中找到用户感兴趣的内容。在这个背景下，推荐系统包含的模块非常多，每个模块将会有很多专业研究的工程和研究工程师，作为刚入门的应届生或者实习生很难对每个模块都有很深的理解，实际上也大可不必，我们完全可以从学习好一个模块技术后，以点带面学习整个系统，虽然正式工作中我们放入门每个人将只会负责的也是整个系统的一部分。但是掌握推荐系统最重要的还是梳理清楚整个推荐系统的架构，知道每一个部分需要完成哪些任务，是如何做的，主要的技术栈是什么，有哪些局限和可以研究的问题，能够对我们学习推荐系统有一个提纲挈领的作用。</p>
<p>所以这篇文章将会从<strong>系统架构</strong>和<strong>算法架构</strong>两个角度出发解析推荐系统通用架构。系统架构设计思想是大数据背景下如何有效利用海量和实时数据，将推荐系统按照对数据利用情况和系统响应要求出发，将整个架构分为<strong>离线层、近线层、在线层</strong>三个模块。然后分析这三个模块分别承担推荐系统什么任务，有什么制约要求。这种角度不和召回、排序这种通俗我们理解算法架构，因为更多的是考虑推荐算法在工程技术实现上的问题，系统架构是如何权衡利弊，如何利用各种技术工具帮助我们达到想要的目的的，方便我们理解为什么推荐系统要这样设计。</p>
<p>而算法架构是从我们比较熟悉的<strong>召回、粗排、排序、重排</strong>等算法环节角度出发的，重要的是要去理解每个环节需要完成的任务，每个环节的评价体系，以及为什么要那么设计。还有一个重要问题是每个环节涉及到的技术栈和主流算法，这部分非常重要而且篇幅较大，所以我们放在下一个章节讲述。</p>
<p>架构设计是一个非常大的话题，设计的核心在于平衡和妥协。在推荐系统不同时期、不同的环境、不同的数据，架构都会面临不一样的问题。网飞官方博客有一段总结：</p>
<blockquote>
<p>We want the ability to use sophisticated machine learning algorithms that can grow to arbitrary complexity and can deal with huge amounts of data. We also want an architecture that allows for flexible and agile innovation where new approaches can be developed and plugged-in easily. Plus, we want our recommendation results to be fresh and respond quickly to new data and user actions. Finding the sweet spot between these desires is not trivial: it requires a thoughtful analysis of requirements, careful selection of technologies, and a strategic decomposition of recommendation algorithms to achieve the best outcomes for our members. <strong>“我们需要具备使用复杂机器学习算法的能力，这些算法要可以适应高度复杂性，可以处理大量数据。我们还要能够提供灵活、敏捷创新的架构，新的方法可以很容易在其基础上开发和插入。而且，我们需要我们的推荐结果足够新，能快速响应新的数据和用户行为。找到这些要求之间恰当的平衡并不容易，需要深思熟虑的需求分析，细心的技术选择，战略性的推荐算法分解，最终才能为客户达成最佳的结果。”</strong></p>
</blockquote>
<p>所以在思考推荐系统架构考虑的第一个问题是确定边界：知道推荐系统要负责哪部分问题，这就是边界内的部分。在这个基础上，架构要分为哪几个部分，每一部分需要完成的子功能是什么，每一部分依赖外界的什么。了解推荐系统架构也和上文讲到的思路一样，我们需要知道的是推荐系统要负责的是怎么问题，每一个子模块分别承担了哪些功能，它们的主流技术栈是什么。从这个角度来阅读本文，将会有助于读者抓住重点。</p>
<h3 id="系统架构"><a href="#系统架构" class="headerlink" title="系统架构"></a>系统架构</h3><p>推荐系统架构，首先从数据驱动角度，对于数据，最简单的方法是存下来，留作后续离线处理，<strong>离线层</strong>就是我们用来管理离线作业的部分架构。<strong>在线层</strong>能更快地响应最近的事件和用户交互，但必须实时完成。这会限制使用算法的复杂性和处理的数据量。离线计算对于数据数量和算法复杂度限制更少，因为它以批量方式完成，没有很强的时间要求。不过，由于没有及时加入最新的数据，所以很容易过时。</p>
<p>个性化架构的关键问题，就是如何以无缝方式结合、管理在线和离线计算过程。<strong>近线层</strong>介于两种方法之间，可以执行类似于在线计算的方法，但又不必以实时方式完成。这种设计思想最经典的就是Netflix在2013年提出的架构，整个架构分为</p>
<ol>
<li>离线层：不用实时数据，不提供实时响应；</li>
<li>近线层：使用实时数据，不保证实时响应；</li>
<li>在线层：使用实时数据，保证实时在线服务；</li>
</ol>
<h3 id="设计思想"><a href="#设计思想" class="headerlink" title="设计思想"></a>设计思想</h3><p>网飞的这个架构提出的非常早，其中的技术也许不是现在常用的技术了，但是架构模型仍然被很多公司采用。</p>
<p>这个架构为什么要这么设计，本质上是因为推荐系统是由大量数据驱动的，大数据框架最经典的就是lambda架构和kappa架构。而推荐系统在不同环节所使用的数据、处理数据的量级、需要的读取速度都是不同的，目前的技术还是很难实现一套端到端的及时响应系统，所以这种架构的设计本质上还是一种权衡后的产物，所以有了下图这种模型：</p>
<p><img src="https://s2.loli.net/2024/04/11/MlPqAFQacgiJpwL.png" alt="image-20240411224023001"></p>
<p><img src="https://s2.loli.net/2024/04/11/1BOuDUGCretljZF.png" alt="image-20240411224115381"></p>
<p>整个数据部分其实是一整个链路，主要是三块，分别是客户端及服务器实时数据处理、流处理平台准实时数据处理和大数据平台离线数据处理这三个部分。</p>
<p>看到这里，一个很直观的问题就是，为什么数据处理需要这么多步骤？这些步骤都是干嘛的，存在的意义是什么？</p>
<p>我们一个一个来说，首先是客户端和服务端的实时数据处理。这个很好理解，这个步骤的工作就是记录。将用户在平台上真实的行为记录下来，比如用户看到了哪些内容，和哪些内容发生了交互，和哪些没有发生了交互。如果再精细一点，还会记录用户停留的时间，用户使用的设备等等。除此之外还会记录行为发生的时间，行为发生的session等其他上下文信息。</p>
<p>这一部分主要是后端和客户端完成，行业术语叫做埋点。所谓的埋点其实就是记录点，因为数据这种东西需要工程师去主动记录，不记录就没有数据，记录了才有数据。既然我们要做推荐系统，要分析用户行为，还要训练模型，显然需要数据。需要数据，就需要记录。</p>
<p>第二个步骤是流处理平台准实时数据处理，这一步是干嘛的呢，其实也是记录数据，不过是记录一些准实时的数据。很多同学又会迷糊了，实时我理解，就是当下立即的意思，准实时是什么意思呢？准实时的意思也是实时，只不过没有那么即时，比如可能存在几分钟的误差。这样存在误差的即时数据，行业术语叫做准实时。那什么样的准实时数据需要记录呢？在推荐领域基本上只有一个类别，就是用户行为数据。也就是用户在观看这个内容之前还看过哪些内容，和哪些内容发生过交互。理想情况这部分数据也需要做成实时，但由于这部分数据量比较大，并且逻辑也相对复杂，所以很难做到非常实时，一般都是通过消息队列加在线缓存的方式做成准实时。</p>
<p>最后我们看第三个步骤，叫做离线数据处理，离线也就是线下处理，基本上就没有时限的要求了。</p>
<p>一般来说，离线处理才是数据处理的大头。所有“脏活累活”复杂的操作都是在离线完成的，比如说一些join操作。后端只是记录了用户交互的商品id，我们需要商品的详细信息怎么办？需要去和商品表关联查表。显然数据关联是一个非常耗时的操作，所以只能放到离线来做。</p>
<p>接下来详细介绍一下这三个模块。</p>
<h3 id="离线层"><a href="#离线层" class="headerlink" title="离线层"></a>离线层</h3><p>离线层是计算量最大的一个部分，它的特点是不依赖实时数据，也不需要实时提供服务。需要实现的主要功能模块是：</p>
<ol>
<li>数据处理、数据存储；</li>
<li>特征工程、离线特征计算；</li>
<li>离线模型的训练；</li>
</ol>
<p>这里我们可以看出离线层的任务是最接近学校中我们处理数据、训练模型这种任务的，不同可能就是需要面临更大规模的数据。离线任务一般会按照天或者更久运行，比如每天晚上定期更新这一天的数据，然后重新训练模型，第二天上线新模型。</p>
<p><img src="https://s2.loli.net/2024/04/11/PNs4UCxmSDdvIuH.png" alt="image-20240411225231577"></p>
<h3 id="离线层优势和不足"><a href="#离线层优势和不足" class="headerlink" title="离线层优势和不足"></a>离线层优势和不足</h3><p>离线层面临的数据量级是最大的，面临主要的问题是海量数据存储、大规模特征工程、多机分布式机器学习模型训练。目前主流的做法是HDFS，收集到我们所有的业务数据，通过HIVE等工具，从全量数据中抽取出我们需要的数据，进行相应的加工，离线阶段主流使用的分布式框架一般是Spark。所以离线层有如下的优势：</p>
<ol>
<li>可以处理大量的数据，进行大规模特征工程；</li>
<li>可以进行批量处理和计算；</li>
<li>不用有响应时间要求；</li>
</ol>
<p>但是同样的，如果我们只使用用户离线数据，最大的不足就是无法反应用户的实时兴趣变化，这就促使了近线层的产生。</p>
<h3 id="近线层"><a href="#近线层" class="headerlink" title="近线层"></a>近线层</h3><p>近线层的主要特点是准实时，它可以获得实时数据，然后快速计算提供服务，但是并不要求它和在线层一样达到几十毫秒这种延时要求。近线层的产生是同时想要弥补离线层和在线层的不足，折中的产物。</p>
<p>它适合处理一些对延时比较敏感的任务，比如：</p>
<ol>
<li>特征的事实更新计算：例如统计用户对不同type的ctr，推荐系统一个老生常谈的问题就是特征分布不一致怎么办，如果使用离线算好的特征就容易出现这个问题。近线层能够获取实时数据，按照用户的实时兴趣计算就能很好避免这个问题。</li>
<li>实时训练数据的获取：比如在使用DIN、DSIN这行网络会依赖于用户的实时兴趣变化，用户几分钟前的点击就可以通过近线层获取特征输入模型。</li>
<li>模型实时训练：可以通过在线学习的方法更新模型，实时推送到线上；</li>
</ol>
<p>近线层的发展得益于最近几年大数据技术的发展，很多流处理框架的提出大大促进了近线层的进步。如今Flink、Storm等工具一统天下。</p>
<p><img src="https://s2.loli.net/2024/04/12/cubdwvZhHnsAx87.png" alt="image-20240412182725567"></p>
<h3 id="在线层"><a href="#在线层" class="headerlink" title="在线层"></a>在线层</h3><p>在线层，就是直接面向用户的的那一层了。最大的特点是对响应延时有要求，因为它是直接面对用户群体的，你可以想象你打开抖音淘宝等界面，几乎都是秒刷出来给你的推荐结果的，不会说还需要让你等待几秒钟时间。所有的用户请求都会发送到在线层，在线层需要快速返回结果，它主要承担的工作有：</p>
<ol>
<li>模型在线服务；包括了快速召回和排序；</li>
<li>在线特征快速处理拼接：：根据传入的用户ID和场景，快速读取特征和处理；</li>
<li>AB实验或者分流：根据不同用户采用不一样的模型，比如冷启动用户和正常服务模型；</li>
<li>运筹优化和业务干预：比如要对特殊商家流量扶持、对某些内容限流；</li>
</ol>
<p>典型的在线服务是用过RESTful/RPC等提供服务，一般是公司后台服务部门调用我们的这个服务，返回给前端。具体部署应用比较多的方式就是使用Docker在K8S部署。而在线服务的数据源就是我们在离线层计算好的每个用户和商品特征，我们事先存放在数据库中，在线层只需要实时拼接，不进行复杂的特征运算，然后输入近线层或者离线层已经训练好的模型，根据推理结果进行排序，最后返回给后台服务器，后台服务器根据我们对每一个用户的打分，再返回给用户。</p>
<p>在线层最大的问题就是对实时性要求特别高，一般来说是几十毫秒，这就限制了我们能做的工作，很多任务往往无法及时完成，需要近线层协助我们做。</p>
<h3 id="算法架构"><a href="#算法架构" class="headerlink" title="算法架构"></a>算法架构</h3><p>我们在入门学习推荐系统的时候，更加关注的是哪个模型AUC更高、topK效果好，哪个模型更加牛逼的问题，从基本的协同过滤到点击率预估算法，从深度学习到强化学习，学术界都始终走在最前列。一个推荐算法从出现到在业界得到广泛应用是一个长期的过程，因为在实际的生产系统中，首先需要保证的是稳定、实时地向用户提供推荐服务，在这个前提下才能追求推荐系统的效果。</p>
<p><img src="https://s2.loli.net/2024/04/13/otixzas2RVNmWl8.png" alt="image-20240413222420120"></p>
<ul>
<li>召回</li>
</ul>
<p>召回层的主要目标时从推荐池中选取几千上万的item，送给后续的排序模块。由于召回面对的候选集十分大，且一般需要在线输出，故召回模块必须<strong>轻量快速低延迟</strong>。由于后续还有排序模块作为保障，<strong>召回不需要十分准确，但不可遗漏</strong>（特别是搜索系统中的召回模块）。</p>
<p>如果没有召回层，每个User都能和每一个Item去在线排序阶段预测目标概率，理论上来说是效果最好，但是是不现实的，<strong>线上不延迟允许</strong>，所以召回和粗排阶段就要做一些<strong>候选集筛选</strong>的工作，保证在有限的能够给到排序层去做精排的候选集的时间里，效果最大化。另一个方面就是通过这种<strong>模型级联</strong>的方式，可以<strong>减少用排序阶段拟合多目标的压力</strong>，比如召回阶段我们现在主要是在<strong>保证Item质量的基础上注重覆盖率多样性</strong>，<strong>粗排阶段主要用简单的模型来解决不同路的召回和当前用户的相关性问题</strong>，最后截断到1k个以内的候选集，这个<strong>候选集符合一定的个性化相关性、视频质量和多样性的保证</strong>，然后做ranking去做复杂模型的predict。</p>
<p>目前基本上<strong>采用多路召回解决范式，分为非个性化召回和个性化召回</strong>。个性化召回又有content-based、behavior-based、feature-based等多种方式。</p>
<p>召回主要考虑的内容有：</p>
<ol>
<li><strong>考虑用户层面</strong>：用户兴趣的多元化，用户需求与场景的多元化：例如：新闻需求，重大要闻，相关内容沉浸阅读等等</li>
<li><strong>考虑系统层面</strong>：增强系统的鲁棒性；部分召回失效，其余召回队列兜底不会导致整个召回层失效；排序层失效，召回队列兜底不会导致整个推荐系统失效</li>
<li><strong>系统多样性内容分发</strong>：图文、视频、小视频；精准、试探、时效一定比例；召回目标的多元化，例如：相关性，沉浸时长，时效性，特色内容等等</li>
<li><strong>可解释性推荐一部分召回是有明确推荐理由的</strong>：很好的解决产品性数据的引入；</li>
</ol>
<ul>
<li>粗排</li>
</ul>
<p>粗排的原因是有时候召回的结果还是太多，精排层速度还是跟不上，所以加入粗排。<strong>粗排可以理解为精排前的一轮过滤机制，减轻精排模块的压力。粗排介于召回和精排之间，要同时兼顾精准性和低延迟</strong>。目前粗排一般也都模型化了，其训练样本类似于精排，选取曝光点击为正样本，曝光未点击为负样本。但由于粗排一般面向上万的候选集，而精排只有几百上千，其解空间大很多。</p>
<p>粗排阶段的架构设计主要是考虑三个方面，一个是<strong>根据精排模型中的重要特征，来做候选集的截断</strong>，另一部分是<strong>有一些召回设计，比如热度或者语义相关的这些结果，仅考虑了item侧的特征，可以用粗排模型来排序跟当前User之间的相关性，据此来做截断</strong>，这样是比单独的按照item侧的倒排分数截断得到更加个性化的结果，最后是算法的选型要在在线服务的性能上有保证，因为这个阶段在pipeline中完成从召回到精排的截断工作，在延迟允许的范围内能处理更多的召回候选集理论上与精排效果正相关。</p>
<ul>
<li>精排</li>
</ul>
<p>精排层，也是我们学习推荐入门最常常接触的层，我们所熟悉的算法很大一部分都来自精排层。这一层的任务是获取粗排模块的结果，对候选集进行打分和排序。精排需要在最大时延允许的情况下，保证打分的精准性，是整个系统中至关重要的一个模块，也是最复杂，研究最多的一个模块。</p>
<p>精排是推荐系统各层级中最纯粹的一层，他的目标比较单一且集中，一门心思的实现目标的调优即可。最开始的时候精排模型的常见目标是ctr,后续逐渐发展了cvr等多类目标。精排和粗排层的基本目标是一致的，都是对商品集合进行排序，但是和粗排不同的是，精排只需要对少量的商品(即粗排输出的商品集合的topN)进行排序即可。因此，精排中可以使用比粗排更多的特征，更复杂的模型和更精细的策略（用户的特征和行为在该层的大量使用和参与也是基于这个原因）。</p>
<p>精排层模型是推荐系统中涵盖的研究方向最多，有非常多的子领域值得研究探索，这也是推荐系统中技术含量最高的部分，毕竟它是直接面对用户，产生的结果对用户影响最大的一层。目前精排层深度学习已经一统天下了，精排阶段采用的方案相对通用，首先一天的样本量是几十亿的级别，我们要解决的是样本规模的问题，尽量多的喂给模型去记忆，另一个方面时效性上，用户的反馈产生的时候，怎么尽快的把新的反馈给到模型里去，学到最新的知识。</p>
<ul>
<li>重排</li>
</ul>
<p>常见的有三种优化目标：Point Wise、Pair Wise 和 List Wise。重排序阶段对精排生成的Top-N个物品的序列进行重新排序，生成一个Top-K个物品的序列，作为排序系统最后的结果，直接展现给用户。重排序的原因是因为多个物品之间往往是相互影响的，而精排序是根据PointWise得分，容易造成推荐结果同质化严重，有很多冗余信息。而重排序面对的挑战就是海量状态空间如何求解的问题，一般在精排层我们使用AUC作为指标，但是在重排序更多关注NDCG等指标。</p>
<p>重排序在业务中，获取精排的排序结果，还会根据一些策略、运营规则参与排序，比如强制去重、间隔排序、流量扶持等、运营策略、多样性、context上下文等，重新进行一个微调。重排序更多的是List Wise作为优化目标的，它关注的是列表中商品顺序的问题来优化模型，但是一般List Wise因为状态空间大，存在训练速度慢的问题。</p>
<p>由于精排模型一般比较复杂，基于系统时延考虑，一般采用point-wise方式，并行对每个item进行打分。这就使得打分时缺少了上下文感知能力。用户最终是否会点击购买一个商品，除了和它自身有关外，和它周围其他的item也息息相关。重排一般比较轻量，可以加入上下文感知能力，提升推荐整体算法效率。比如三八节对美妆类目商品提权，类目打散、同图打散、同卖家打散等保证用户体验措施。重排中规则比较多，但目前也有不少基于模型来提升重排效果的方案。</p>
<ul>
<li>混排</li>
</ul>
<p>多个业务线都想在Feeds流中获取曝光，则需要对它们的结果进行混排。比如推荐流中插入广告、视频流中插入图文和banner等。可以基于规则策略（如广告定坑）和强化学习来实现。</p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>整篇文章从系统架构梳理了Netfliex的经典推荐系统架构，整个架构更多是偏向实时性能和效果之间trade off的结果。如果从另外的角度看推荐系统架构，比如从数据流向，或者说从推荐系统各个时序依赖来看，就是我们最熟悉的召回、粗排、精排、重排、混排等模块了。这种角度来看是把推荐系统从前往后串起来，其中每一个模块既有在离线层工作的，也有在在线层工作的。而从数据驱动角度看，更能够看到推荐系统的完整技术栈，推荐系统当前面临的局限和发展方向。</p>
<p>召回、排序这些里面单拿出任何一个模块都非常复杂。这也是为什么大家都说大厂拧螺丝的原因，因为很可能某个人只会负责其中很小的一个模块。许多人说起自己的模块来如数家珍，但对于全局缺乏认识，带来的结果是当你某天跳槽了或者是工作内容变化了，之前从工作当中的学习积累很难沉淀下来，这对于程序员的成长来说是很不利的。</p>
<p>所以希望这篇文章能够帮助大家在负责某一个模块，优化某一个功能的时候，除了能够有算法和数据，还能能够考虑对整个架构带来的影响，如何提升整体的一个性能，慢慢开阔自己的眼界，构建出一个更好的推荐系统。</p>
<h2 id="1-3推荐系统的技术栈"><a href="#1-3推荐系统的技术栈" class="headerlink" title="1.3推荐系统的技术栈"></a>1.3推荐系统的技术栈</h2><p>推荐系统是一个非常大的框架，有非常多的模块在里面，完整的一套推荐系统体系里，不仅会涉及到推荐算法工程师、后台开发工程师、数据挖掘/分析工程师、NLP/CV工程师还有前端、客户端甚至产品、运营等支持。我们作为算法工程师，需要掌握的技术栈主要就是在算法和工程两个区域了，所以这篇文章将会分别从算法和工程两个角度出发，结合两者分析当前主流的一些推荐算法技术栈。</p>
<h3 id="算法"><a href="#算法" class="headerlink" title="算法"></a>算法</h3><p>首先我们从推荐系统架构出发，一种分法是将整个推荐系统架构分为召回、粗排、精排、重排、混排等模块。它的分解方法是从一份数据如何从生产出来，到线上服务完整顺序的一个流程。因为在不同环节，我们一般会考虑不同的算法，所以这种角度出发我们来研究推荐系统主流的算法技术栈。</p>
<p><img src="C:/Users/YL.YL.000/AppData/Roaming/Typora/typora-user-images/image-20240417131451288.png" alt="image-20240417131451288"></p>
<p>为了帮助新手在后文方便理解，首先简单介绍这些模块的功能主要是：</p>
<ul>
<li>召回：从推荐池中选取几千上万的item，送给后续的排序模块。由于召回面对的候选集十分大，且一般需要在线输出，故召回模块必须轻量快速低延迟。由于后续还有排序模块作为保障，召回不需要十分准确，但不可遗漏（特别是搜索系统中的召回模块）。目前基本上采用多路召回解决范式，分为非个性化召回和个性化召回。个性化召回又有content-based、behavior-based、feature-based等多种方式。</li>
<li>粗排：粗拍的原因是有时候召回的结果还是太多，精排层速度还是跟不上，所以加入粗排。粗排可以理解为精排前的一轮过滤机制，减轻精排模块的压力。粗排介于召回和精排之间，要同时兼顾精准性和低延迟。一般模型也不能过于复杂</li>
<li>精排：获取粗排模块的结果，对候选集进行打分和排序。精排需要在最大时延允许的情况下，保证打分的精准性，是整个系统中至关重要的一个模块，也是最复杂，研究最多的一个模块。精排系统构建一般需要涉及样本、特征、模型三部分。</li>
<li>重排：获取精排的排序结果，基于运营策略、多样性、context上下文等，重新进行一个微调。比如三八节对美妆类目商品提权，类目打散、同图打散、同卖家打散等保证用户体验措施。重排中规则比较多，但目前也有不少基于模型来提升重排效果的方案。</li>
<li>混排：多个业务线都想在Feeds流中获取曝光，则需要对它们的结果进行混排。比如推荐流中插入广告、视频流中插入图文和banner等。可以基于规则策略（如广告定坑）和强化学习来实现。</li>
</ul>
<h3 id="画像层"><a href="#画像层" class="headerlink" title="画像层"></a>画像层</h3><p>首先是推荐系统的物料库，这部分内容里，算法主要体现在如何绘制一个用户画像和商品画像。这个环节是推荐系统架构的基础设施，一般可能新用户/商品进来，或者每周定期会重新一次整个物料库，计算其中信息，为用户打上标签，计算统计信息，为商品做内容理解等内容。其中用户画像是大家比较容易理解的，比如用户年龄、爱好通常APP会通过注册界面收集这些信息。而商品画像形式就非常多了，比如淘宝主要推荐商品，抖音主要是短视频，所以大家的物料形式比较多，内容、质量差异也比较大，所以内容画像各家的做法也不同，当前比较主流的都会涉及到一个多模态信息内容理解。下面我贴了一个微信看一看的内容画像框架，然后我们来介绍下在这一块主要使用的算法技术。</p>
<p><img src="C:/Users/YL.YL.000/AppData/Roaming/Typora/typora-user-images/image-20240417131728890.png" alt="image-20240417131728890"></p>
<p>一般推荐系统会加入多模态的一个内容理解。我们用短视频形式举个例子，假设用户拍摄了一条短视频，上传到了平台，从推荐角度看，首先我们有的信息是这条短视频的作者、长度、作者为它选择的标签、时间戳这些信息。但是这对于推荐来说是远远不够的，首先作者打上的标签不一定准确反映作品，原因可能是我们模型的语义空间可能和作者/现实世界不一致。其次我们需要更多维度的特征，比如有些用户喜欢看小姐姐跳舞，那我希望能够判断一条视频中是否有小姐姐，这就涉及到封面图的基于CV的内容抽取或者整个视频的抽取；再比如作品的标题一般能够反映主题信息，除了很多平台常用的用“#”加上一个标签以外，我们也希望能够通过标题抽取出基于NLP的信息。还有更多的维度可以考虑：封面图多维度的多媒体特征体系，包括人脸识别，人脸embedding，标签，一二级分类，视频embedding表示，水印，OCR识别，清晰度，低俗色情，敏感信息等多种维度。</p>
<p>这里面涉及的任务主要是CV的目标检测、语义分割等任务，NLP中的情感分析、摘要抽取、自然语言理解等任务。但是这部分算法一般团队都会有专门负责的组，不需要推荐算法工程师来负责，他们会有多模态的语意标签输出，主要形式是各种粒度的Embedding。我们只需要在我们的推荐模型中引入这些预训练的Embedding。</p>
<h3 id="文本理解"><a href="#文本理解" class="headerlink" title="文本理解"></a>文本理解</h3><p>这应该是用的最多的模态信息，包括item的标题、正文、OCR、评论等数据。这里面也可以产生不同粒度的信息，比如文本分类，把整个item做一个粗粒度的分类。</p>
<p>这里的典型算法有：RNN、TextCNN、FastText、Bert等；</p>
<h3 id="关键词标签"><a href="#关键词标签" class="headerlink" title="关键词标签"></a>关键词标签</h3><p>相比文本分类，关键词是更细粒度的信息，往往是一个mutil-hot的形式，它会对item在我们的标签库的选取最合适的关键词或者标签。</p>
<p>这里典型的算法有：TF-IDF、Bert、LSTM-CRF等。</p>
<h3 id="内容理解"><a href="#内容理解" class="headerlink" title="内容理解"></a>内容理解</h3><p>在很多场景下，推荐的主题都是视频或者图片，远远多于仅推荐文本的情况，这里视频/图片item中的内容中除了文本的内容以外，更多的信息其实来源于视频/图片内容本身, 因此需要尝试从多种模态中抽取更丰富的信息。主要包括分类信息、封面图OCR的信息、视频标签信息等</p>
<p>这里典型的算法有：TSN、RetinaFace、PSENet等。</p>
<h3 id="知识图谱"><a href="#知识图谱" class="headerlink" title="知识图谱"></a>知识图谱</h3><p>知识图谱作为知识承载系统，用于对接内外部关键词信息与词关系信息；内容画像会将原关系信息整合，并构建可业务应用的关系知识体系，其次，依赖业务中积累用户行为产生的实体关系数据，本身用户需求的标签信息，一并用于构建业务知识的兴趣图谱，基于同构网络与异构网络表示学习等核心模型，输出知识表示与表达，抽象后的图谱用于文本识别，推荐语义理解，兴趣拓展推理等场景，直接用于兴趣推理的冷启场景已经验证有很不错的收益。</p>
<p>这方面的算法有：KGAT、RippleNet等。</p>
<h3 id="召回-粗排"><a href="#召回-粗排" class="headerlink" title="召回/粗排"></a>召回/粗排</h3><p>推荐系统的召回阶段可以理解为根据用户的历史行为数据，为用户在海量的信息中粗选一批待推荐的内容，挑选出一个小的候选集的过程。粗排用到的很多技术与召回重合，所以放在一起讲，粗排也不是必需的环节，它的功能对召回的结果进行个粗略的排序，在保证一定精准的前提下，进一步减少往后传送的物品数量，这就是粗排的作用。</p>
<p><img src="http://ryluo.oss-cn-chengdu.aliyuncs.com/图片image-20220410000221817.png" alt="在这里插入图片描述"></p>
<p>召回模块面对几百上千万的推荐池物料规模，候选集十分庞大。由于后续有排序模块作为保障，故不需要十分准确，但必须保证不要遗漏和低延迟。目前主要通过多路召回来实现，一方面各路可以并行计算，另一方面取长补短。可以看到各类同类竞品的系统虽然细节上多少存在差异，但不约而同的采取了多路召回的架构，这类设计考虑如下几点问题：</p>
<ol>
<li><strong>考虑用户层面</strong>：用户兴趣的多元化，用户需求与场景的多元化：例如：新闻需求，重大要闻，相关内容沉浸阅读等等</li>
<li><strong>考虑系统层面</strong>：增强系统的鲁棒性；部分召回失效，其余召回队列兜底不会导致整个召回层失效；排序层失效，召回队列兜底不会导致整个推荐系统失效</li>
<li><strong>系统多样性内容分发</strong>：图文、视频、小视频；精准、试探、时效一定比例；召回目标的多元化，例如：相关性，沉浸时长，时效性，特色内容等等</li>
<li><strong>可解释性推荐一部分召回是有明确推荐理由的</strong>：很好的解决产品性数据的引入；</li>
</ol>
<p>介绍了召回任务的目的和场景后，接下来分析召回层面主要的技术栈，因为召回一般都是多路召回，从模型角度分析有很多召回算法，这种一般是在召回层占大部分比例点召回，除此之外，还会有探索类召回、策略运营类召回、社交类召回等。接下来我们着重介绍模型类召回。</p>
<h3 id="经典召回模型"><a href="#经典召回模型" class="headerlink" title="经典召回模型"></a>经典召回模型</h3><p>随着技术发展，在Embedding基础上的模型化召回是一个技术发展潮流方向。这种召回的范式是通过某种算法，对user和item分别打上Embedding，然后user与item在线进行KNN计算实时查询最近邻结果作为召回结果，快速找出匹配的物品。需要注意的是如果召回采用模型召回方法，优化目标最好和排序的优化目标一致，否则可能被过滤掉。</p>
<p>在这方面典型的算法有：FM、双塔DSSM、Multi-View DNN等。</p>
<h3 id="序列召回模型"><a href="#序列召回模型" class="headerlink" title="序列召回模型"></a>序列召回模型</h3><p>推荐系统主要解决的是基于用户的隐式阅读行为来做个性化推荐的问题，序列模型一些基于神经网络模型学习得到Word2Vec模型，再后面的基于RNN的语言模型，最先用的最多的Bert，这些方法都可以应用到召回的学习中。</p>
<p>用户在使用 APP 或者网站的时候，一般会产生一些针对物品的行为，比如点击一些感兴趣的物品，收藏或者互动行为，或者是购买商品等。而一般用户之所以会对物品发生行为，往往意味着这些物品是符合用户兴趣的，而不同类型的行为，可能代表了不同程度的兴趣。比如购买就是比点击更能表征用户兴趣的行为。在召回阶段，如何根据用户行为序列打 embedding，可以采取有监督的模型，比如 Next Item Prediction 的预测方式即可；也可以采用无监督的方式，比如物品只要能打出 embedding，就能无监督集成用户行为序列内容，例如 Sum Pooling。</p>
<p>这方面典型的算法有：CBOW、Skip-Gram、GRU、Bert等。</p>
<h3 id="用户序列拆分"><a href="#用户序列拆分" class="headerlink" title="用户序列拆分"></a>用户序列拆分</h3><p>上文讲了利用用户行为物品序列，打出用户兴趣 Embedding 的做法。但是，另外一个现实是：用户往往是多兴趣的，比如可能同时对娱乐、体育、收藏感兴趣。这些不同的兴趣也能从用户行为序列的物品构成上看出来，比如行为序列中大部分是娱乐类，一部分体育类，少部分收藏类等。那么能否把用户行为序列物品中，这种不同类型的用户兴趣细分，而不是都笼统地打到一个用户兴趣 Embedding 里呢？用户多兴趣拆分就是解决这类更细致刻画用户兴趣的方向。</p>
<p>本质上，把用户行为序列打到多个 embedding 上，实际它是个类似聚类的过程，就是把不同的 Item，聚类到不同的兴趣类别里去。目前常用的拆分用户兴趣 embedding 的方法，主要是胶囊网络和 Memory Network，但是理论上，很多类似聚类的方法应该都是有效的，所以完全可以在这块替换成你自己的能产生聚类效果的方法来做。</p>
<p>这方面典型的算法有：Multi-Interest Network with Dynamic Routing for Recommendation at Tmall等。</p>
<h3 id="知识图谱-1"><a href="#知识图谱-1" class="headerlink" title="知识图谱"></a>知识图谱</h3><p>知识图谱有一个独有的优势和价值，那就是对于推荐结果的可解释性；比如推荐给用户某个物品，可以在知识图谱里通过物品的关键关联路径给出合理解释，这对于推荐结果的解释性来说是很好的，因为知识图谱说到底是人编码出来让自己容易理解的一套知识体系，所以人非常容易理解其间的关系。知识图谱的可解释性往往是和图路径方法关联在一起的，而 Path 类方法，很多实验证明了，在排序角度来看，是效果最差的一类方法，但是它在可解释性方面有很好的效果，所以往往可以利用知识图谱构建一条可解释性的召回通路。</p>
<p>这方面的算法有：KGAT、RippleNet等。</p>
<h3 id="图模型"><a href="#图模型" class="headerlink" title="图模型"></a>图模型</h3><p>推荐系统中User和Item相关的行为、需求、属性和社交信息具有天然的图结构，可以使用一张复杂的异构图来表示整个推荐系统。图神经网络模型推荐就是基于这个想法，把异构网络中包含的结构和语义信息编码到结点Embedding表示中，并使用得到向量进行个性化推荐。知识图谱其实是图神经网络的一个比较特殊的具体实例，但是，知识图谱因为编码的是静态知识，而不是用户比较直接的行为数据，和具体应用距离比较远，这可能是导致两者在推荐领域表现有差异的主要原因。</p>
<p>这方面典型的算法有：GraphSAGE、PinSage等。</p>
<h3 id="精排"><a href="#精排" class="headerlink" title="精排"></a>精排</h3><p>排序模型是推荐系统中涵盖的研究方向最多，有非常多的子领域值得研究探索，这也是推荐系统中技术含量最高的部分，毕竟它是直接面对用户，产生的结果对用户影响最大的一层。目前精排层深度学习已经一统天下了，这是王喆老师《深度学习推荐算法》书中的精排层模型演化线路。具体来看分为DNN、Wide&amp;Deep两大块，实际深入还有序列建模，以及没有提到的多任务建模都是工业界非常常用的，所以我们接下来具体谈论其中每一块的技术栈。</p>
<p><img src="https://s2.loli.net/2024/04/17/xZzb5hRlyasTJM9.png" alt="在这里插入图片描述"></p>
<h3 id="特征交叉模型"><a href="#特征交叉模型" class="headerlink" title="特征交叉模型"></a>特征交叉模型</h3><p>在深度学习推荐算法发展早期，很多论文聚焦于如何提升模型的特征组合和交叉的能力，这其中既包含隐式特征交叉Deep Crossing也有采用显式特征交叉的探究。本质上是希望模型能够摆脱人工先验的特征工程，实现端到端的一套模型。</p>
<p>在早期的推荐系统中，基本是由人工进行特征交叉的，往往凭借对业务的理解和经验，但是费时费力。于是有了很多的这方面的研究，从FM到GBDT+LR都是引入模型进行自动化的特征交叉。再往后就是深度模型，深度模型虽然有万能近似定理，但是真正想要发挥模型的潜力，显式的特征交叉还是必不可少的。</p>
<p>这方面的经典研究工作有：DCN、DeepFM、xDeepFM等；</p>
<h3 id="序列模型"><a href="#序列模型" class="headerlink" title="序列模型"></a>序列模型</h3><p>在推荐系统中，历史行为序列是非常重要的特征。在序列建模中，主要任务目标是得到用户此刻的兴趣向量（user interest vector）。如何刻画用户兴趣的广泛性，是推荐系统比较大的一个难点，用户历史行为序列建模的研究经历了从Pooling、RNN到attention、capsule再到transformer的顺序。在序列模型中，又有很多细分的方向，比如根据用户行为长度有研究用户终身行为序列的，也有聚焦当下兴趣的，还有研究如何抽取序列特征的抽取器，比如研究attention还是胶囊网络。</p>
<p>这方面典型的研究工作有：DIN、DSIN、DIEN、SIM等；</p>
<h3 id="多模态信息融合"><a href="#多模态信息融合" class="headerlink" title="多模态信息融合"></a>多模态信息融合</h3><p>在上文我们提到算法团队往往会利用内容画像信息，既有基于CV也有基于NLP抽取出来的信息。这是非常合理的，我们在逛抖音、淘宝的时候关注的不仅仅item的价格、品牌，同样会关注封面小姐姐好不好看、标题够不够震惊等信息。除此之外，在冷启动场景下，我们能够利用等信息不够多，如果能够使用多模态信息，能很大程度上解决数据稀疏的问题。</p>
<p>传统做法在多模态信息融合就是希望把不同模态信息利用起来，通过Embedding技术融合进模型。在推荐领域，主流的做法还是一套非端到端的体系，由其他模型抽取出多模态信息，推荐只需要融合入这些信息就好了。同时也有其他工作是利用注意力机制等方法来学习不同模态之间的关联，来增强多模态的表示。</p>
<p>比较典型的工作有：Image Matters: Visually modeling user behaviors using Advanced Model Server、UMPR等。</p>
<h3 id="多任务学习"><a href="#多任务学习" class="headerlink" title="多任务学习"></a>多任务学习</h3><p>很多场景下我们模型优化的目标都是CTR，有一些场景只考虑CTR是不够的，点击率模型、时长模型和完播率模型是大部分信息流产品推荐算法团队都会尝试去做的模型。单独优化点击率模型容易推出来标题党，单独优化时长模型可能推出来的都是长视频或长文章，单独优化完播率模型可能短视频短图文就容易被推出来，所以多目标就应运而生。信息流推荐中，我们不仅希望用户点进我们的item，还希望能有一个不错的完播率，即希望用户能看完我们推荐的商品。或者电商场景希望用户不仅点进来，还希望他买下或者加入购物车了。这些概率实际上就是模型要学习的目标，多种目标综合起来，包括阅读、点赞、收藏、分享等等一系列的行为，归纳到一个模型里面进行学习，这就是推荐系统的多目标学习。</p>
<p>这方面比较典型的算法有：ESSM、MMoE、DUPN等。</p>
<h3 id="强化学习"><a href="#强化学习" class="headerlink" title="强化学习"></a>强化学习</h3><p>强化学习与一般有监督的深度学习相比有一些很显著的优势，首先强化学习能够比较灵活的定义优化的业务目标，考虑推荐系统长短期的收益，比如用户留存，在深度模型下，我们很难设计这个指标的优化函数，而强化学习是可以对长期收益下来建模。第二是能够体现用户兴趣的动态变化，比如在新闻推荐下，用户兴趣变化很快，强化学习更容易通过用户行为动态产生推荐结果。最后是EE也就是利用探索机制，这种一种当前和长期收益的权衡，强化学习能够更好的调节这里的回报。</p>
<p>这方面比较典型的算法有：DQN、Reinforcement Learning for Slate-based Recommender Systems: A Tractable Decomposition and Practical Methodology；</p>
<h3 id="跨域推荐"><a href="#跨域推荐" class="headerlink" title="跨域推荐"></a>跨域推荐</h3><p>一般一家公司业务线都是非常多的，比如腾讯既有腾讯视频，也有微信看一看、视频号，还有腾讯音乐，如果能够结合这几个场景的数据，同时进行推荐，一方面对于冷启动是非常有利的，另一方面也能补充更多数据，更好的进行精确推荐。</p>
<p>跨域推荐系统相比一般的推荐系统要更加复杂。在传统推荐系统中，我们只需要考虑建立当前领域内的一个推荐模型进行分析；而在跨域推荐中，我们更要关心在不同领域间要选择何种信息进行迁移，以及如何迁移这些信息，这是跨域推荐系统中非常关键的问题。</p>
<p>这方面典型的模型有：DTCDR、MV-DNN、EMCDR等；</p>
<h3 id="重排序"><a href="#重排序" class="headerlink" title="重排序"></a>重排序</h3><p>我们知道常见的有三种优化目标：Point Wise、Pair Wise 和 List Wise。重排序阶段对精排生成的Top-N个物品的序列进行重新排序，生成一个Top-K个物品的序列，作为排序系统最后的结果，直接展现给用户。重排序的原因是因为多个物品之间往往是相互影响的，而精排序是根据PointWise得分，容易造成推荐结果同质化严重，有很多冗余信息。而重排序面对的挑战就是海量状态空间如何求解的问题，一般在精排层我们使用AUC作为指标，但是在重排序更多关注NDCG等指标。</p>
<p>重排序在业务中，还会根据一些策略、运营规则参与排序，比如强制去重、间隔排序、流量扶持等，但是总计趋势上看还是算法排序越来越占据主流趋势。重排序更多的是List Wise作为优化目标的，它关注的是列表中商品顺序的问题来优化模型，但是一般List Wise因为状态空间大，存在训练速度慢的问题。这方面典型的做法，基于RNN、Transformer、强化学习的都有，这方面因为不是推荐的一个核心，所以没有展开来讲，而且这一块比较依赖实际的业务场景。</p>
<p>这里的经典算法有：MRR、DPP、RNN等；</p>
<h3 id="工程"><a href="#工程" class="headerlink" title="工程"></a>工程</h3><p>推荐系统的实现需要依托工程，很多研究界Paper的idea满天飞，却忽视了工业界能否落地，进入工业界我们很难或者很少有组是做纯research的，所以我们同样有很多工程技术需要掌握。下面列举了在推荐中主要用到的工程技术：</p>
<ul>
<li><strong>编程语言</strong>：Python、Java（scala）、C++、sql、shell；</li>
<li><strong>机器学习</strong>：Tensorflow/Pytorch、GraphLab/GraphCHI、LGB/Xgboost、SKLearn；</li>
<li><strong>数据分析</strong>：Pandas、Numpy、Seaborn、Spark；</li>
<li>数据存储：mysql、redis、mangodb、hive、kafka、es、hbase；</li>
<li>相似计算：annoy、faiss、kgraph</li>
<li>流计算：Spark Streaming、Flink</li>
<li>分布式：Hadoop、Spark</li>
</ul>
<p>上面那么多技术，我内容最重要的就是加粗的三部分，第一是语言：必须掌握的是Python，C++和JAVA中根据不同的组使用的是不同的语言，这个如果没有时间可以等进组后慢慢学习。然后是机器学习框架：Tensorflow和Pytorch至少要掌握一个吧，前期不用纠结学哪个，这个迁移成本很低，基本能够达到触类旁通，而且面试官不会为难你只会这个不会那个。最后是数据分析工具：Pandas是我们处理单机规模数据的利器，但是进入工业界，Hadoop和Spark是需要会用的，不过不用学太深，会用即可。</p>
<h3 id="总结-1"><a href="#总结-1" class="headerlink" title="总结"></a>总结</h3><p>本文从算法和工程两个角度分析了推荐系统的一个技术栈，但是还有很多方向遗漏，也有很多方向受限于现在的技术水平深度不够和有错误的情况，后续会不断补充和更正。</p>
<p>所以技术栈我列出的是一个非常广度的技术，实际上每一个技术钻研下去都需要非常多时间，而且不一定是你实际工作中会遇到的，所以不要被那么多技术吓到，也要避免陷入技术细节的海洋中。</p>
<p>我和非常多的大厂面试官讨论过技术深度和广度的问题，得出来的结论是对于入门的推荐算法工程师而言，实际上深度和广度的要求取决于你要去的组，有些组有很深的推荐技术沉淀，有很强的工程师团队，这样的组就会希望候选者能够在某个方面有比较深入的研究，这个方面既包含工程方面也包含研究方面。但是如果是比较新的组、或者技术沉淀不深、推荐不是主要任务的组，对深度要求就不会很高。总而言之，我认为对于应届生/实习生来说，在推荐最重要的工程技术/研究方向，至少在召回和排序模块，需要选一个作为方向，是需要较深钻研。对于其他技术/研究方向需要有一定了解，比如可以没用过强化学习，但是要知道强化学习能够在推荐中解决什么问题，剩下的可以等到真实<strong>遇到需要后再去学习</strong>。</p>
<p><strong>参考资料</strong></p>
<ul>
<li><a target="_blank" rel="noopener" href="https://mp.weixin.qq.com/s/aaOosZ57qJpIU6cma820Xw">万字入门推荐系统</a></li>
<li><a target="_blank" rel="noopener" href="https://mp.weixin.qq.com/s?__biz=MzU1NTMyOTI4Mw==&amp;mid=2247496363&amp;idx=1&amp;sn=0d2b2ac176e2a72eb2e760b7b591788f&amp;chksm=fbd740c7cca0c9d16c76fdeb1a874a53f7408d8125b2e1bed3173ecb69d131167c1c9c35c71f&amp;scene=21#wechat_redirect">张俊林：技术演进趋势：召回-&gt;排序-&gt;重排</a></li>
<li><a target="_blank" rel="noopener" href="https://mp.weixin.qq.com/s?__biz=MzU1NTMyOTI4Mw==&amp;mid=2247503484&amp;idx=2&amp;sn=e2a2cdd3a517ab09e903e69ccb1e9f94&amp;chksm=fbd77c10cca0f50642dde47439ed919aa2e61b7ff57bc4cbaacc3acaac3c620a1ed6f92684ab&amp;scene=21#wechat_redirect">微信”看一看”多模型内容策略与召回</a></li>
<li><a target="_blank" rel="noopener" href="https://mp.weixin.qq.com/s/u_5RdZ-BcIu_RoWNri76ig">多目标学习在推荐系统中的应用</a></li>
<li><a target="_blank" rel="noopener" href="https://mp.weixin.qq.com/s?__biz=MjM5NjQ5MTI5OA==&amp;mid=2651749434&amp;idx=2&amp;sn=343e811408542dd1984582b8639240a6&amp;chksm=bd12a5778a652c61ed4297f1a17582cad4ca6b8e8d4d66843f169e0eda9f6aede988bc675743&amp;mpshare=1&amp;scene=23&amp;srcid=1115EcgbMw6GAhMnzV0URvgd#rd">强化学习在美团“猜你喜欢”的实践</a></li>
<li><a target="_blank" rel="noopener" href="https://mp.weixin.qq.com/s/YorzRyK0iplzqutnhEhrvw">推荐系统技术演进趋势：重排篇</a></li>
<li><a target="_blank" rel="noopener" href="https://mp.weixin.qq.com/s/ylavFA_MXLUhIBLCqxAjLQ">阿里强化学习重排实践</a></li>
</ul>
<h1 id="第二章-推荐系统算法基础"><a href="#第二章-推荐系统算法基础" class="headerlink" title="第二章 推荐系统算法基础"></a>第二章 推荐系统算法基础</h1><h2 id="2-1经典召回模型"><a href="#2-1经典召回模型" class="headerlink" title="2.1经典召回模型"></a>2.1经典召回模型</h2><h3 id="2-1-1基于协同过滤的召回"><a href="#2-1-1基于协同过滤的召回" class="headerlink" title="2.1.1基于协同过滤的召回"></a>2.1.1基于协同过滤的召回</h3><h3 id="核心思想"><a href="#核心思想" class="headerlink" title="核心思想"></a>核心思想</h3><p>协同过滤（Collaborative Filtering）推荐算法是最经典、最常用的推荐算法。基本思想是：</p>
<ul>
<li>根据用户之前的喜好以及其他兴趣相近的用户的选择来给用户推荐物品。<ul>
<li>基于对用户历史行为数据的挖掘发现用户的喜好偏向， 并预测用户可能喜好的产品进行推荐。</li>
<li>一般是仅仅基于用户的行为数据（评价、购买、下载等）, 而不依赖于项的任何附加信息（物品自身特征）或者用户的任何附加信息（年龄， 性别等）。</li>
</ul>
</li>
<li>目前应用比较广泛的协同过滤算法是基于邻域的方法，主要有：<ul>
<li>基于用户的协同过滤算法（UserCF）：给用户推荐和他兴趣相似的其他用户喜欢的产品。</li>
<li>基于物品的协同过滤算法（ItemCF）：给用户推荐和他之前喜欢的物品相似的物品。</li>
</ul>
</li>
</ul>
<p><strong>不管是 UserCF 还是 ItemCF 算法， 重点是计算用户之间（或物品之间）的相似度。</strong></p>
<h3 id="相似度度量方法"><a href="#相似度度量方法" class="headerlink" title="相似度度量方法"></a>相似度度量方法</h3><ol>
<li><p><strong>杰卡德（Jaccard）相似系数</strong></p>
<p><code>Jaccard</code> 系数是衡量两个集合的相似度一种指标，计算公式如下：</p>
<script type="math/tex; mode=display">
sim_{uv} = \frac{\left| N(u) \cap N(v) \right|}{\left| N(u) \cup N(v) \right|}</script></li>
</ol>
<ul>
<li>其中 $N(u)$，$N(v)$分别表示用户$u$和用户交$v$互物品的集合。</li>
<li><p>对于用户$u$ 和$v$，该公式反映了两个交互物品交集的数量占这两个用户交互物品并集的数量的比例。</p>
<p>由于杰卡德相似系数一般无法反映具体用户的评分喜好信息，所以常用来评估用户是否会对某物品进行打分， 而不是预估用户会对某物品打多少分。</p>
</li>
</ul>
<ol>
<li><p><strong>余弦相似度</strong> 余弦相似度衡量了两个向量的夹角，夹角越小越相似。余弦相似度的计算如下，其与杰卡德（Jaccard）相似系数只是在分母上存在差异：</p>
<script type="math/tex; mode=display">
sim_{uv} = \frac{\left| N(u) \cap N(v) \right|}{\sqrt{\left| N(u) \cdot N(v) \right|}}</script><p>从向量的角度进行描述，令矩阵$A$ 为用户-物品交互矩阵，矩阵的行表示用户，列表示物品。</p>
<ul>
<li><p>设用户和物品数量分别为$m$, $n$，交互矩阵$A$就是一个$m$行$n$列的矩阵。</p>
</li>
<li><p>矩阵中的元素均为$ 0/1$。若用户$i$对物品$j$存在交互，那么 $A_{ij} =1$，否则为 $0$。</p>
</li>
<li><p>那么，用户之间的相似度可以表示为：</p>
<script type="math/tex; mode=display">
sim_{uv} = cos(u, v) = \frac{u \cdot v}{\left| u \right| \cdot \left| v \right|}</script></li>
<li><p>向量 $u$,$v$在形式都是 one-hot 类型，$u \cdot v$表示向量点积。            </p>
</li>
</ul>
<p>上述用户-物品交互矩阵在现实中是十分稀疏的，为了节省内存，交互矩阵会采用字典进行存储。在 <code>sklearn</code> 中，余弦相似度的实现：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.metrics.pairwise <span class="keyword">import</span> cosine_similarity</span><br><span class="line"></span><br><span class="line">i = [<span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>]</span><br><span class="line">j = [<span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>]</span><br><span class="line">cosine_similarity([i, j])</span><br><span class="line"></span><br></pre></td></tr></table></figure>
</li>
</ol>
<ol>
<li><p><strong>皮尔逊相关系数</strong></p>
<p>在用户之间的余弦相似度计算时，将用户向量的内积展开为各元素乘积和：</p>
<script type="math/tex; mode=display">
sim_{uv} = \frac{\sum_ir_{ui} * r_{vi}}{\sqrt{\sum_i r_{ui}^{2} \sqrt{\sum_i r_{vi}^{2}}}}</script><ul>
<li>其中，$r<em>{ui},r</em>{vi}$分别表示用户$u$和用户$v$对物品 $i$是否有交互(或具体评分值)。</li>
</ul>
<p>皮尔逊相关系数与余弦相似度的计算公式非常的类似，如下：</p>
<script type="math/tex; mode=display">
sim(u, v) = \frac{\sum_{i \in I}(r_{ui} - \bar r_u )(r_{vi} - \bar r_v)}{\sqrt{\sum_{i \in I}(r_{ui} - \bar r_u )^{2}} \sqrt{\sum_{i \in I}(r_{vi} - \bar r_v)^{2}}}</script><ul>
<li>其中，$r<em>{ui}, r</em>{vi}$分别表示用户$u$和用户$i$对物品$i$是否有交互(或具体评分值)；</li>
<li>$\bar r_u, \bar r_v$分别表示用户$u$和用户$v$交互的所有物品交互数量或者评分的平均值；</li>
</ul>
<p>相较于余弦相似度，皮尔逊相关系数通过使用用户的平均分对各独立评分进行修正，减小了用户评分偏置的影响。在<code>scipy</code>中，皮尔逊相关系数的实现：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> scipy.stats <span class="keyword">import</span> pearsonr</span><br><span class="line"></span><br><span class="line">i = [<span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>]</span><br><span class="line">j = [<span class="number">1</span>, <span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">0</span>]</span><br><span class="line">pearsonr(i, j)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
</li>
</ol>
<h3 id="适用场景"><a href="#适用场景" class="headerlink" title="适用场景"></a>适用场景</h3><ul>
<li><em>Jaccard</em> 相似度表示两个集合的交集元素个数在并集中所占的比例 ，所以适用于隐式反馈数据（0-1）。</li>
<li>余弦相似度在度量文本相似度、用户相似度、物品相似度的时候都较为常用。</li>
<li>皮尔逊相关度，实际上也是一种余弦相似度。不过先对向量做了中心化，范围在 −1−1 到 11。<ul>
<li>相关度量的是两个变量的变化趋势是否一致，两个随机变量是不是同增同减。</li>
<li>不适合用作计算布尔值向量（0-1）之间相关度。</li>
</ul>
</li>
</ul>
<h3 id="基于用户的协同过滤"><a href="#基于用户的协同过滤" class="headerlink" title="基于用户的协同过滤"></a>基于用户的协同过滤</h3><h4 id="核心思想-1"><a href="#核心思想-1" class="headerlink" title="核心思想"></a>核心思想</h4><p>基于用户的协同过滤（UserCF）：</p>
<ul>
<li><p>例如，我们要对用户$A$进行物品推荐，可以先找到和他有相似兴趣的其他用户。</p>
</li>
<li><p>然后，将共同兴趣用户喜欢的，但用户$A$未交互过的物品推荐给 $A$。</p>
<p><img src="https://s2.loli.net/2024/04/11/bZEseFk2vx4RM7q.png" alt="image-20240411110402334"></p>
</li>
</ul>
<h3 id="计算过程"><a href="#计算过程" class="headerlink" title="计算过程"></a>计算过程</h3><p>以下图为例，给用户推荐物品的过程可以形象化为：预测用户对物品进行打分的任务，表格里面是5个用户对于5件物品的一个打分情况，就可以理解为用户对物品的喜欢程度。</p>
<p><img src="https://s2.loli.net/2024/04/11/n1FNf8TB9seOc7S.png" alt="image-20240411110604527"></p>
<p>UserCF算法的两个步骤：</p>
<ul>
<li>首先，根据前面的这些打分情况(或者说已有的用户向量）计算一下 Alice 和用户1， 2， 3， 4的相似程度， 找出与 Alice 最相似的 n 个用户。</li>
<li>根据这 n 个用户对物品 5 的评分情况和与 Alice 的相似程度会猜测出 Alice 对物品5的评分。如果评分比较高的话， 就把物品5推荐给用户 Alice， 否则不推荐。</li>
</ul>
<h3 id="具体过程"><a href="#具体过程" class="headerlink" title="具体过程"></a>具体过程</h3><ol>
<li><p>计算用户之间的相似度</p>
<ul>
<li>根据 1.2 节的几种方法， 我们可以计算出各用户之间的相似程度。对于用户 Alice，选取出与其最相近的$N$个用户。</li>
</ul>
</li>
<li><p>计算用户对新物品的评分预测</p>
<ul>
<li>常用的方式之一：利用目标用户与相似用户之间的相似度以及相似用户对物品的评分，来预测目标用户对候选物品的评分估计：</li>
</ul>
<script type="math/tex; mode=display">
R_{u,p} = \frac{\sum_{s \in S}(w_{u, s} \cdot R_{s, p})}{\sum_{s \in S}w_{u, s}}</script><ul>
<li><p>其中，权重 $w<em>{u, s}$是用户$u$和用户$s$ 的相似度， $R</em>{s, p}$是用户$s$对物品$p$的评分。</p>
</li>
<li><p>另一种方式：考虑到用户评分的偏置，即有的用户喜欢打高分， 有的用户喜欢打低分的情况。公式如下：</p>
<script type="math/tex; mode=display">
R_{u,p} =  \bar R_u +\frac{\sum_{s \in S}(w_{u, s} \cdot (R_{s, p} - \bar R_s))}{\sum_{s \in S}w_{u, s}}</script><ul>
<li>其中，$\bar R_s$表示用户$s$对物品的历史平均评分。</li>
</ul>
</li>
</ul>
</li>
<li><p>对用户进行物品推荐</p>
<ul>
<li>在获得用户$u$ 对不同物品的评价预测后， 最终的推荐列表根据预测评分进行排序得到。</li>
</ul>
</li>
</ol>
<h3 id="手动计算"><a href="#手动计算" class="headerlink" title="手动计算"></a>手动计算</h3><p>根据上面的问题， 下面手动计算 Alice 对物品 5 的得分：</p>
<ol>
<li><p>计算 Alice 与其他用户的相似度（基于皮尔逊相关系数）</p>
<ul>
<li>手动计算 Alice 与用户 1 之间的相似度：</li>
</ul>
<blockquote>
<ul>
<li><p><strong>用户向量</strong>$Alice:(5,3,4,4),user1:(3,1,2,3),user2:(4,3,4,3),user3:(3,3,1,5),user4:(1,5,5,2)Alice:(5,3,4,4),user1:(3,1,2,3),user2:(4,3,4,3),user3:(3,3,1,5),user4:(1,5,5,2)$</p>
</li>
<li><p><strong>计算Alice与user1的余弦相似性:</strong></p>
</li>
</ul>
<script type="math/tex; mode=display">
sim(Alice, user_1) = cos(Alice, user_1) = \frac{5*3 + 3*1+4*2+4*3}{\sqrt(5^2+3^2+4^2+4^2)*\sqrt(3^2+1^2+2^2+3^2)}</script><ul>
<li><strong>计算Alice与user1皮尔逊相关系数:</strong><ul>
<li>计算均值：$Alice<em>{ave}=4, user</em>{1ave}=2.25$</li>
<li>各向量减去均值：$Alice:(1,-1,0,0), user_1:(0.75,-1.25,-0.25,0.75)$</li>
<li>最后计算两个新向量的余弦相似度，与上面的计算过程一致，结果是0.852</li>
</ul>
</li>
</ul>
</blockquote>
<ul>
<li><p>基于 sklearn 计算所有用户之间的皮尔逊相关系数。可以看出，与 Alice 相似度最高的用户为用户1和用户2。</p>
<p><img src="https://s2.loli.net/2024/04/11/c8hMAeuiIvyJgxp.png" alt="image-20240411113155651"></p>
</li>
</ul>
</li>
<li><p><strong>根据相似度用户计算 Alice对物品5的最终得分</strong> 用户1对物品5的评分是3， 用户2对物品5的打分是5， 那么根据上面的计算公式， 可以计算出 Alice 对物品5的最终得分是</p>
<script type="math/tex; mode=display">
P_{Alice, item_5} = \bar R_{Alice} + \frac{\sum_{k=1}^2 (w_{Alice, userk}(R_{userk, item_5} - \bar R_{userk}))}{\sum_{k=1}^2 w_{Alice, userk}}=4+\frac{0.85*(3-2.4)+0.7*(5-3.8)}{0.85+0.7}=4.87</script></li>
<li><p><strong>根据用户评分对用户进行推荐</strong></p>
<ul>
<li>根据 Alice 的打分对物品排个序从大到小：<script type="math/tex; mode=display">
物品1>物品5>物品3=物品4>物品2</script></li>
</ul>
</li>
</ol>
<ul>
<li>如果要向 Alice 推荐2款产品的话， 我们就可以推荐物品 1 和物品 5 给 Alice。</li>
</ul>
<h3 id="UserCF编程实现"><a href="#UserCF编程实现" class="headerlink" title="UserCF编程实现"></a>UserCF编程实现</h3><ol>
<li><p>建立实验使用的数据表：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">loadData</span>():</span><br><span class="line">    users = &#123;<span class="string">&#x27;Alice&#x27;</span>: &#123;<span class="string">&#x27;A&#x27;</span>: <span class="number">5</span>, <span class="string">&#x27;B&#x27;</span>: <span class="number">3</span>, <span class="string">&#x27;C&#x27;</span>: <span class="number">4</span>, <span class="string">&#x27;D&#x27;</span>: <span class="number">4</span>&#125;,</span><br><span class="line">             <span class="string">&#x27;user1&#x27;</span>: &#123;<span class="string">&#x27;A&#x27;</span>: <span class="number">3</span>, <span class="string">&#x27;B&#x27;</span>: <span class="number">1</span>, <span class="string">&#x27;C&#x27;</span>: <span class="number">2</span>, <span class="string">&#x27;D&#x27;</span>: <span class="number">3</span>, <span class="string">&#x27;E&#x27;</span>: <span class="number">3</span>&#125;,</span><br><span class="line">             <span class="string">&#x27;user2&#x27;</span>: &#123;<span class="string">&#x27;A&#x27;</span>: <span class="number">4</span>, <span class="string">&#x27;B&#x27;</span>: <span class="number">3</span>, <span class="string">&#x27;C&#x27;</span>: <span class="number">4</span>, <span class="string">&#x27;D&#x27;</span>: <span class="number">3</span>, <span class="string">&#x27;E&#x27;</span>: <span class="number">5</span>&#125;,</span><br><span class="line">             <span class="string">&#x27;user3&#x27;</span>: &#123;<span class="string">&#x27;A&#x27;</span>: <span class="number">3</span>, <span class="string">&#x27;B&#x27;</span>: <span class="number">3</span>, <span class="string">&#x27;C&#x27;</span>: <span class="number">1</span>, <span class="string">&#x27;D&#x27;</span>: <span class="number">5</span>, <span class="string">&#x27;E&#x27;</span>: <span class="number">4</span>&#125;,</span><br><span class="line">             <span class="string">&#x27;user4&#x27;</span>: &#123;<span class="string">&#x27;A&#x27;</span>: <span class="number">1</span>, <span class="string">&#x27;B&#x27;</span>: <span class="number">5</span>, <span class="string">&#x27;C&#x27;</span>: <span class="number">5</span>, <span class="string">&#x27;D&#x27;</span>: <span class="number">2</span>, <span class="string">&#x27;E&#x27;</span>: <span class="number">1</span>&#125;</span><br><span class="line">             &#125;</span><br><span class="line">    <span class="keyword">return</span> users</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<ul>
<li>这里使用字典来建立用户-物品的交互表。<ul>
<li>字典<code>users</code>的键表示不同用户的名字，值为一个评分字典，评分字典的键值对表示某物品被当前用户的评分。</li>
<li>由于现实场景中，用户对物品的评分比较稀疏。如果直接使用矩阵进行存储，会存在大量空缺值，故此处使用了字典。</li>
</ul>
</li>
</ul>
</li>
<li><p>计算用户相似性矩阵</p>
<ul>
<li>由于训练数据中共包含 5 个用户，所以这里的用户相似度矩阵的维度也为$5 * 5$。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">user_data = loadData()</span><br><span class="line">similarity_matrix = pd.DataFrame(</span><br><span class="line">	np.identity(<span class="built_in">len</span>(user_data)), <span class="comment">#创建对角矩阵，对角线为1，其余为0</span></span><br><span class="line">	index=user_data.keys(),</span><br><span class="line">	columns=user_data.keys(),</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 遍历每条用户-物品评分数据</span></span><br><span class="line"><span class="keyword">for</span> u1, item1 <span class="keyword">in</span> user_data.items():</span><br><span class="line">	<span class="keyword">for</span> u2, item2 <span class="keyword">in</span> user_data.items():</span><br><span class="line">		<span class="keyword">if</span> u1 == u2:</span><br><span class="line">			<span class="keyword">continue</span></span><br><span class="line">		vec1, vec2 = [], []</span><br><span class="line">		<span class="keyword">for</span> item, rating1 <span class="keyword">in</span> item1.items():</span><br><span class="line">			rating2 = item2.get(item, -<span class="number">1</span>)</span><br><span class="line">			<span class="keyword">if</span> rating2 == -<span class="number">1</span>:</span><br><span class="line">				<span class="keyword">continue</span></span><br><span class="line">			vec1.append(rating1)</span><br><span class="line">			vec2.append(rating2)</span><br><span class="line">			<span class="comment"># 计算不同用户之间的皮尔逊相关系数</span></span><br><span class="line">			similarity_matrix[u1][u2] = np.corrcoef(vec1, vec2)[<span class="number">0</span>][<span class="number">1</span>]</span><br><span class="line">  </span><br><span class="line"><span class="built_in">print</span>(similarity_matrix)</span><br></pre></td></tr></table></figure>
</li>
</ol>
   <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">          <span class="number">1</span>         <span class="number">2</span>         <span class="number">3</span>         <span class="number">4</span>         <span class="number">5</span></span><br><span class="line"><span class="number">1</span>  <span class="number">1.000000</span>  <span class="number">0.852803</span>  <span class="number">0.707107</span>  <span class="number">0.000000</span> -<span class="number">0.792118</span></span><br><span class="line"><span class="number">2</span>  <span class="number">0.852803</span>  <span class="number">1.000000</span>  <span class="number">0.467707</span>  <span class="number">0.489956</span> -<span class="number">0.900149</span></span><br><span class="line"><span class="number">3</span>  <span class="number">0.707107</span>  <span class="number">0.467707</span>  <span class="number">1.000000</span> -<span class="number">0.161165</span> -<span class="number">0.466569</span></span><br><span class="line"><span class="number">4</span>  <span class="number">0.000000</span>  <span class="number">0.489956</span> -<span class="number">0.161165</span>  <span class="number">1.000000</span> -<span class="number">0.641503</span></span><br><span class="line"><span class="number">5</span> -<span class="number">0.792118</span> -<span class="number">0.900149</span> -<span class="number">0.466569</span> -<span class="number">0.641503</span>  <span class="number">1.000000</span></span><br></pre></td></tr></table></figure>
<ol>
<li><p>计算与 Alice 最相似的 <code>num</code> 个用户</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">target_user = <span class="string">&#x27;Alice&#x27;</span></span><br><span class="line">num = <span class="number">2</span></span><br><span class="line"><span class="comment"># 由于最相似的用户为自己，忽略本身</span></span><br><span class="line">sim_users = similarity_matrix[target_user].sort_values(ascend=<span class="literal">False</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;与用户<span class="subst">&#123;target_user&#125;</span>最相似的<span class="subst">&#123;num&#125;</span>个用户为：<span class="subst">&#123;sim_users&#125;</span>&#x27;</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">与用户 Alice 最相似的<span class="number">2</span>个用户为：[<span class="string">&#x27;user1&#x27;</span>, <span class="string">&#x27;user2&#x27;</span>]</span><br></pre></td></tr></table></figure>
</li>
<li><p>预测用户 Alice 对物品 <code>E</code> 的评分</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">weighted_scores = <span class="number">0.</span></span><br><span class="line">cor_values_sum = <span class="number">0.</span></span><br><span class="line"></span><br><span class="line">target_item = <span class="string">&#x27;E&#x27;</span></span><br><span class="line"><span class="comment"># 基于皮尔逊相关稀疏预测用户评分</span></span><br><span class="line"><span class="keyword">for</span> user <span class="keyword">in</span> sim_users:</span><br><span class="line">    corr_value = similaity_matrix[target_user][user]</span><br><span class="line">    user_mean_rating = np.mean(<span class="built_in">list</span>(user_data[user].values()))</span><br><span class="line">    </span><br><span class="line">    weighted_scores += corr_value * (user_data[user][target_item] - user_mean_rating)</span><br><span class="line">    corr_values_sum += corr_value</span><br><span class="line">    </span><br><span class="line">target_user_mean_rating = np.mean(<span class="built_in">list</span>(user_data[target_user].values()))</span><br><span class="line">target_item_pred = target_user_mean_rating + weighted_scores / corr_values_sum</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;用户<span class="subst">&#123;target_user&#125;</span>对物品<span class="subst">&#123;target_item&#125;</span>的预测评分为：<span class="subst">&#123;target_item_pred&#125;</span>&#x27;</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">用户 Alice 对物品E的预测评分为：<span class="number">4.871979899370592</span></span><br></pre></td></tr></table></figure>
</li>
</ol>
<h3 id="UserCF优缺点"><a href="#UserCF优缺点" class="headerlink" title="UserCF优缺点"></a>UserCF优缺点</h3><p>User-based算法存在两个重大问题：</p>
<ol>
<li>数据稀疏性<ul>
<li>一个大型的电子商务推荐系统一般有非常多的物品，用户可能买的其中不到1%的物品，不同用户之间买的物品重叠性较低，导致算法无法找到一个用户的邻居，即偏好相似的用户。</li>
<li>这导致UserCF不适用于那些正反馈获取较困难的应用场景(如酒店预订， 大件物品购买等低频应用)。</li>
</ul>
</li>
<li>算法扩展性<ol>
<li>基于用户的协同过滤需要维护用户相似度矩阵以便快速的找出$TopN$相似用户， 该矩阵的存储开销非常大，存储空间随着用户数量的增加而增加。</li>
<li>故不适合用户数据量大的情况使用。</li>
</ol>
</li>
</ol>
<p>由于UserCF技术上的两点缺陷， 导致很多电商平台并没有采用这种算法， 而是采用了ItemCF算法实现最初的推荐系统。</p>
<h2 id="算法评估"><a href="#算法评估" class="headerlink" title="算法评估"></a>算法评估</h2><p>由于UserCF和ItemCF结果评估部分是共性知识点， 所以在这里统一标识。</p>
<h3 id="召回率"><a href="#召回率" class="headerlink" title="召回率"></a>召回率</h3><p>对用户$u$推荐 $N$个物品记为,$R(u)$ 令用户在$u$测试集上喜欢的物品集合为$T(u)$， 那么召回率定义为：</p>
<script type="math/tex; mode=display">
Recall=\frac{\sum_u \left| R(u) \cap T(u) \right|}{\sum_u \left| T(u) \right|}</script><ul>
<li>含义：<strong>在模型召回预测的物品中，预测准确的物品占用户实际喜欢的物品的比例</strong>。</li>
</ul>
<h3 id="精确率"><a href="#精确率" class="headerlink" title="精确率"></a>精确率</h3><p>精确率定义为：</p>
<script type="math/tex; mode=display">
Precision = \frac{\sum_u \left| R(u) \cap T(u) \right|}{\sum_u \left| R(u) \right|}</script><ul>
<li>含义：<strong>推荐的物品中，对用户准确推荐的物品占总物品的比例</strong>。</li>
<li>如要确保召回率高，一般是推荐更多的物品，期望推荐的物品中会涵盖用户喜爱的物品。而实际中，推荐的物品中用户实际喜爱的物品占少数，推荐的精确率就会很低。故同时要确保高召回率和精确率往往是矛盾的，所以实际中需要在二者之间进行权衡。</li>
</ul>
<h3 id="覆盖率"><a href="#覆盖率" class="headerlink" title="覆盖率"></a>覆盖率</h3><p><strong>覆盖率反映了推荐算法发掘长尾的能力， 覆盖率越高， 说明推荐算法越能将长尾中的物品推荐给用户。</strong></p>
<script type="math/tex; mode=display">
Coverage=\frac{\left| U_{u \in U} R(u) \right|}{\left| I \right|}</script><ul>
<li>含义：<strong>推荐系统能够推荐出来的物品占总物品集合的比例</strong>。<ul>
<li>其中$I$表示所有物品的个数；</li>
<li>系统的用户集合为$U$;</li>
<li>推荐系统给每个用户推荐一个长度为$N$的物品列表$R(u)$.</li>
</ul>
</li>
<li>覆盖率表示最终的推荐列表中包含多大比例的物品。如果所有物品都被给推荐给至少一个用户， 那么覆盖率是100%。</li>
</ul>
<h3 id="新颖度"><a href="#新颖度" class="headerlink" title="新颖度"></a>新颖度</h3><p>用推荐列表中物品的平均流行度度量推荐结果的新颖度。 如果推荐出的物品都很热门， 说明推荐的新颖度较低。 由于物品的流行度分布呈长尾分布， 所以为了流行度的平均值更加稳定， 在计算平均流行度时对每个物品的流行度取对数。</p>
<ul>
<li>O’scar Celma 在博士论文 “<a target="_blank" rel="noopener" href="http://mtg.upf.edu/static/media/PhD_ocelma.pdf">Music Recommendation and Discovery in the Long Tail</a> “ 中研究了新颖度的评测。</li>
</ul>
<h3 id="基于物品的协同过滤"><a href="#基于物品的协同过滤" class="headerlink" title="基于物品的协同过滤"></a>基于物品的协同过滤</h3><h3 id="基本思想"><a href="#基本思想" class="headerlink" title="基本思想"></a>基本思想</h3><p>基于物品的协同过滤（ItemCF）：</p>
<ul>
<li>预先根据所有用户的历史行为数据，计算物品之间的相似性。</li>
<li>然后，把与用户喜欢的物品相类似的物品推荐给用户。</li>
</ul>
<p>举例来说，如果用户 1 喜欢物品 A ，而物品 A 和 C 非常相似，则可以将物品 C 推荐给用户1。ItemCF算法并不利用物品的内容属性计算物品之间的相似度， 主要通过分析用户的行为记录计算物品之间的相似度， 该算法认为， 物品 A 和物品 C 具有很大的相似度是因为喜欢物品 A 的用户极可能喜欢物品 C。</p>
<p><img src="C:/Users/YL.YL.000/AppData/Roaming/Typora/typora-user-images/image-20240427215458380.png" alt="image-20240427215458380"></p>
<h3 id="计算过程-1"><a href="#计算过程-1" class="headerlink" title="计算过程"></a>计算过程</h3><p>基于物品的协同过滤算法和基于用户的协同过滤算法很像， 所以我们这里直接还是拿上面 Alice 的那个例子来看。</p>
<p><img src="https://s2.loli.net/2024/04/27/8viM1fWGdKuEXN3.png" alt="image-20240427215540918"></p>
<p>如果想知道 Alice 对物品5打多少分， 基于物品的协同过滤算法会这么做：</p>
<ul>
<li>首先计算一下物品5和物品1， 2， 3， 4之间的相似性。</li>
<li>在Alice找出与物品 5 最相近的 n 个物品。</li>
<li>根据 Alice 对最相近的 n 个物品的打分去计算对物品 5 的打分情况。</li>
</ul>
<p><strong>手动计算</strong>：</p>
<ol>
<li>手动计算物品之间的相似度</li>
</ol>
<blockquote>
<p><strong>物品向量</strong>：</p>
<p>物品1(3,4,3,1),物品2(1,3,3,5),物品3(2,4,1,5),物品4(3,3,5,2),物品5(3,5,4,1)</p>
<ul>
<li><strong>下面计算物品 5 和物品 1 之间的余弦相似性:</strong><script type="math/tex; mode=display">
sim(物品1,物品5)=\frac{3*3+4*5+3*4+1*1}{\sqrt{9+16+9+1}*\sqrt{9+25+16+1}}</script></li>
</ul>
<ul>
<li><strong>皮尔逊相关系数类似。</strong></li>
</ul>
</blockquote>
<ol>
<li>基于 <code>sklearn</code> 计算物品之间的皮尔逊相关系数：</li>
</ol>
<p><img src="https://s2.loli.net/2024/04/27/Ma1HrBGQpk3xsdy.png" alt="image-20240427220522091"></p>
<ol>
<li>根据皮尔逊相关系数， 可以找到与物品5最相似的2个物品是 item1 和 item4， 下面基于上面的公式计算最终得分：</li>
</ol>
<script type="math/tex; mode=display">
P_{Alice,物品5}=\bar R_{物品5}+\frac{\sum_{k=1}^2 (w_{物品5,物品k} (R_{Alice,物品k-\bar R_{物品k}}))}{\sum_{k=1}^2 w_{物品k,物品5}} \\
=\frac{13}{4}+\frac{0.97*(5-3.2)+0.58*(4-3.4)}{0.97+0.58} \\ =4.6</script><h3 id="ItemCF编程实现"><a href="#ItemCF编程实现" class="headerlink" title="ItemCF编程实现"></a>ItemCF编程实现</h3><ol>
<li><p>构建物品-用户的评分矩阵</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">loadData</span>():</span><br><span class="line">    items = &#123;</span><br><span class="line">        <span class="string">&#x27;A&#x27;</span>: &#123;<span class="string">&#x27;Alice&#x27;</span>: <span class="number">5.0</span>, <span class="string">&#x27;user1&#x27;</span>: <span class="number">3.0</span>, <span class="string">&#x27;user2&#x27;</span>: <span class="number">4.0</span>, <span class="string">&#x27;user3&#x27;</span>: <span class="number">3.0</span>, <span class="string">&#x27;user4&#x27;</span>: <span class="number">1.0</span>&#125;,</span><br><span class="line">             <span class="string">&#x27;B&#x27;</span>: &#123;<span class="string">&#x27;Alice&#x27;</span>: <span class="number">3.0</span>, <span class="string">&#x27;user1&#x27;</span>: <span class="number">1.0</span>, <span class="string">&#x27;user2&#x27;</span>: <span class="number">3.0</span>, <span class="string">&#x27;user3&#x27;</span>: <span class="number">3.0</span>, <span class="string">&#x27;user4&#x27;</span>: <span class="number">5.0</span>&#125;,</span><br><span class="line">             <span class="string">&#x27;C&#x27;</span>: &#123;<span class="string">&#x27;Alice&#x27;</span>: <span class="number">4.0</span>, <span class="string">&#x27;user1&#x27;</span>: <span class="number">2.0</span>, <span class="string">&#x27;user2&#x27;</span>: <span class="number">4.0</span>, <span class="string">&#x27;user3&#x27;</span>: <span class="number">1.0</span>, <span class="string">&#x27;user4&#x27;</span>: <span class="number">5.0</span>&#125;,</span><br><span class="line">             <span class="string">&#x27;D&#x27;</span>: &#123;<span class="string">&#x27;Alice&#x27;</span>: <span class="number">4.0</span>, <span class="string">&#x27;user1&#x27;</span>: <span class="number">3.0</span>, <span class="string">&#x27;user2&#x27;</span>: <span class="number">3.0</span>, <span class="string">&#x27;user3&#x27;</span>: <span class="number">5.0</span>, <span class="string">&#x27;user4&#x27;</span>: <span class="number">2.0</span>&#125;,</span><br><span class="line">             <span class="string">&#x27;E&#x27;</span>: &#123;<span class="string">&#x27;user1&#x27;</span>: <span class="number">3.0</span>, <span class="string">&#x27;user2&#x27;</span>: <span class="number">5.0</span>, <span class="string">&#x27;user3&#x27;</span>: <span class="number">4.0</span>, <span class="string">&#x27;user4&#x27;</span>: <span class="number">1.0</span>&#125;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> items</span><br></pre></td></tr></table></figure>
<ol>
<li>计算物品间的相似度矩阵</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">item_data = loadData()</span><br><span class="line"></span><br><span class="line">simiality_matrix = pd.DataFrame(</span><br><span class="line">    np.identity(<span class="built_in">len</span>(item_data)),</span><br><span class="line">    index=item_data.keys(),</span><br><span class="line">    columns=item_data.keys(),</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment">#遍历每条物品-用户评分数据</span></span><br><span class="line"><span class="keyword">for</span> i1, user1 <span class="keyword">in</span> item_data.items():</span><br><span class="line">    <span class="keyword">for</span> i2, user2 <span class="keyword">in</span> item_data.items():</span><br><span class="line">        <span class="keyword">if</span> i1 == i2:</span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line">        vec1, vec2 = [], []</span><br><span class="line">        <span class="keyword">for</span> user, rating1 <span class="keyword">in</span> unser1.items();</span><br><span class="line">        	rating2 = user2.get(user, -<span class="number">1</span>)</span><br><span class="line">            <span class="keyword">if</span> rating2 == -<span class="number">1</span>:</span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line">            vec1.append(rating1)</span><br><span class="line">            vec2.append(rating2)</span><br><span class="line">    	similarity_matrix[i1][i2] = np.corrcoef(vec1, vec2)[<span class="number">0</span>][<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(similarity_matrix)</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">       A         B         C         D         E</span><br><span class="line">A  1.000000 -0.476731 -0.123091  0.532181  0.969458</span><br><span class="line">B -0.476731  1.000000  0.645497 -0.310087 -0.478091</span><br><span class="line">C -0.123091  0.645497  1.000000 -0.720577 -0.427618</span><br><span class="line">D  0.532181 -0.310087 -0.720577  1.000000  0.581675</span><br><span class="line">E  0.969458 -0.478091 -0.427618  0.581675  1.000000</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<ol>
<li>从 Alice 购买过的物品中，选出与物品 <code>E</code> 最相似的 <code>num</code> 件物品。</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">target_user = <span class="string">&#x27;Alice&#x27;</span></span><br><span class="line">target_item = <span class="string">&#x27;E&#x27;</span></span><br><span class="line">num = <span class="number">2</span></span><br><span class="line"></span><br><span class="line">sim_items = []</span><br><span class="line">sim_items_list = similarity_matrix[target_item].sort_values(ascending=<span class="literal">False</span>).index.tolist()</span><br><span class="line"><span class="keyword">for</span> item <span class="keyword">in</span> sim_items_list:</span><br><span class="line">    <span class="comment"># 如果target_user对物品item评分过</span></span><br><span class="line">    <span class="keyword">if</span> target_user <span class="keyword">in</span> item_data[item]:</span><br><span class="line">        sim_items.append(item)</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">len</span>(sim_items) == num:</span><br><span class="line">        <span class="keyword">break</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;与物品<span class="subst">&#123;target_item&#125;</span>最相似的<span class="subst">&#123;num&#125;</span>个物品为：<span class="subst">&#123;sim_items&#125;</span>&#x27;</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">与物品E最相似的2个物品为：[&#x27;A&#x27;, &#x27;D&#x27;]</span><br></pre></td></tr></table></figure>
<ol>
<li>预测用户 Alice 对物品 <code>E</code> 的评分</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">target_user_mean_rating = np.mean(<span class="built_in">list</span>(item_data[target_item].values()))</span><br><span class="line">weighted_scores = <span class="number">0.</span></span><br><span class="line">corr_values_sum = <span class="number">0.</span></span><br><span class="line"></span><br><span class="line">target_item = <span class="string">&#x27;E&#x27;</span></span><br><span class="line"><span class="keyword">for</span> item <span class="keyword">in</span> sim_items:</span><br><span class="line">    corr_value = similarity_matrix[target_item][item]</span><br><span class="line">    user_mean_rating = np.mean(<span class="built_in">list</span>(item_data[item].values()))</span><br><span class="line"></span><br><span class="line">    weighted_scores += corr_value * (item_data[item][target_user] - user_mean_rating)</span><br><span class="line">    corr_values_sum += corr_value</span><br><span class="line"></span><br><span class="line">target_item_pred = target_user_mean_rating + weighted_scores / corr_values_sum</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;用户<span class="subst">&#123;target_user&#125;</span>对物品<span class="subst">&#123;target_item&#125;</span>的预测评分为：<span class="subst">&#123;target_item_pred&#125;</span>&#x27;</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">用户 Alice 对物品E的预测评分为：4.6</span><br></pre></td></tr></table></figure>
</li>
</ol>
<h3 id="协同过滤算法的权重改进"><a href="#协同过滤算法的权重改进" class="headerlink" title="协同过滤算法的权重改进"></a>协同过滤算法的权重改进</h3><ul>
<li><p>base 公式</p>
<script type="math/tex; mode=display">
w_{ij} = \frac{|N(i) \cap N(j) |}{|N(i)|}</script><ul>
<li>该公式表示同时喜好物品$i$和物品$j$的用户数，占喜爱物品$i$的比例。</li>
<li>缺点：若物品$j$为热门物品，那么它与任何物品的相似度都很高。</li>
</ul>
</li>
<li><p>对热门物品进行惩罚</p>
<script type="math/tex; mode=display">
w_{ij} = \frac{|N(i) \cap N(j)|}{\sqrt{|N(i)||N(j)|}}</script><ul>
<li>根据 base 公式在的问题，对物品$j$进行打压。打压的出发点很简单，就是在分母再除以一个物品$j$被购买的数量。</li>
<li>此时，若物品品$j$为热门物品，那么对应的$N(j)$也会很大，受到的惩罚更多</li>
</ul>
</li>
<li><p>控制对热门物品的惩罚力度</p>
<script type="math/tex; mode=display">
w_{ij} = \frac{|N(i) \cap N(j)|}{|N(i)|^{1-a}|N(j)|^{a}}</script><ul>
<li>除了第二点提到的办法，在计算物品之间相似度时可以对热门物品进行惩罚外。</li>
<li>可以在此基础上，进一步引入参数a ，这样可以通过控制参数a来决定对热门物品的惩罚力度。</li>
</ul>
</li>
<li><p>对活跃用户的惩罚</p>
<ul>
<li><p>在计算物品之间的相似度时，可以进一步将用户的活跃度考虑进来。</p>
<script type="math/tex; mode=display">
w_{ij} = \frac{\sum_{u \in N(i) \cap N(j)} \frac{1}{log1+|N(u)|}}{|N(i)|^{1-a}|N(j)|^{a}}</script></li>
<li><p>对于异常活跃的用户，在计算物品之间的相似度时，他的贡献应该小于非活跃用户。</p>
</li>
</ul>
</li>
</ul>
<h3 id="协同过滤算法的问题分析"><a href="#协同过滤算法的问题分析" class="headerlink" title="协同过滤算法的问题分析"></a>协同过滤算法的问题分析</h3><p>协同过滤算法存在的问题之一就是泛化能力弱：</p>
<ul>
<li>即协同过滤无法将两个物品相似的信息推广到其他物品的相似性上。</li>
<li>导致的问题是<strong>热门物品具有很强的头部效应， 容易跟大量物品产生相似， 而尾部物品由于特征向量稀疏， 导致很少被推荐</strong>。</li>
</ul>
<p>比如下面这个例子：</p>
<p><img src="https://s2.loli.net/2024/04/28/2KL4yXo7pnV8YMJ.png" alt="image-20240428211817361"></p>
<ul>
<li>左边矩阵中，𝐴,𝐵,𝐶,𝐷<em>A</em>,<em>B</em>,<em>C</em>,<em>D</em> 表示的是物品。</li>
<li>可以看出，D是一件热门物品，其与 A、B、C的相似度比较大。因此，推荐系统更可能将D推荐给用过 A、B、C的用户。</li>
<li>但是，推荐系统无法找出 A、B、C之间相似性的原因是交互数据太稀疏， 缺乏相似性计算的直接数据。</li>
</ul>
<p>所以这就是协同过滤的天然缺陷：<strong>推荐系统头部效应明显， 处理稀疏向量的能力弱</strong>。</p>
<p>为了解决这个问题， 同时增加模型的泛化能力。2006年，<strong>矩阵分解技术(Matrix Factorization, MF</strong>)被提出：</p>
<ul>
<li>该方法在协同过滤共现矩阵的基础上， 使用更稠密的隐向量表示用户和物品， 挖掘用户和物品的隐含兴趣和隐含特征。</li>
<li>在一定程度上弥补协同过滤模型处理稀疏矩阵能力不足的问题。</li>
</ul>
<h3 id="课后思考"><a href="#课后思考" class="headerlink" title="课后思考"></a>课后思考</h3><ol>
<li><strong>什么时候使用UserCF，什么时候使用ItemCF？为什么？</strong></li>
</ol>
<blockquote>
<p>（1）UserCF</p>
<ul>
<li>由于是基于用户相似度进行推荐， 所以具备更强的社交特性， 这样的特点非常适于<strong>用户少， 物品多， 时效性较强的场合</strong>。<ul>
<li>比如新闻推荐场景， 因为新闻本身兴趣点分散， 相比用户对不同新闻的兴趣偏好， 新闻的及时性，热点性往往更加重要， 所以正好适用于发现热点，跟踪热点的趋势。</li>
<li>另外还具有推荐新信息的能力， 更有可能发现惊喜, 因为看的是人与人的相似性, 推出来的结果可能更有惊喜，可以发现用户潜在但自己尚未察觉的兴趣爱好。</li>
</ul>
</li>
</ul>
<p>（2）itemCF</p>
<ul>
<li>这个更适用于兴趣变化较为稳定的应用， 更接近于个性化的推荐， 适合<strong>物品少，用户多，用户兴趣固定持久， 物品更新速度不是太快的场合</strong>。</li>
<li>比如推荐艺术品， 音乐， 电影。</li>
</ul>
</blockquote>
<ol>
<li><strong>协同过滤在计算上有什么缺点？有什么比较好的思路可以解决（缓解）？</strong></li>
</ol>
<blockquote>
<p>该问题答案参考上一小节的<strong>协同过滤算法的问题分析</strong>。</p>
</blockquote>
<ol>
<li><strong>上面介绍的相似度计算方法有什么优劣之处？</strong></li>
</ol>
<blockquote>
<p><strong>cosine相似度计算简单方便，一般较为常用。但是，当用户的评分数据存在 bias 时，效果往往不那么好。</strong></p>
<ul>
<li>简而言之，就是不同用户评分的偏向不同。部分用户可能乐于给予好评，而部分用户习惯给予差评或者乱评分。</li>
<li>这个时候，根据cosine 相似度计算出来的推荐结果效果会打折扣。</li>
</ul>
<p>举例来说明，如下图（<code>X,Y,Z</code> 表示物品，<code>d,e,f</code>表示用户）：</p>
<p><img src="https://s2.loli.net/2024/04/28/s7BDKTPaZ3VEGuR.png" alt="image-20240428212534039"></p>
<ul>
<li>如果使用余弦相似度进行计算，用户 d 和 e 之间较为相似。但是实际上，用户 d 和 f 之间应该更加相似。只不过由于 d 倾向于打高分，e 倾向于打低分导致二者之间的余弦相似度更高。</li>
<li>这种情况下，可以考虑使用皮尔逊相关系数计算用户之间的相似性关系。</li>
</ul>
</blockquote>
<ol>
<li><strong>协同过滤还存在其他什么缺陷？有什么比较好的思路可以解决（缓解）？</strong></li>
</ol>
<blockquote>
<ul>
<li>协同过滤的优点就是没有使用更多的用户或者物品属性信息，仅利用用户和物品之间的交互信息就能完成推荐，该算法简单高效。</li>
<li>但这也是协同过滤算法的一个弊端。由于未使用更丰富的用户和物品特征信息，这也导致协同过滤算法的模型表达能力有限。</li>
<li>对于该问题，逻辑回归模型（LR）可以更好地在推荐模型中引入更多特征信息，提高模型的表达能力。</li>
</ul>
</blockquote>
<h3 id="Swing-Graph-based"><a href="#Swing-Graph-based" class="headerlink" title="Swing(Graph-based)"></a>Swing(Graph-based)</h3><h4 id="动机"><a href="#动机" class="headerlink" title="动机"></a>动机</h4><p>大规模推荐系统需要实时对用户行为做出海量预测，为了保证这种实时性，大规模的推荐系统通常严重依赖于预先计算好的产品索引。产品索引的功能为：<strong>给定种子产品返回排序后的候选相关产品列表</strong>。</p>
<p><img src="https://s2.loli.net/2024/06/25/NFDPOKqxpiTyuIj.png" alt="在这里插入图片描述"></p>
<p>相关性产品索引主要包含两部分：<strong>替代性产品和互补性产品</strong>。例如图中的不同种类的衬衫构成了替代关系，而衬衫和风衣裤子等构成了互补关系。用户通常希望在完成购买行为之前尽可能看更多的衬衫，而用户购买过衬衫之后更希望看到与之搭配的单品而不是其他衬衫了。</p>
<h4 id="之前方法的局限性"><a href="#之前方法的局限性" class="headerlink" title="之前方法的局限性"></a>之前方法的局限性</h4><ul>
<li>基于 Cosine, Jaccard, 皮尔逊相关性等相似度计算的协同过滤算法，在计算邻居关联强度的时候只关注于 Item-based (常用，因为item相比于用户变化的慢，且新Item特征比较容易获得)，Item-based CF 只关注于 Item-User-Item 的路径，把所有的User-Item交互都平等得看待，从而忽视了 User-Item 交互中的大量噪声，推荐精度存在局限性。</li>
<li>对互补性产品的建模不足，可能会导致用户购买过手机之后还继续推荐手机，但用户短时间内不会再继续购买手机，因此产生无效曝光。</li>
</ul>
<h4 id="贡献"><a href="#贡献" class="headerlink" title="贡献"></a>贡献</h4><p>提出了高效建立产品索引图的技术。 主要包括：</p>
<ul>
<li>Swing 算法利用 user-item 二部图的子结构捕获产品间的替代关系。</li>
<li>Surprise 算法利用商品分类信息和用户共同购买图上的聚类技术来建模产品之间的组合关系。</li>
</ul>
<h4 id="Swing算法"><a href="#Swing算法" class="headerlink" title="Swing算法"></a>Swing算法</h4><p>Swing 通过利用 User-Item-User 路径中所包含的信息，考虑 User-Item 二部图中的鲁棒内部子结构计算相似性。</p>
<ul>
<li><p>什么是内部子结构？ 以经典的啤酒尿布故事为例，张三同时购买了啤酒和尿布，这可能是一种巧合。但两个甚至多个顾客都同时购买了啤酒尿布，这就证明啤酒和尿布具有相关关系。这样共同购买啤酒和尿布的用户越多，啤酒和尿布的相关度就会越高。图中的红色四边形就是一种Swing子结构，这种子结构可以作为给王五推荐尿布的依据。<img src="https://s2.loli.net/2024/06/25/AG2rNRbMCuJylsH.png" alt="在这里插入图片描述"></p>
</li>
<li><p>通俗解释：若用户$u$和用户$v$之间除了购买过$i$外，还购买过商品$j$ ，则认为两件商品是具有某种程度上的相似的。也就是说，商品与商品之间的相似关系，是通过用户关系来传递的。为了衡量物品$i$和$j$ 的相似性，比较同时购买了物品$i$和$j$ 的用户$u$和用户$v$， 如果这两个用户共同购买的物品越少，即这两个用户原始兴趣不相似，但仍同时购买了两个相同的物品$i$和$j$， 则物品$i$和$j$的相似性越高。</p>
</li>
<li><p>计算公式</p>
<script type="math/tex; mode=display">
s(i,j) = \sum</script><p>其中$U_i$是点击过商品$i$的用户集合，$I_u$是用户$u$点击过的商品集合，$a$是平滑系数。</p>
<p>$w_u=\frac{1}{\sqrt{I_u}},w_v=\frac{1}{\sqrt{I_v}$是用户权重参数，来降低活跃用户的影响。</p>
</li>
</ul>
<ul>
<li><p>代码实现</p>
<ul>
<li><p>Python （建议自行debug方便理解）</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> itertools <span class="keyword">import</span> combinations</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line">alpha = <span class="number">0.5</span></span><br><span class="line">top_k = <span class="number">20</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">load_data</span>(<span class="params">train_path</span>):</span><br><span class="line">    train_data = pd.read_csv(train_path, sep=<span class="string">&quot;\t&quot;</span>, engine=<span class="string">&quot;python&quot;</span>, name=[<span class="string">&quot;userid&quot;</span>, <span class="string">&quot;itemid&quot;</span>, <span class="string">&quot;rate&quot;</span>]) <span class="comment">#提取用户交互记录数据</span></span><br><span class="line">    <span class="built_in">print</span>(train_data.head(<span class="number">3</span>))</span><br><span class="line">    <span class="keyword">return</span> train_data</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_uitems_iusers</span>(<span class="params">train</span>):</span><br><span class="line">    u_items = <span class="built_in">dict</span>()</span><br><span class="line">    i_users = <span class="built_in">dict</span>()</span><br><span class="line">    <span class="keyword">for</span> index, row <span class="keyword">in</span> train.iterrows():<span class="comment">#处理用户交互记录 </span></span><br><span class="line">        u_items.setdefault(row[<span class="string">&quot;userid&quot;</span>], <span class="built_in">set</span>())</span><br><span class="line">        i_users.setdefault(row[<span class="string">&quot;itemid&quot;</span>], <span class="built_in">set</span>())</span><br><span class="line">        u_items[row[<span class="string">&quot;userid&quot;</span>]].add(row[<span class="string">&quot;itemid&quot;</span>])<span class="comment">#得到user交互过的所有item</span></span><br><span class="line">        i_users[row[<span class="string">&quot;itemid&quot;</span>]].add(row[<span class="string">&quot;userid&quot;</span>])<span class="comment">#得到item交互过的所有user</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;使用的用户个数为：&#123;&#125;&quot;</span>.<span class="built_in">format</span>(<span class="built_in">len</span>(u_items)))</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;使用的item个数为：&#123;&#125;&quot;</span>.<span class="built_in">format</span>(<span class="built_in">len</span>(i_users)))</span><br><span class="line">    <span class="keyword">return</span> u_items, i_users </span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">swing_model</span>(<span class="params">u_items, i_users</span>):</span><br><span class="line">    <span class="comment"># print([i for i in i_users.values()][:5])</span></span><br><span class="line">    <span class="comment"># print([i for i in u_items.values()][:5])</span></span><br><span class="line">    item_pairs = <span class="built_in">list</span>(combinations(i_users.keys(), <span class="number">2</span>)) <span class="comment">#全排列组合对</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;item pairs length：&#123;&#125;&quot;</span>.<span class="built_in">format</span>(<span class="built_in">len</span>(item_pairs)))</span><br><span class="line">    item_sim_dict = <span class="built_in">dict</span>()</span><br><span class="line">    <span class="keyword">for</span> (i, j) <span class="keyword">in</span> item_pairs:</span><br><span class="line">        user_pairs = <span class="built_in">list</span>(combinations(i_users[i] &amp; i_users[j], <span class="number">2</span>)) <span class="comment">#item_i和item_j对应的user取交集后全排列 得到user对</span></span><br><span class="line">        result = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> (u, v) <span class="keyword">in</span> user_pairs:</span><br><span class="line">            result += <span class="number">1</span> / (alpha + <span class="built_in">list</span>(u_items[u] &amp; u_items[v]).__len__()) <span class="comment">#分数公式</span></span><br><span class="line">        <span class="keyword">if</span> result != <span class="number">0</span> :</span><br><span class="line">            item_sim_dict.setdefault(i, <span class="built_in">dict</span>())</span><br><span class="line">            item_sim_dict[i][j] = <span class="built_in">format</span>(result, <span class="string">&#x27;.6f&#x27;</span>)</span><br><span class="line">    <span class="keyword">return</span> item_sim_dict</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">save_item_sims</span>(<span class="params">item_sim_dict, top_k, path</span>):</span><br><span class="line">    new_item_sim_dict = <span class="built_in">dict</span>()</span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        writer = <span class="built_in">open</span>(path, <span class="string">&#x27;w&#x27;</span>, encoding=<span class="string">&#x27;utf-8&#x27;</span>)</span><br><span class="line">        <span class="keyword">for</span> item, sim_items <span class="keyword">in</span> item_sim_dict.items():</span><br><span class="line">            new_item_sim_dict.setdefault(item, <span class="built_in">dict</span>())</span><br><span class="line">            new_item_sim_dict[item] = <span class="built_in">dict</span>(<span class="built_in">sorted</span>(sim_items.items(), key = <span class="keyword">lambda</span> k:k[<span class="number">1</span>], reverse=<span class="literal">True</span>)[:top_k])<span class="comment">#排序取出 top_k个相似的item</span></span><br><span class="line">            writer.write(<span class="string">&#x27;item_id:%d\t%s\n&#x27;</span> % (item, new_item_sim_dict[item]))</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;SUCCESS: top_&#123;&#125; item saved&quot;</span>.<span class="built_in">format</span>(top_k))</span><br><span class="line">    <span class="keyword">except</span> Exception <span class="keyword">as</span> e:</span><br><span class="line">        <span class="built_in">print</span>(e.args)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    train_data_path = <span class="string">&quot;./ratings_final.txt&quot;</span></span><br><span class="line">    item_sim_save_path = <span class="string">&quot;./item_sim_dict.txt&quot;</span></span><br><span class="line">    top_k = <span class="number">10</span> <span class="comment">#与item相似的前 k 个item</span></span><br><span class="line">    train = load_data(train_data_path)</span><br><span class="line">    u_items, i_users = get_uitems_iusers(train)</span><br><span class="line">    item_sim_dict = swing_model(u_items, i_users)</span><br><span class="line">    save_item_sims(item_sim_dict, top_k, item_sim_save_path)</span><br></pre></td></tr></table></figure>
</li>
<li><p>Spark（仅为核心代码需要补全配置才能跑通）</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">Swing</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> spark = <span class="type">SparkSession</span>.builder()</span><br><span class="line">      .appName(<span class="string">&quot;test&quot;</span>)</span><br><span class="line">      .master(<span class="string">&quot;local[2]&quot;</span>)</span><br><span class="line">      .getOrCreate()</span><br><span class="line">    <span class="keyword">val</span> alpha = <span class="number">1</span> <span class="comment">//分数计算参数</span></span><br><span class="line">    <span class="keyword">val</span> filter_n_items = <span class="number">10000</span> <span class="comment">//想要计算的item数量 测试的时候取少点</span></span><br><span class="line">    <span class="keyword">val</span> top_n_items = <span class="number">500</span> <span class="comment">//保存item的score排序前500个相似的item</span></span><br><span class="line">    <span class="keyword">val</span> model = <span class="keyword">new</span> <span class="type">SwingModel</span>(spark)</span><br><span class="line">      .setAlpha(alpha.toDouble)</span><br><span class="line">      .setFilter_N_Items(filter_n_items.toInt)</span><br><span class="line">      .setTop_N_Items(top_n_items.toInt)</span><br><span class="line">    <span class="keyword">val</span> url = <span class="string">&quot;file:///usr/local/var/scala/common/part-00022-e17c0014.snappy.parquet&quot;</span></span><br><span class="line">    <span class="keyword">val</span> ratings = <span class="type">DataLoader</span>.getRatingLog(spark, url)</span><br><span class="line">    <span class="keyword">val</span> df = model.fit(ratings).item2item()</span><br><span class="line">    df.show(<span class="number">3</span>,<span class="literal">false</span>)</span><br><span class="line"><span class="comment">//    df.write.mode(&quot;overwrite&quot;).parquet(dest_url)</span></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">    * swing</span></span><br><span class="line"><span class="comment">    * @param ratings 打分dataset</span></span><br><span class="line"><span class="comment">    * @return</span></span><br><span class="line"><span class="comment">    */</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">fit</span></span>(ratings: <span class="type">Dataset</span>[<span class="type">Rating</span>]): <span class="type">SwingModel</span> = &#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">interWithAlpha</span> </span>= udf(</span><br><span class="line">      (array_1: <span class="type">Seq</span>[<span class="type">GenericRowWithSchema</span>],</span><br><span class="line">       array_2: <span class="type">Seq</span>[<span class="type">GenericRowWithSchema</span>]) =&gt; &#123;</span><br><span class="line">        <span class="keyword">var</span> score = <span class="number">0.0</span></span><br><span class="line">        <span class="keyword">val</span> u_set_1 = array_1.toSet</span><br><span class="line">        <span class="keyword">val</span> u_set_2 = array_2.toSet</span><br><span class="line">        <span class="keyword">val</span> user_set = u_set_1.intersect(u_set_2).toArray <span class="comment">//取交集得到两个item共同user</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> (i &lt;- user_set.indices; j &lt;- i + <span class="number">1</span> until user_set.length) &#123;</span><br><span class="line">          <span class="keyword">val</span> user_1 = user_set(i)</span><br><span class="line">          <span class="keyword">val</span> user_2 = user_set(j)</span><br><span class="line">          <span class="keyword">val</span> item_set_1 = user_1.getAs[<span class="type">Seq</span>[<span class="type">String</span>]](<span class="string">&quot;_2&quot;</span>).toSet</span><br><span class="line">          <span class="keyword">val</span> item_set_2 = user_2.getAs[<span class="type">Seq</span>[<span class="type">String</span>]](<span class="string">&quot;_2&quot;</span>).toSet</span><br><span class="line">          score = score + <span class="number">1</span> / (item_set_1</span><br><span class="line">            .intersect(item_set_2)</span><br><span class="line">            .size</span><br><span class="line">            .toDouble + alpha.get)</span><br><span class="line">        &#125;</span><br><span class="line">        score</span><br><span class="line">      &#125;</span><br><span class="line">    )</span><br><span class="line">    <span class="keyword">val</span> df = ratings.repartition(defaultParallelism).cache()</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">val</span> groupUsers = df</span><br><span class="line">      .groupBy(<span class="string">&quot;user_id&quot;</span>)</span><br><span class="line">      .agg(collect_set(<span class="string">&quot;item_id&quot;</span>)) <span class="comment">//聚合itme_id</span></span><br><span class="line">      .toDF(<span class="string">&quot;user_id&quot;</span>, <span class="string">&quot;item_set&quot;</span>)</span><br><span class="line">      .repartition(defaultParallelism)</span><br><span class="line">    println(<span class="string">&quot;groupUsers&quot;</span>)</span><br><span class="line">    groupUsers.show(<span class="number">3</span>, <span class="literal">false</span>)<span class="comment">//user_id|[item_id_set]: 422|[6117,611,6117] </span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> groupItems = df</span><br><span class="line">      .join(groupUsers, <span class="string">&quot;user_id&quot;</span>)</span><br><span class="line">      .rdd</span><br><span class="line">      .map &#123; x =&gt;</span><br><span class="line">        <span class="keyword">val</span> item_id = x.getAs[<span class="type">String</span>](<span class="string">&quot;item_id&quot;</span>)</span><br><span class="line">        <span class="keyword">val</span> user_id = x.getAs[<span class="type">String</span>](<span class="string">&quot;user_id&quot;</span>)</span><br><span class="line">        <span class="keyword">val</span> item_set = x.getAs[<span class="type">Seq</span>[<span class="type">String</span>]](<span class="string">&quot;item_set&quot;</span>)</span><br><span class="line">        (item_id, (user_id, item_set))</span><br><span class="line">      &#125;<span class="comment">//i_[user(item_set)]</span></span><br><span class="line">      .toDF(<span class="string">&quot;item_id&quot;</span>, <span class="string">&quot;user&quot;</span>)</span><br><span class="line">      .groupBy(<span class="string">&quot;item_id&quot;</span>)</span><br><span class="line">      .agg(collect_set(<span class="string">&quot;user&quot;</span>), count(<span class="string">&quot;item_id&quot;</span>))</span><br><span class="line">      .toDF(<span class="string">&quot;item_id&quot;</span>, <span class="string">&quot;user_set&quot;</span>, <span class="string">&quot;count&quot;</span>)</span><br><span class="line">      .filter(<span class="string">&quot;size(user_set) &gt; 1&quot;</span>)<span class="comment">//过滤掉没有交互的</span></span><br><span class="line">      .sort($<span class="string">&quot;count&quot;</span>.desc) <span class="comment">//根据count倒排item_id数量</span></span><br><span class="line">      .limit(filter_n_items.get)<span class="comment">//item可能百万级别但后面召回的需求量小所以只取前n个item进行计算</span></span><br><span class="line">      .drop(<span class="string">&quot;count&quot;</span>)</span><br><span class="line">      .repartition(defaultParallelism)</span><br><span class="line">      .cache()</span><br><span class="line">    println(<span class="string">&quot;groupItems&quot;</span>) <span class="comment">//得到与itme_id有交互的user_id</span></span><br><span class="line">    groupItems.show(<span class="number">3</span>, <span class="literal">false</span>)<span class="comment">//item_id|[[user_id,[item_set]],[user_id,[item_set]]]: 67|[[562,[66, 813, 61, 67]],[563,[67, 833, 62, 64]]]</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> itemJoined = groupItems</span><br><span class="line">      .join(broadcast(groupItems))<span class="comment">//内连接两个item列表</span></span><br><span class="line">      .toDF(<span class="string">&quot;item_id_1&quot;</span>, <span class="string">&quot;user_set_1&quot;</span>, <span class="string">&quot;item_id_2&quot;</span>, <span class="string">&quot;user_set_2&quot;</span>)</span><br><span class="line">      .filter(<span class="string">&quot;item_id_1 &gt; item_id_2&quot;</span>)<span class="comment">//内连接 item两两配对</span></span><br><span class="line">      .withColumn(<span class="string">&quot;score&quot;</span>, interWithAlpha(col(<span class="string">&quot;user_set_1&quot;</span>), col(<span class="string">&quot;user_set_2&quot;</span>)))<span class="comment">//将上面得到的与item相关的user_set输入到函数interWithAlpha计算分数</span></span><br><span class="line">      .select(<span class="string">&quot;item_id_1&quot;</span>, <span class="string">&quot;item_id_2&quot;</span>, <span class="string">&quot;score&quot;</span>)</span><br><span class="line">      .filter(<span class="string">&quot;score &gt; 0&quot;</span>)</span><br><span class="line">      .repartition(defaultParallelism)</span><br><span class="line">      .cache()</span><br><span class="line">    println(<span class="string">&quot;itemJoined&quot;</span>)</span><br><span class="line">    itemJoined.show(<span class="number">5</span>)<span class="comment">//得到两两item之间的分数结果 item_id_1 item_id_2 score</span></span><br><span class="line">    similarities = <span class="type">Option</span>(itemJoined)</span><br><span class="line">    <span class="keyword">this</span></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">    * 从fit结果，对item_id进行聚合并排序，每个item后截取n个item，并返回。</span></span><br><span class="line"><span class="comment">    * @param num 取n个item</span></span><br><span class="line"><span class="comment">    * @return</span></span><br><span class="line"><span class="comment">    */</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">item2item</span></span>(): <span class="type">DataFrame</span> = &#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">itemWithScore</span>(<span class="params">item_id: <span class="type">String</span>, score: <span class="type">Double</span></span>)</span></span><br><span class="line">    <span class="keyword">val</span> sim = similarities.get.select(<span class="string">&quot;item_id_1&quot;</span>, <span class="string">&quot;item_id_2&quot;</span>, <span class="string">&quot;score&quot;</span>)</span><br><span class="line">    <span class="keyword">val</span> topN = sim</span><br><span class="line">      .map &#123; x =&gt;</span><br><span class="line">        <span class="keyword">val</span> item_id_1 = x.getAs[<span class="type">String</span>](<span class="string">&quot;item_id_1&quot;</span>)</span><br><span class="line">        <span class="keyword">val</span> item_id_2 = x.getAs[<span class="type">String</span>](<span class="string">&quot;item_id_2&quot;</span>)</span><br><span class="line">        <span class="keyword">val</span> score = x.getAs[<span class="type">Double</span>](<span class="string">&quot;score&quot;</span>)</span><br><span class="line">        (item_id_1, (item_id_2, score))</span><br><span class="line">      &#125;</span><br><span class="line">      .toDF(<span class="string">&quot;item_id&quot;</span>, <span class="string">&quot;itemWithScore&quot;</span>)</span><br><span class="line">      .groupBy(<span class="string">&quot;item_id&quot;</span>)</span><br><span class="line">      .agg(collect_set(<span class="string">&quot;itemWithScore&quot;</span>))</span><br><span class="line">      .toDF(<span class="string">&quot;item_id&quot;</span>, <span class="string">&quot;item_set&quot;</span>)<span class="comment">//item_id |[[item_id1:score],[item_id2:score]]</span></span><br><span class="line">      .rdd</span><br><span class="line">      .map &#123; x =&gt;</span><br><span class="line">        <span class="keyword">val</span> item_id_1 = x.getAs[<span class="type">String</span>](<span class="string">&quot;item_id&quot;</span>)</span><br><span class="line">        <span class="keyword">val</span> item_set = x   <span class="comment">//对itme_set中score进行排序操作</span></span><br><span class="line">          .getAs[<span class="type">Seq</span>[<span class="type">GenericRowWithSchema</span>]](<span class="string">&quot;item_set&quot;</span>)</span><br><span class="line">          .map &#123; x =&gt;</span><br><span class="line">            <span class="keyword">val</span> item_id_2 = x.getAs[<span class="type">String</span>](<span class="string">&quot;_1&quot;</span>)</span><br><span class="line">            <span class="keyword">val</span> score = x.getAs[<span class="type">Double</span>](<span class="string">&quot;_2&quot;</span>)</span><br><span class="line">            (item_id_2, score)</span><br><span class="line">          &#125;</span><br><span class="line">          .sortBy(-_._2)<span class="comment">//根据score进行排序</span></span><br><span class="line">          .take(top_n_items.get)<span class="comment">//取top_n</span></span><br><span class="line">          .map(x =&gt; x._1 + <span class="string">&quot;:&quot;</span> + x._2.toString)</span><br><span class="line">        (item_id_1, item_set)</span><br><span class="line">      &#125;</span><br><span class="line">      .filter(_._2.nonEmpty)</span><br><span class="line">      .toDF(<span class="string">&quot;id&quot;</span>, <span class="string">&quot;sorted_items&quot;</span>)</span><br><span class="line">    topN</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
</ul>
<h4 id="Surprise算法"><a href="#Surprise算法" class="headerlink" title="Surprise算法"></a>Surprise算法</h4><p><strong>首先在行为相关性中引入连续时间衰减因子，然后引入基于交互数据的聚类方法解决数据稀疏的问题，旨在帮助用户找到互补商品</strong>。互补相关性主要从三个层面考虑，类别层面，商品层面和聚类层面。</p>
<ul>
<li><p>类别层面 首先通过商品和类别的映射关系，我们可以得到 user-category 矩阵。随后使用简单的相关性度量可以计算出类别$i,j$的相关性。</p>
<script type="math/tex; mode=display">
\theta_{i,j}=p(c_{i,j}|c_j)=\frac{c_{i,j}}{c_j}</script><p>即，$N(c_{i,j})$为在购买过$i$之后购买$j$类的数量，$c_j$为购买$j$类的数量。</p>
<p>由于类别直接的种类差异，每个类别的相关类数量存在差异，因此采用最大相对落点来作为划分阈值。</p>
<p><img src="https://s2.loli.net/2024/06/26/cIJshlBxM32mLzE.png" alt="在这里插入图片描述"></p>
<p>例如图(a)中T恤的相关类选择前八个，图(b)中手机的相关类选择前三个。</p>
</li>
<li><p>商品层面 商品层面的相关性挖掘主要有两个关键设计：</p>
<ul>
<li>商品的购买顺序是需要被考虑的，例如在用户购买手机后推荐充电宝是合理的，但在用户购买充电宝后推荐手机是不合理的。</li>
<li>两个商品购买的时间间隔也是需要被考虑的，时间间隔越短越能证明两个商品的互补关系。</li>
<li>最终商品层面的互补相关性被定义为：$s<em>1(i,j)=\frac{\sum</em>{u \in U<em>i \cap U_j}\frac{1}{1+|t</em>{ui}-t{uj}|}}}{||U_i \dot U_j||}$</li>
</ul>
</li>
<li><p>聚类层面</p>
<ul>
<li>如何聚类？ 传统的聚类算法（基于密度和 k-means ）在数十亿产品规模下的淘宝场景中不可行，所以作者采用了标签传播算法。</li>
<li>在哪里标签传播？ Item-item 图，其中又 Swing 计算的排名靠前 item 为邻居，边的权重就是 Swing 分数。</li>
<li>表现如何？ 快速而有效，15分钟即可对数十亿个项目进行聚类。 最终聚类层面的相关度计算同上面商品层面的计算公式</li>
</ul>
</li>
<li><p>线性组合： $s(i,j)=w<em>s_1(i,j)+(1-w)</em>s_2(i,j)$,其中$w=0.8$是作者设置的权重超参数。 Surprise算法<strong>利用类别信息和标签传播技术解决了用户共同购买图上的稀疏性问题</strong>。</p>
</li>
</ul>
<p><strong>参考资料</strong></p>
<ul>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2010.05525">Large Scale Product Graph Construction for Recommendation in E-commerce</a></li>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/383346471">推荐召回-Swing</a></li>
</ul>
<h4 id="矩阵分解"><a href="#矩阵分解" class="headerlink" title="矩阵分解"></a>矩阵分解</h4><h5 id="隐语义模型与矩阵分解"><a href="#隐语义模型与矩阵分解" class="headerlink" title="隐语义模型与矩阵分解"></a>隐语义模型与矩阵分解</h5><p>协同过滤算法的特点：</p>
<ul>
<li>协同过滤算法的特点就是<strong>完全没有利用到物品本身或者是用户自身的属性， 仅仅利用了用户与物品的交互信息就可以实现推荐</strong>，是一个可解释性很强， 非常直观的模型。</li>
<li>但是也存在一些问题，处理稀疏矩阵的能力比较弱。</li>
</ul>
<p>为了使得协同过滤更好处理稀疏矩阵问题， 增强泛化能力。从协同过滤中衍生出矩阵分解模型(Matrix Factorization, MF)或者叫隐语义模型：</p>
<ul>
<li>在协同过滤共现矩阵的基础上， 使用更稠密的隐向量表示用户和物品。</li>
<li>通过挖掘用户和物品的隐含兴趣和隐含特征， 在一定程度上弥补协同过滤模型处理稀疏矩阵能力不足的问题。</li>
</ul>
<h5 id="隐语义模型"><a href="#隐语义模型" class="headerlink" title="隐语义模型"></a>隐语义模型</h5><p>隐语义模型最早在文本领域被提出，用于找到文本的隐含语义。在2006年， 被用于推荐中， 它的核心思想是通过隐含特征（latent factor）联系用户兴趣和物品（item）， 基于用户的行为找出潜在的主题和分类， 然后对物品进行自动聚类，划分到不同类别/主题(用户的兴趣)。</p>
<p>以项亮老师《推荐系统实践》书中的内容为例：</p>
<blockquote>
<p><strong>如果我们知道了用户A和用户B两个用户在豆瓣的读书列表， 从他们的阅读列表可以看出，用户A的兴趣涉及侦探小说、科普图书以及一些计算机技术书， 而用户B的兴趣比较集中在数学和机器学习方面。 那么如何给A和B推荐图书呢？ 先说说协同过滤算法， 这样好对比不同：</strong></p>
<ul>
<li>对于UserCF，首先需要找到和他们看了同样书的其他用户（兴趣相似的用户），然后给他们推荐那些用户喜欢的其他书。</li>
<li>对于ItemCF，需要给他们推荐和他们已经看的书相似的书，比如作者B看了很多关于数据挖掘的书，可以给他推荐机器学习或者模式识别方面的书。</li>
</ul>
<p><strong>而如果是隐语义模型的话， 它会先通过一些角度把用户兴趣和这些书归一下类， 当来了用户之后， 首先得到他的兴趣分类， 然后从这个分类中挑选他可能喜欢的书籍。</strong></p>
</blockquote>
<p>隐语义模型和协同过滤的不同主要体现在隐含特征上， 比如书籍的话它的内容， 作者， 年份， 主题等都可以算隐含特征。</p>
<p>以王喆老师《深度学习推荐系统》中的一个原理图为例，看看是如何通过隐含特征来划分开用户兴趣和物品的。</p>
<h5 id="音乐评分实例"><a href="#音乐评分实例" class="headerlink" title="音乐评分实例"></a>音乐评分实例</h5><p>假设每个用户都有自己的听歌偏好， 比如用户 A 喜欢带有<strong>小清新的</strong>， <strong>吉他伴奏的</strong>， <strong>王菲</strong>的歌曲，如果一首歌正好<strong>是王菲唱的， 并且是吉他伴奏的小清新</strong>， 那么就可以将这首歌推荐给这个用户。 也就是说是<strong>小清新， 吉他伴奏， 王菲</strong>这些元素连接起了用户和歌曲。</p>
<p>当然每个用户对不同的元素偏好不同， 每首歌包含的元素也不一样， 所以我们就希望找到下面的两个矩阵：</p>
<ol>
<li>潜在因子—— 用户矩阵Q 这个矩阵表示不同用户对于不同元素的偏好程度， 1代表很喜欢， 0代表不喜欢， 比如下面这样：</li>
<li></li>
</ol>
<h3 id="向量的召回"><a href="#向量的召回" class="headerlink" title="向量的召回"></a>向量的召回</h3><h2 id="FM模型结构"><a href="#FM模型结构" class="headerlink" title="FM模型结构"></a>FM模型结构</h2><p>FM 模型用于排序时，模型的公式定义如下：</p>
<script type="math/tex; mode=display">
\hat{y}(x):= w_0+\sum_{i=1}^n w_ix_i + \sum_{i=1}^{n} \sum_{j=i+1}^n \langle v_i,v_j \rangle x_ix_j</script><ul>
<li>其中，$i$表示特征的序号，$n$表示特征的数量；$x_i \in R$表示第$i$个特征的值。</li>
<li>$v<em>i,v_j \in R^k$分别表示特征$x_i,x_j$对应的隐语义向量（Embedding向量），$\langle v_i,v_j \rangle := \sum</em>{f=1}^k v<em>{i,f} \cdot v</em>{j,f}$。</li>
<li>$w_0,w_i \in R$表示需要学习的参数。</li>
</ul>
<p> <strong>FM的一阶交互特征</strong></p>
<p>在 FM 的表达式中，前两项为特征的一阶交互项。将其拆分为用户特征和物品特征的一阶特征交互项，如下：</p>
<script type="math/tex; mode=display">
w_0 + \sum_{i=1}^n w_ix_i =w_0 + \sum_{t \in I} w_tx_t + \sum_{u \in U} w_u x_u</script><ul>
<li>其中，$U$表示用户相关特征集合，$I$表示物品相关特征集合。</li>
</ul>
<p><strong>FM 的二阶特征交互</strong></p>
<p><img src="https://s2.loli.net/2024/04/29/nL1HB7tQdFo2yw6.png" alt="image-20240429222930591"></p>
<ul>
<li>公式变换后，计算复杂度由$O(kn^2)$降到$O(kn)$。</li>
</ul>
<p>由于本文章需要将 FM 模型用在召回，故将二阶特征交互项拆分为用户和物品项。有：</p>
<p><img src="https://s2.loli.net/2024/04/29/mKQL1DdTsNwu2Ec.png" alt="image-20240429223104780"></p>
<h2 id="FM用于召回"><a href="#FM用于召回" class="headerlink" title="FM用于召回"></a>FM用于召回</h2><h2 id="2-2经典排序模型"><a href="#2-2经典排序模型" class="headerlink" title="2.2经典排序模型"></a>2.2经典排序模型</h2><h3 id="2-2-3Wide-amp-Deep系列"><a href="#2-2-3Wide-amp-Deep系列" class="headerlink" title="2.2.3Wide&amp;Deep系列"></a>2.2.3Wide&amp;Deep系列</h3><h3 id="2-2-1GBDT-LR简介"><a href="#2-2-1GBDT-LR简介" class="headerlink" title="2.2.1GBDT+LR简介"></a>2.2.1GBDT+LR简介</h3><p>前面介绍的协同过滤和矩阵分解存在的劣势就是<strong>仅利用了用户与物品相互行为信息进行推荐， 忽视了用户自身特征， 物品自身特征以及上下文信息等</strong>，导致生成的结果往往会比较片面。 而这次介绍的这个模型是2014年由Facebook提出的GBDT+LR模型， 该模型利用GBDT自动进行特征组合和选择， 进而生成新的离散特征向量， 再把该特征向量当做LR模型的输入， 来产生最后的预测结果， 该模型能够综合利用用户、物品和上下文等多种不同的特征， 生成较为全面的推荐结果， 在CTR点击率预估场景下使用较为广泛。</p>
<p>下面首先会介绍逻辑回归和GBDT模型各自的原理及优缺点， 然后介绍GBDT+LR模型的工作原理和细节。</p>
<h4 id="逻辑回归模型"><a href="#逻辑回归模型" class="headerlink" title="逻辑回归模型"></a>逻辑回归模型</h4><p>逻辑回归模型非常重要， 在推荐领域里面， 相比于传统的协同过滤， 逻辑回归模型能够综合利用用户、物品、上下文等多种不同的特征生成较为“全面”的推荐结果， 关于逻辑回归的更多细节， 可以参考下面给出的链接，这里只介绍比较重要的一些细节和在推荐中的应用。</p>
<p>逻辑回归是在线性回归的基础上加了一个 Sigmoid 函数（非线形）映射，使得逻辑回归成为了一个优秀的分类算法， 学习逻辑回归模型， 首先应该记住一句话：<strong>逻辑回归假设数据服从伯努利分布,通过极大似然的方法，运用梯度下降来求解参数，来达到将数据二分类的目的。</strong></p>
<p>相比于协同过滤和矩阵分解利用用户的物品“相似度”进行推荐， 逻辑回归模型将问题看成了一个分类问题， 通过预测正样本的概率对物品进行排序。这里的正样本可以是用户“点击”了某个商品或者“观看”了某个视频， 均是推荐系统希望用户产生的“正反馈”行为， 因此<strong>逻辑回归模型将推荐问题转化成了一个点击率预估问题</strong>。而点击率预测就是一个典型的二分类， 正好适合逻辑回归进行处理， 那么逻辑回归是如何做推荐的呢？ 过程如下：</p>
<ol>
<li>将用户年龄、性别、物品属性、物品描述、当前时间、当前地点等特征转成数值型向量</li>
<li>确定逻辑回归的优化目标，比如把点击率预测转换成二分类问题， 这样就可以得到分类问题常用的损失作为目标， 训练模型</li>
<li>在预测的时候， 将特征向量输入模型产生预测， 得到用户“点击”物品的概率</li>
<li>利用点击概率对候选物品排序， 得到推荐列表</li>
</ol>
<p>推断过程可以用下图来表示：</p>
<p>这里的关键就是每个特征的权重参数$w$， 我们一般是使用梯度下降的方式， 首先会先随机初始化参数$w$， 然后将特征向量（也就是我们上面数值化出来的特征）输入到模型， 就会通过计算得到模型的预测概率， 然后通过对目标函数求导得到每个$w$的梯度， 然后进行更新$w$</p>
<script type="math/tex; mode=display">
J(w) = - \frac{1}{m}(\sum_{i=1}^m (y^ilogf_w(x^i)+(1-y^i)log(1-f_w(x^i))))</script><p>求导之后：</p>
<script type="math/tex; mode=display">
w_j \leftarrow wj- \gamma \frac{1}{m}\sum_{i=1}^m (f_w(x^i) - y^i)x_j^i</script><p>这样通过若干次迭代， 就可以得到最终的𝑤<em>w</em>了， 关于这些公式的推导，可以参考下面给出的文章链接， 下面我们分析一下逻辑回归模型的优缺点。</p>
<p><strong>优点</strong></p>
<ol>
<li>LR模型形式简单，<strong>可解释性好</strong>，从特征的权重可以看到不同的特征对最后结果的影响。</li>
<li>训练时便于<strong>并行化</strong>，在预测时只需要对特征进行线性加权，所以<strong>性能比较好</strong>，往往适合处理<strong>海量id类特征</strong>，用id类特征有一个很重要的好处，就是<strong>防止信息损失</strong>（相对于范化的 CTR 特征），对于头部资源会有更细致的描述</li>
<li>资源占用小,尤其是内存。在实际的工程应用中只需要存储权重比较大的特征及特征对应的权重。</li>
<li>方便输出结果调整。逻辑回归可以很方便的得到最后的分类结果，因为输出的是每个样本的概率分数，我们可以很容易的对这些概率分数进行cutoff，也就是划分阈值(大于某个阈值的是一类，小于某个阈值的是一类)</li>
</ol>
<p><strong>当然， 逻辑回归模型也有一定的局限性</strong></p>
<ol>
<li>表达能力不强， 无法进行特征交叉， 特征筛选等一系列“高级“操作（这些工作都得人工来干， 这样就需要一定的经验， 否则会走一些弯路）， 因此可能造成信息的损失</li>
<li>准确率并不是很高。因为这毕竟是一个线性模型加了个sigmoid， 形式非常的简单(非常类似线性模型)，很难去拟合数据的真实分布</li>
<li>处理非线性数据较麻烦。逻辑回归在不引入其他方法的情况下，只能处理线性可分的数据， 如果想处理非线性， 首先对连续特征的处理需要先进行<strong>离散化</strong>（离散化的目的是为了引入非线性），如上文所说，人工分桶的方式会引入多种问题。</li>
<li>LR 需要进行<strong>人工特征组合</strong>，这就需要开发者有非常丰富的领域经验，才能不走弯路。这样的模型迁移起来比较困难，换一个领域又需要重新进行大量的特征工程。</li>
</ol>
<p>所以如何<strong>自动发现有效的特征、特征组合，弥补人工经验不足，缩短LR特征实验周期</strong>，是亟需解决的问题， 而GBDT模型， 正好可以<strong>自动发现特征并进行有效组合</strong></p>
<h4 id="GBDT模型"><a href="#GBDT模型" class="headerlink" title="GBDT模型"></a>GBDT模型</h4><p>GBDT全称梯度提升决策树，在传统机器学习算法里面是对真实分布拟合的最好的几种算法之一，在前几年深度学习还没有大行其道之前，gbdt在各种竞赛是大放异彩。原因大概有几个，一是效果确实挺不错。二是即可以用于分类也可以用于回归。三是可以筛选特征， 所以这个模型依然是一个非常重要的模型。</p>
<p>GBDT是通过采用加法模型(即基函数的线性组合），以及不断减小训练过程产生的误差来达到将数据分类或者回归的算法， 其训练过程如下：</p>
<p>gbdt通过多轮迭代， 每轮迭代会产生一个弱分类器， 每个分类器在上一轮分类器的残差基础上进行训练。 gbdt对弱分类器的要求一般是足够简单， 并且低方差高偏差。 因为训练的过程是通过降低偏差来不断提高最终分类器的精度。 由于上述高偏差和简单的要求，每个分类回归树的深度不会很深。最终的总分类器是将每轮训练得到的弱分类器加权求和得到的（也就是加法模型）。</p>
<p>关于GBDT的详细细节，依然是可以参考下面给出的链接。这里想分析一下GBDT如何来进行二分类的，因为我们要明确一点就是<strong>gbdt 每轮的训练是在上一轮的训练的残差基础之上进行训练的</strong>， 而这里的残差指的就是当前模型的负梯度值， 这个就要求每轮迭代的时候，弱分类器的输出的结果相减是有意义的， 而<strong>gbdt 无论用于分类还是回归一直都是使用的CART 回归树</strong>， 那么既然是回归树， 是如何进行二分类问题的呢？</p>
<p>GBDT 来解决二分类问题和解决回归问题的本质是一样的，都是通过不断构建决策树的方式，使预测结果一步步的接近目标值， 但是二分类问题和回归问题的损失函数是不同的， 关于GBDT在回归问题上的树的生成过程， 损失函数和迭代原理可以参考给出的链接， 回归问题中一般使用的是平方损失， 而二分类问题中， GBDT和逻辑回归一样， 使用的下面这个：</p>
<script type="math/tex; mode=display">
L=arg \, min [\sum_i^n-(y_ilog(p_i)+(1-y_i)log(1-p_i))]</script><p>其中， $y_i$是第$i$个样本的观测值， 取值要么是0要么是1， 而是$p_i$第$i$个样本的预测值， 取值是0-1之间的概率，由于我们知道GBDT拟合的残差是当前模型的负梯度， 那么我们就需要求出这个模型的导数， 即$\frac{dL}{dp_i}$， 对于某个特定的样本， 求导的话就可以只考虑它本身， 去掉加和号， 那么就变成了$\frac{dl}{dp_i}$， 其中$l$如下：</p>
<script type="math/tex; mode=display">
l=-y_ilog(p_i) - (1 - y_i)log(1-p_i) \\
=-y_i log(p_i) - log(1 - p_i) + y_i log(1-p_i) \\
=-y_i(log(\frac{p_i}{1-p_i})) - log(1-p_i)</script><p>如果对逻辑回归非常熟悉的话，$log(\frac{p_i}{1-p_i})$ 一定不会陌生吧， 这就是对几率比取了个对数， 并且在逻辑回归里面这个式子会等于$\theta X$，所以才推出了$p_i=\frac{1}{1+e^{- \theta X}}$的那个形式。 这里令$\eta_i=\frac{p_i}{1-p_i}$，即$p_i=\frac{\eta_i}{1+\eta_i}$，则上面这个式子变成了：</p>
<script type="math/tex; mode=display">
l=-y_ilog(\eta_i)-log(1-\frac{e^{log(\eta_i)}}{1+e^{log(\eta_i)}}) \\
= -y_ilog(\eta_i)-log(\frac{e^{log(\eta_i)}}{1+e^{log(\eta_i)}}) \\
=-y_ilog(\eta_i)+log(1+e^{log(\eta_i)}) \\</script><p>这时候，我们对$log(\eta_i)$求导， 得</p>
<script type="math/tex; mode=display">
\frac{dl}{dlog(\eta_i)}=-y_i+\frac{e^{log(\eta_i)}}{1+e^{log(\eta_i)}}=-y_i+p_i=p_i-y_i</script><p>这样， 我们就得到了某个训练样本在当前模型的梯度值了， 那么残差就是$y_i-p_i$。GBDT二分类的这个思想，其实和逻辑回归的思想一样，<strong>逻辑回归是用一个线性模型去拟合$P(y=1|x)$这个事件的对数几率$log\frac{p}{1-p}=\theta^Tx$</strong>， GBDT二分类也是如此， 用一系列的梯度提升树去拟合这个对数几率， 其分类模型可以表达为：</p>
<script type="math/tex; mode=display">
P(Y=1|x)=\frac{1}{1+e^{-F_M(x)}}</script><p>初始化GBDT 和回归问题一样， 分类 GBDT 的初始状态也只有一个叶子节点，该节点为所有样本的初始预测值，如下：</p>
<script type="math/tex; mode=display">
F_0(x)=arg \, min(\sum_{i=1}^n L(y, \gamma))</script><p>上式里面，$F$代表GBDT模型， $F_0$是模型的初始状态， 该式子的意思是找到一个，$\gamma$使所有样本的 Loss 最小，在这里及下文中，$\gamma$都表示节点的输出，即叶子节点， 且它是一个$log(\eta_i)$形式的值(回归值)，在初始状态，$\gamma=F_0$。</p>
<p>下面看例子(该例子来自下面的第二个链接)， 假设我们有下面3条样本：</p>
<p><img src="https://s2.loli.net/2024/04/29/IzmOvTwlacWe4XQ.png" alt="image-20240429104938229"></p>
<p>我们希望构建 GBDT 分类树，它能通过「喜欢爆米花」、「年龄」和「颜色偏好」这 3 个特征来预测某一个样本是否喜欢看电影。我们把数据代入上面的公式中求Loss:</p>
<script type="math/tex; mode=display">
Loss = L(1, \gamma) + L(1, \gamma) + L(0, \gamma)</script><p>为了令其最小， 我们求导， 且让导数为0， 则：</p>
<script type="math/tex; mode=display">
Loss = p - 1 + p - 1 + p - 0 = 0</script><p>于是， 就得到了初始值$p=\frac{2}{3}, \gamma=log(\frac{p}{1-p})=0.69$，模型的初始状态$F_0(x) = 0.69$</p>
<p>说了一大堆，<strong>实际上你却可以很容易的算出该模型的初始值，它就是正样本数比上负样本数的 log 值</strong>，例子中，正样本数为 2 个，负样本为 1 个，那么：</p>
<script type="math/tex; mode=display">
F_0(x)=\log(\frac{positive_count}{negative_count})=\log(\frac{2}{1})=0.69</script><ol>
<li><p>循环生成决策树 这里回忆一下回归树的生成步骤， 其实有4小步， 第一就是计算负梯度值得到残差， 第二步是用回归树拟合残差， 第三步是计算叶子节点的输出值， 第四步是更新模型。 下面我们一一来看：</p>
<ol>
<li><p>计算负梯度得到残差</p>
<script type="math/tex; mode=display">
r_{im} = - [\frac{\partial L(y_i, F(x_i))}{\partial F(x_i)}]_{F(x)=F_{m-1}(x)}</script><p>此处使用$m-1$棵树的模型， 计算每个样本的残差,$r_{im}$ 就是上面的$y_i- p_i$, 于是例子中， 每个样本的残差：</p>
<p><img src="https://s2.loli.net/2024/04/29/JiMa9vt7uXpqLyD.png" alt="image-20240429110324439"></p>
</li>
<li><p>使用回归树来拟合$\gamma_{jm}$， 这里的表$i$示样本哈，回归树的建立过程可以参考下面的链接文章，简单的说就是遍历每个特征， 每个特征下遍历每个取值， 计算分裂后两组数据的平方损失， 找到最小的那个划分节点。 假如我们产生的第2棵决策树如下：</p>
<p><img src="https://s2.loli.net/2024/04/29/HNqI53UDMw2V6zc.png" alt="image-20240429110613002"></p>
<p>对于每个叶子节点$j$, 计算最佳残差拟合值</p>
<script type="math/tex; mode=display">
\gamma_{jm}=\arg \min_r \sum_{x \in R_{ij}} L(y_i, F_{m-1}+\gamma)</script><p>意思是， 在刚构建的树$m$中， 找到每个节点$j$的输出$\gamma_{jm}$, 能使得该节点的loss最小。 那么我们看一下这个$\gamma$的求解方式， 这里非常的巧妙。 首先， 我们把损失函数写出来， 对于左边的第一个样本， 有</p>
<script type="math/tex; mode=display">
L(y_1,F_{m-1}(x_1)+\gamma)=-y_1(F_{m-1}(x_1)+\gamma)+log(1+e^{F_{m-1}(x_1)+\gamma})</script><p>这个式子就是上面推导的𝑙<em>l</em>， 因为我们要用回归树做分类， 所以这里把分类的预测概率转换成了对数几率回归的形式， 即$log(\eta_i)$， 这个就是模型的回归输出值。而如果求这个损失的最小值， 我们要求导， 解出令损失最小的$\gamma$。 但是上面这个式子求导会很麻烦， 所以这里介绍了一个技巧就是<strong>使用二阶泰勒公式来近似表示该式， 再求导</strong>， 还记得伟大的泰勒吗？</p>
<script type="math/tex; mode=display">
f(x+\Delta x) \approx f(x) + \Delta f'(x) + \frac{1}{2} \Delta x^2 f''(x) + O(\Delta x)</script><p>这里就相当于把$L(y<em>1,F</em>{m-1}(x_1))$当作常量$f(x)$，$\gamma$ 作为变量$\Delta x$，将$f(x)$二阶展开：</p>
<script type="math/tex; mode=display">
L(y_1,F_{m-1}(x_1) + \gamma) \approx L(y_1,F_{m-1}(x_1)) + L'(y_1,F_{m-1}(x_1)) \gamma + \frac{1}{2}L''(y_1,F_{m-1}(x_1)) \gamma^2</script><p>这时候再求导就简单了</p>
<script type="math/tex; mode=display">
\frac{dL}{d\gamma}=L'(y_1,F_{m-1}(x_1)) + L''(y_1,F_{m-1}(x_1))\gamma</script><p>Loss最小的时候， 上面的式子等于0， 就可以得到$\gamma$:</p>
<script type="math/tex; mode=display">
\gamma_{11}=\frac{-L'(y_1,F_{m-1}(x_1))}{L''(y_1,F_{m-1}(x_1))\gamma}</script><p><strong>因为分子就是残差(上述已经求到了)， 分母可以通过对残差求导，得到原损失函数的二阶导：</strong></p>
<p><img src="https://s2.loli.net/2024/04/29/TXCVQtEjUomghYB.png" alt="image-20240429114857610"></p>
</li>
</ol>
</li>
</ol>
<pre><code>  这时候， 就可以算出该节点的输出
  $$
  \gamma_&#123;11&#125;=\frac&#123;r_&#123;11&#125;&#125;&#123;p_&#123;10&#125;(1-p_&#123;10&#125;)&#125;=\frac&#123;0.33&#125;&#123;0.67 * 0.33&#125;=1.49
  $$
  这里的下面$\gamma_&#123;jm&#125;$表示第$m$棵树的第$j$个叶子节点。 接下来是右边节点的输出， 包含样本2和样本3， 同样使用二阶泰勒公式展开：

  ![image-20240429115219850](https://s2.loli.net/2024/04/29/kMBUL7gdFIsH5TJ.png)
</code></pre><p>​                求导， 令其结果为0，就会得到， 第1棵树的第2个叶子节点的输出：</p>
<p>​                <img src="https://s2.loli.net/2024/04/29/EqmsJanTiGpb4yt.png" alt="image-20240429115351409"></p>
<p>​                可以看出， 对于任意叶子节点， 我们可以直接计算其输出值：</p>
<p>​                </p>
<script type="math/tex; mode=display">
\gamma_{jm} = \frac{\sum_{i=1}^{R_{ij}}r_{im}}{\sum_{i=1}^{R_{ij}}p_{i,m-1}(1-p_{i,m-1})}</script><ol>
<li>更新模型$F_m(x)$</li>
</ol>
<p>​                </p>
<script type="math/tex; mode=display">
F_m(x) = F_{m-1}(x) + v \sum_{j=1}^{J_m} \gamma_m</script><p><strong>下面分析一下GBDT的优缺点：</strong></p>
<p>我们可以把树的生成过程理解成<strong>自动进行多维度的特征组合</strong>的过程，从根结点到叶子节点上的整个路径(多个特征值判断)，才能最终决定一棵树的预测值， 另外，对于<strong>连续型特征</strong>的处理，GBDT 可以拆分出一个临界阈值，比如大于 0.027 走左子树，小于等于 0.027（或者 default 值）走右子树，这样很好的规避了人工离散化的问题。这样就非常轻松的解决了逻辑回归那里<strong>自动发现特征并进行有效组合</strong>的问题， 这也是GBDT的优势所在。</p>
<p>但是GBDT也会有一些局限性， 对于<strong>海量的 id 类特征</strong>，GBDT 由于树的深度和棵树限制（防止过拟合），不能有效的存储；另外海量特征在也会存在性能瓶颈，当 GBDT 的 one hot 特征大于 10 万维时，就必须做分布式的训练才能保证不爆内存。所以 GBDT 通常配合少量的反馈 CTR 特征来表达，这样虽然具有一定的范化能力，但是同时会有<strong>信息损失</strong>，对于头部资源不能有效的表达。</p>
<p>所以， 我们发现其实<strong>GBDT和LR的优缺点可以进行互补</strong>。</p>
<h4 id="GBDT-LR模型"><a href="#GBDT-LR模型" class="headerlink" title="GBDT+LR模型"></a>GBDT+LR模型</h4><p>2014年， Facebook提出了一种利用GBDT自动进行特征筛选和组合， 进而生成新的离散特征向量， 再把该特征向量当做LR模型的输入， 来产生最后的预测结果， 这就是著名的GBDT+LR模型了。GBDT+LR 使用最广泛的场景是CTR点击率预估，即预测当给用户推送的广告会不会被用户点击。</p>
<h4 id="DeepFM"><a href="#DeepFM" class="headerlink" title="DeepFM"></a>DeepFM</h4><h4 id="动机-1"><a href="#动机-1" class="headerlink" title="动机"></a>动机</h4><p>对于CTR问题，被证明的最有效的提升任务表现的策略是特征组合(Feature Interaction), 在CTR问题的探究历史上来看就是如何更好地学习特征组合，进而更加精确地描述数据的特点。可以说这是基础推荐模型到深度学习推荐模型遵循的一个主要的思想。而组合特征大牛们研究过组合二阶特征，三阶甚至更高阶，但是面临一个问题就是随着阶数的提升，复杂度就成几何倍的升高。这样即使模型的表现更好了，但是推荐系统在实时性的要求也不能满足了。所以很多模型的出现都是为了解决另外一个更加深入的问题：如何更高效的学习特征组合？</p>
<p>为了解决上述问题，出现了FM和FFM来优化LR的特征组合较差这一个问题。并且在这个时候科学家们已经发现了DNN在特征组合方面的优势，所以又出现了FNN和PNN等使用深度网络的模型。但是DNN也存在局限性。</p>
<ul>
<li><strong>DNN局限</strong> 当我们使用DNN网络解决推荐问题的时候存在网络参数过于庞大的问题，这是因为在进行特征处理的时候我们需要使用one-hot编码来处理离散特征，这会导致输入的维度猛增。这里借用AI大会的一张图片：</li>
</ul>
<p><img src="https://s2.loli.net/2024/04/20/qpC37U1iFvQArsD.png" alt="image-20240420184743955"></p>
<p>这样庞大的参数量也是不实际的。为了解决DNN参数量过大的局限性，可以采用非常经典的Field思想，将OneHot特征转换为Dense Vector</p>
<p><img src="https://s2.loli.net/2024/04/20/RIcVbTMsCmjkwWO.png" alt="image-20240420184904998"></p>
<p>此时通过增加全连接层就可以实现高阶的特征组合，如下图所示：</p>
<p><img src="https://s2.loli.net/2024/04/20/ErF3zNLtidIesAv.png" alt="image-20240420185002554"></p>
<p>但是仍然缺少低阶的特征组合，于是增加FM来表示低阶的特征组合。</p>
<ul>
<li><strong>FNN和PNN</strong> 结合FM和DNN其实有两种方式，可以并行结合也可以串行结合。这两种方式各有几种代表模型。在DeepFM之前有FNN，虽然在影响力上可能并不如DeepFM，但是了解FNN的思想对我们理解DeepFM的特点和优点是很有帮助的。</li>
</ul>
<p><img src="https://s2.loli.net/2024/04/20/fDnehZHYQj91dLM.png" alt="image-20240420185121873"></p>
<p>FNN是使用预训练好的FM模块，得到隐向量，然后把隐向量作为DNN的输入，但是经过实验进一步发现，在Embedding layer和hidden layer1之间增加一个product层（如上图所示）可以提高模型的表现，所以提出了PNN，使用product layer替换FM预训练层。</p>
<ul>
<li><strong>Wide&amp;Deep</strong> FNN和PNN模型仍然有一个比较明显的尚未解决的缺点：对于低阶组合特征学习到的比较少，这一点主要是由于FM和DNN的串行方式导致的，也就是虽然FM学到了低阶特征组合，但是DNN的全连接结构导致低阶特征并不能在DNN的输出端较好的表现。看来我们已经找到问题了，将串行方式改进为并行方式能比较好的解决这个问题。于是Google提出了Wide&amp;Deep模型（将前几章），但是如果深入探究Wide&amp;Deep的构成方式，虽然将整个模型的结构调整为了并行结构，在实际的使用中Wide Module中的部分需要较为精巧的特征工程，换句话说人工处理对于模型的效果具有比较大的影响（这一点可以在Wide&amp;Deep模型部分得到验证）。</li>
</ul>
<p><img src="C:/Users/YL.YL.000/AppData/Roaming/Typora/typora-user-images/image-20240420223257343.png" alt="image-20240420223257343"></p>
<h3 id="2-2-2特征交叉"><a href="#2-2-2特征交叉" class="headerlink" title="2.2.2特征交叉"></a>2.2.2特征交叉</h3><h2 id="DCN"><a href="#DCN" class="headerlink" title="DCN"></a>DCN</h2><h2 id="动机-2"><a href="#动机-2" class="headerlink" title="动机"></a>动机</h2><p>Wide&amp;Deep模型的提出不仅综合了“记忆能力”和“泛化能力”， 而且开启了不同网络结构融合的新思路。 所以后面就有各式各样的模型改进Wide部分或者Deep部分， 而Deep&amp;Cross模型(DCN)就是其中比较典型的一个，这是2017年斯坦福大学和谷歌的研究人员在ADKDD会议上提出的， 该模型针对W&amp;D的wide部分进行了改进， 因为Wide部分有一个不足就是需要人工进行特征的组合筛选， 过程繁琐且需要经验， 而2阶的FM模型在线性的时间复杂度中自动进行特征交互，但是这些特征交互的表现能力并不够，并且随着阶数的上升，模型复杂度会大幅度提高。于是乎，作者用一个Cross Network替换掉了Wide部分，来自动进行特征之间的交叉，并且网络的时间和空间复杂度都是线性的。 通过与Deep部分相结合，构成了深度交叉网络（Deep &amp; Cross Network），简称DCN。</p>
<h2 id="模型结构及原理"><a href="#模型结构及原理" class="headerlink" title="模型结构及原理"></a>模型结构及原理</h2><p><img src="https://s2.loli.net/2024/05/02/1kzsxSruqDcEl2B.png" alt="image-20240502131845415"></p>
<p>这个模型的结构也是比较简洁的， 从下到上依次为：Embedding和Stacking层， Cross网络层与Deep网络层并列， 以及最后的输出层。下面也是一一为大家剖析。</p>
<h2 id="Embedding和Stacking层"><a href="#Embedding和Stacking层" class="headerlink" title="Embedding和Stacking层"></a>Embedding和Stacking层</h2><p>Embedding层我们已经非常的熟悉了吧， 这里的作用依然是把稀疏离散的类别型特征变成低维密集型。</p>
<script type="math/tex; mode=display">
X_{embed,i}=W_{embed,iX_i}</script><p>其中对于某一类稀疏分类特征（如id），$X<em>{embed,i}$是第$i$个分类值（id序号）的embedding向量。$W</em>{embed,i}$是embedding矩阵，$n_e \times n_v$维度，$n_e$是embedding维度，$n_v$是该类特征的唯一取值个数。$x_i$属于该特征的二元稀疏向量(one-hot)编码的。【实质上就是在训练得到的Embedding参数矩阵中找到属于当前样本对应的Embedding向量】。其实绝大多数基于深度学习的推荐模型都需要Embedding操作，参数学习是通过神经网络进行训练。</p>
<p>最后，该层需要将所有的密集型特征与通过embedding转换后的特征进行联合（Stacking）：</p>
<script type="math/tex; mode=display">
X_0=[X^T_{embed,1},...,X^T_{embed,k},X^T_{dense}]</script><p>一共𝑘<em>k</em>个类别特征， dense是数值型特征， 两者在特征维度拼在一块。 上面的这两个操作如果是看了前面的模型的话，应该非常容易理解了。</p>
<h2 id="Cross-NetWork"><a href="#Cross-NetWork" class="headerlink" title="Cross NetWork"></a>Cross NetWork</h2><p>这个就是本模型最大的亮点了【Cross网络】， 这个思路感觉非常Nice。设计该网络的目的是增加特征之间的交互力度。交叉网络由多个交叉层组成， 假设第$l$层的输出向量$x<em>l$， 那么对于第$l+1$层的输出向量$x</em>{l+1}$表示为：</p>
<script type="math/tex; mode=display">
x_{l+1}=x_0x_l^Tw_l+b_l+x_l=f(x_l,w_l,b_l) + x_l</script><p>可以看到， 交叉层的二阶部分非常类似PNN提到的外积操作， 在此基础上增加了外积操作的权重向量$w_l$， 以及原输入向量和$x_l$偏置向量$b_l$。 交叉层的可视化如下：</p>
<p><img src="https://s2.loli.net/2024/05/02/ui65yf3hXp1zjE4.png" alt="image-20240502133426935"></p>
<p>可以看到， 每一层增加了一个$n$维的权重向量$w_l$（n表示输入向量维度）， 并且在每一层均保留了输入向量， 因此输入和输出之间的变化不会特别明显。关于这一层， 原论文里面有个具体的证明推导Cross Network为啥有效， 不过比较复杂，这里我拿一个式子简单的解释下上面这个公式的伟大之处：</p>
<blockquote>
<p><strong>我们根据上面这个公式， 尝试的写前面几层看看:</strong></p>
<p>$l=0:\mathbf{x_1}=\mathbf{x_0}\mathbf{x_0^T}\mathbf{w_0}+\mathbf{b_0}+\mathbf{x_0}$</p>
<p>$l=1:\mathbf{x_2}=\mathbf{x_0}\mathbf{x_1^T}\mathbf{w_1}+\mathbf{b_1}+\mathbf{x_1}=\mathbf{x_0}[\mathbf{x_0}\mathbf{x_0^T}\mathbf{w_0}+\mathbf{b_0}+\mathbf{x_0}]^T\mathbf{w_1}+\mathbf{b_1}+\mathbf{x_1}$</p>
<p>$l=2:\mathbf{x_3}=\mathbf{x_0}\mathbf{x_2^T}\mathbf{w_2}+\mathbf{b_2}+\mathbf{x_2}=\mathbf{x_0}[\mathbf{x_0}[\mathbf{x_0}\mathbf{x_0^T}\mathbf{w_0}+\mathbf{b_0}+\mathbf{x_0}]^T\mathbf{w_1}+\mathbf{b_1}+\mathbf{x_1}]^T\mathbf{w_2}+\mathbf{b_2}+\mathbf{x_2}$</p>
</blockquote>
<p>我们暂且写到第3层的计算， 我们会发现什么结论呢？ 给大家总结一下：</p>
<ol>
<li><p>$x_1$中包含了所有的$x_0$的1,2阶特征的交互，$x_2$包含了所有的$x_1,x_0$的1、2、3阶特征的交互，$x_3$中包含了所有的$x_2,x_1,x_0$的交互，$x_0$的1、2、3、4阶特征交互。 因此， 交叉网络层的叉乘阶数是有限的。<strong>第$l$层特征对应的最高的叉乘阶数$l+1$</strong></p>
</li>
<li><p>Cross网络的参数是共享的， 每一层的这个权重特征之间共享， 这个可以使得模型泛化到看不见的特征交互作用， 并且对噪声更具有鲁棒性。 例如两个稀疏的特征$x_i,x_j$，它们在数据中几乎不发生交互，那么学习$x_i,x_j$的权重对于预测没有任何的意义。</p>
</li>
<li><p>计算交叉网络的参数数量。 假设交叉层的数量是$L_c$，特征$x$的维度是$n$， 那么总共的参数是：</p>
<script type="math/tex; mode=display">
n \times L_c \times 2</script><p>这个就是每一层会有$w$和$b$。且$w$维度和$x$的维度是一致的。</p>
</li>
<li><p>交叉网络的时间和空间复杂度是线性的。这是因为， 每一层都只有$w$和$b$ 没有激活函数的存在，相对于深度学习网络， 交叉网络的复杂性可以忽略不计。</p>
</li>
<li><p>Cross网络是FM的泛化形式， 在FM模型中， 特征$x<em>i$的权重是$v_i$，那么交叉项$x_i,x_j$的权重为$\langle x_i,x_j \rangle$。在DCN中，$x_i$的权重为${W</em>{K}^{(i)^l}}<em>{k=1}$， 交叉项$x_i,x_j$的权重是参数${W</em>{K}^{(i)^l}}<em>{k=1}$和${W</em>{K}^{(j)^l}}_{k=1}$的乘积，这个看上面那个例子展开感受下。因此两个模型都各自学习了独立于其他特征的一些参数，并且交叉项的权重是相应参数的某种组合。FM只局限于2阶的特征交叉(一般)，而DCN可以构建更高阶的特征交互， 阶数由网络深度决定，并且交叉网络的参数只依据输入的维度线性增长。</p>
</li>
<li><p>还有一点我们也要了解，对于每一层的计算中， 都会跟着$x_0$, 这个是咱们的原始输入， 之所以会乘以一个这个，是为了保证后面不管怎么交叉，都不能偏离我们的原始输入太远，别最后交叉交叉都跑偏了。</p>
</li>
<li><p>$\mathbf{x_{l+1}}=f(\mathbf{x_l},\mathbf{w_l}.\mathbf{b_l})+\mathbf{x_l}$, 这个东西其实有点跳远连接的意思，也就是和ResNet也有点相似，无形之中还能有效的缓解梯度消失现象。</p>
</li>
<li><p>好了， 关于本模型的交叉网络的细节就介绍到这里了。这应该也是本模型的精华之处了，后面就简单了。</p>
</li>
</ol>
<h2 id="Deep-Network"><a href="#Deep-Network" class="headerlink" title="Deep Network"></a>Deep Network</h2><p>这个就和上面的D&amp;W的全连接层原理一样。这里不再过多的赘述。</p>
<script type="math/tex; mode=display">
\mathbf{h_{l+1}}=f(w_l\mathbf{h_l}+\mathbf{b_l})</script><p>具体的可以参考W&amp;D模型。</p>
<h2 id="组合输出层"><a href="#组合输出层" class="headerlink" title="组合输出层"></a>组合输出层</h2><p>这个层负责将两个网络的输出进行拼接， 并且通过简单的Logistics回归完成最后的预测：</p>
<script type="math/tex; mode=display">
p=\sigma([\mathbf{x_{L_1}^T},\mathbf{h_{L_2}^T}]\mathbf{w}_{logits})</script><p>其中$\mathbf{x<em>{L_1}^T},\mathbf{h</em>{L_2}^T}$分别表示交叉网络和深度网络的输出。 最后二分类的损失函数依然是交叉熵损失：</p>
<script type="math/tex; mode=display">
loss=-\frac{1}{N}\sum_{i=1}^Ny_i\log(p_i)+(1-y_i)\log(1-p_i)+\lambda \sum_l ||\mathbf{w}_i||^2</script><p>Cross&amp;Deep模型的原理就是这些了，其核心部分就是Cross Network， 这个可以进行特征的自动交叉， 避免了更多基于业务理解的人工特征组合。 该模型相比于W&amp;D，Cross部分表达能力更强， 使得模型具备了更强的非线性学习能力。</p>
<h3 id="2-2-4序列模型"><a href="#2-2-4序列模型" class="headerlink" title="2.2.4序列模型"></a>2.2.4序列模型</h3><h2 id="DIN"><a href="#DIN" class="headerlink" title="DIN"></a>DIN</h2><h2 id="动机-3"><a href="#动机-3" class="headerlink" title="动机"></a>动机</h2><p>Deep Interest Network(DIIN)是2018年阿里巴巴提出来的模型， 该模型基于业务的观察，从实际应用的角度进行改进，相比于之前很多“学术风”的深度模型， 该模型更加具有业务气息。该模型的应用场景是阿里巴巴的电商广告推荐业务， 这样的场景下一般<strong>会有大量的用户历史行为信息</strong>， 这个其实是很关键的，因为DIN模型的创新点或者解决的问题就是使用了注意力机制来对用户的兴趣动态模拟， 而这个模拟过程存在的前提就是用户之前有大量的历史行为了，这样我们在预测某个商品广告用户是否点击的时候，就可以参考他之前购买过或者查看过的商品，这样就能猜测出用户的大致兴趣来，这样我们的推荐才能做的更加到位，所以这个模型的使用场景是<strong>非常注重用户的历史行为特征（历史购买过的商品或者类别信息）</strong>，也希望通过这一点，能够和前面的一些深度学习模型对比一下。</p>
<p>在个性化的电商广告推荐业务场景中，也正式由于用户留下了大量的历史交互行为，才更加看出了之前的深度学习模型(作者统称Embeding&amp;MLP模型)的不足之处。如果学习了前面的各种深度学习模型，就会发现Embeding&amp;MLP模型对于这种推荐任务一般有着差不多的固定处理套路，就是大量稀疏特征先经过embedding层， 转成低维稠密的，然后进行拼接，最后喂入到多层神经网络中去。</p>
<p>这些模型在这种个性化广告点击预测任务中存在的问题就是<strong>无法表达用户广泛的兴趣</strong>，因为这些模型在得到各个特征的embedding之后，就蛮力拼接了，然后就各种交叉等。这时候根本没有考虑之前用户历史行为商品具体是什么，究竟用户历史行为中的哪个会对当前的点击预测带来积极的作用。 而实际上，对于用户点不点击当前的商品广告，很大程度上是依赖于他的历史行为的，王喆老师举了个例子</p>
<blockquote>
<p>假设广告中的商品是键盘， 如果用户历史点击的商品中有化妆品， 包包，衣服， 洗面奶等商品， 那么大概率上该用户可能是对键盘不感兴趣的， 而如果用户历史行为中的商品有鼠标， 电脑，iPad，手机等， 那么大概率该用户对键盘是感兴趣的， 而如果用户历史商品中有鼠标， 化妆品， T-shirt和洗面奶， 鼠标这个商品embedding对预测“键盘”广告的点击率的重要程度应该大于后面的那三个。</p>
</blockquote>
<p>这里也就是说如果是之前的那些深度学习模型，是没法很好的去表达出用户这广泛多样的兴趣的，如果想表达的准确些， 那么就得加大隐向量的维度，让每个特征的信息更加丰富， 那这样带来的问题就是计算量上去了，毕竟真实情景尤其是电商广告推荐的场景，特征维度的规模是非常大的。 并且根据上面的例子， 也<strong>并不是用户所有的历史行为特征都会对某个商品广告点击预测起到作用</strong>。所以对于当前某个商品广告的点击预测任务，没必要考虑之前所有的用户历史行为。</p>
<p>这样， DIN的动机就出来了，在业务的角度，我们应该自适应的去捕捉用户的兴趣变化，这样才能较为准确的实施广告推荐；而放到模型的角度， 我们应该<strong>考虑到用户的历史行为商品与当前商品广告的一个关联性</strong>，如果用户历史商品中很多与当前商品关联，那么说明该商品可能符合用户的品味，就把该广告推荐给他。而一谈到关联性的话， 我们就容易想到“注意力”的思想了， 所以为了更好的从用户的历史行为中学习到与当前商品广告的关联性，学习到用户的兴趣变化， 作者把注意力引入到了模型，设计了一个”local activation unit”结构，利用候选商品和历史问题商品之间的相关性计算出权重，这个就代表了对于当前商品广告的预测，用户历史行为的各个商品的重要程度大小， 而加入了注意力权重的深度学习网络，就是这次的主角DIN， 下面具体来看下该模型。</p>
<h2 id="DIN模型结构及原理"><a href="#DIN模型结构及原理" class="headerlink" title="DIN模型结构及原理"></a>DIN模型结构及原理</h2><p>在具体分析DIN模型之前， 我们还得先介绍两块小内容，一个是DIN模型的数据集和特征表示， 一个是上面提到的之前深度学习模型的基线模型， 有了这两个， 再看DIN模型，就感觉是水到渠成了。</p>
<h2 id="特征表示"><a href="#特征表示" class="headerlink" title="特征表示"></a>特征表示</h2><p>工业上的CTR预测数据集一般都是<code>multi-group categorial form</code>的形式，就是类别型特征最为常见，这种数据集一般长这样：</p>
<p>这里的亮点就是框出来的那个特征，这个包含着丰富的用户兴趣信息。</p>
<p>对于特征编码，作者这里举了个例子：<code>[weekday=Friday, gender=Female, visited_cate_ids=&#123;Bag,Book&#125;, ad_cate_id=Book]</code>， 这种情况我们知道一般是通过one-hot的形式对其编码， 转成系数的二值特征的形式。但是这里我们会发现一个<code>visted_cate_ids</code>， 也就是用户的历史商品列表， 对于某个用户来讲，这个值是个多值型的特征， 而且还要知道这个特征的长度不一样长，也就是用户购买的历史商品个数不一样多，这个显然。这个特征的话，我们一般是用到multi-hot编码，也就是可能不止1个1了，有哪个商品，对应位置就是1， 所以经过编码后的数据长下面这个样子：</p>
<p>这个就是喂入模型的数据格式了，这里还要注意一点 就是上面的特征里面没有任何的交互组合，也就是没有做特征交叉。这个交互信息交给后面的神经网络去学习。</p>
<h2 id="基线模型"><a href="#基线模型" class="headerlink" title="基线模型"></a>基线模型</h2><p>这里的base 模型，就是上面提到过的Embedding&amp;MLP的形式， 这个之所以要介绍，就是因为DIN网络的基准也是他，只不过在这个的基础上添加了一个新结构(注意力网络)来学习当前候选广告与用户历史行为特征的相关性，从而动态捕捉用户的兴趣。</p>
<p>基准模型的结构相对比较简单，我们前面也一直用这个基准， 分为三大模块：Embedding layer，Pooling &amp; Concat layer和MLP， 结构如下:</p>
<p>前面的大部分深度模型结构也是遵循着这个范式套路， 简介一下各个模块。</p>
<ol>
<li><p><strong>Embedding layer</strong>：这个层的作用是把高维稀疏的输入转成低维稠密向量， 每个离散特征下面都会对应着一个embedding词典， 维度是$D \times K$， 这里的$D$表示的是隐向量的维度， 而表$K$示的是当前离散特征的唯一取值个数, 这里为了好理解，这里举个例子说明，就比如上面的weekday特征：</p>
<blockquote>
<p>假设某个用户的weekday特征就是周五，化成one-hot编码的时候，就是[0,0,0,0,1,0,0]表示，这里如果再假设隐向量维度是D， 那么这个特征对应的embedding词典是一个$D\times7$的一个矩阵(每一列代表一个embedding，7列正好7个embedding向量，对应周一到周日)，那么该用户这个one-hot向量经过embedding层之后会得到一个𝐷×1<em>D</em>×1的向量，也就是周五对应的那个embedding，怎么算的，其实就是$embedding矩阵*[0,0,0,0,1,0,0]^T$。其实也就是直接把embedding矩阵中one-hot向量为1的那个位置的embedding向量拿出来。 这样就得到了稀疏特征的稠密向量了。其他离散特征也是同理，只不过上面那个multi-hot编码的那个，会得到一个embedding向量的列表，因为他开始的那个multi-hot向量不止有一个是1，这样乘以embedding矩阵，就会得到一个列表了。通过这个层，上面的输入特征都可以拿到相应的稠密embedding向量了。</p>
</blockquote>
</li>
<li><p><strong>pooling layer and Concat layer</strong>： pooling层的作用是将用户的历史行为embedding这个最终变成一个定长的向量，因为每个用户历史购买的商品数是不一样的， 也就是每个用户multi-hot中1的个数不一致，这样经过embedding层，得到的用户历史行为embedding的个数不一样多，也就是上面的embedding列表$t_i$不一样长， 那么这样的话，每个用户的历史行为特征拼起来就不一样长了。 而后面如果加全连接网络的话，我们知道，他需要定长的特征输入。 所以往往用一个pooling layer先把用户历史行为embedding变成固定长度(统一长度)，所以有了这个公式：</p>
<script type="math/tex; mode=display">
e_i=pooling(e_{i1},e_{i2},...,e_{ik})</script><p>这里的$e_{ij}$是用户历史行为的那些embedding。就$e_i$变成了定长的向量， 这里的$e_i$表示第$i$个历史特征组(是历史行为，比如历史的商品id，历史的商品类别id等)， 这里的$k$表示对应历史特种组里面用户购买过的商品数量，也就是历史embedding的数量，看上面图里面的user behaviors系列，就是那个过程了。 Concat layer层的作用就是拼接了，就是把这所有的特征embedding向量，如果再有连续特征的话也算上，从特征维度拼接整合，作为MLP的输入。</p>
</li>
<li><p><strong>MLP</strong>：这个就是普通的全连接，用了学习特征之间的各种交互。</p>
</li>
<li><p><strong>Loss</strong>: 由于这里是点击率预测任务， 二分类的问题，所以这里的损失函数用的负的log对数似然：</p>
<script type="math/tex; mode=display">
L=-\frac{1}{N}\sum_{(\mathbf{x},y)\in S}(y \log p(x) + (1-y) \log (1-p(\mathbf(x))))</script></li>
</ol>
<p>这就是base 模型的全貌， 这里应该能看出这种模型的问题， 通过上面的图也能看出来， 用户的历史行为特征和当前的候选广告特征在全都拼起来给神经网络之前，是一点交互的过程都没有， 而拼起来之后给神经网络，虽然是有了交互了，但是原来的一些信息，比如，<strong>每个历史商品的信息会丢失了一部分，因为这个与当前候选广告商品交互的是池化后的历史特征embedding， 这个embedding是综合了所有的历史商品信息， 这个通过我们前面的分析，对于预测当前广告点击率，并不是所有历史商品都有用，综合所有的商品信息反而会增加一些噪声性的信息</strong>，可以联想上面举得那个键盘鼠标的例子，如果加上了各种洗面奶，衣服啥的反而会起到反作用。其次就是这样综合起来，已经没法再看出到底用户历史行为中的哪个商品与当前商品比较相关，也就是丢失了历史行为中各个商品对当前预测的重要性程度。最后一点就是如果所有用户浏览过的历史行为商品，最后都通过embedding和pooling转换成了固定长度的embedding，这样会限制模型学习用户的多样化兴趣。</p>
<p>那么改进这个问题的思路有哪些呢？ 第一个就是加大embedding的维度，增加之前各个商品的表达能力，这样即使综合起来，embedding的表达能力也会加强， 能够蕴涵用户的兴趣信息，但是这个在大规模的真实推荐场景计算量超级大，不可取。 另外一个思路就是<strong>在当前候选广告和用户的历史行为之间引入注意力的机制</strong>，这样在预测当前广告是否点击的时候，让模型更关注于与当前广告相关的那些用户历史产品，也就是说<strong>与当前商品更加相关的历史行为更能促进用户的点击行为</strong>。 作者这里又举了之前的一个例子：</p>
<blockquote>
<p><strong>想象一下，当一个年轻母亲访问电子商务网站时，她发现展示的新手袋很可爱，就点击它。让我们来分析一下点击行为的驱动力。</strong></p>
<p><strong>展示的广告通过软搜索这位年轻母亲的历史行为，发现她最近曾浏览过类似的商品，如大手提袋和皮包，从而击中了她的相关兴趣</strong></p>
</blockquote>
<p>第二个思路就是DIN的改进之处了。DIN通过给定一个候选广告，然后去注意与该广告相关的局部兴趣的表示来模拟此过程。 <strong>DIN不会通过使用同一向量来表达所有用户的不同兴趣，而是通过考虑历史行为的相关性来自适应地计算用户兴趣的表示向量（对于给的广告）。 该表示向量随不同广告而变化。</strong>下面看一下DIN模型。</p>
<h2 id="DIN模型架构"><a href="#DIN模型架构" class="headerlink" title="DIN模型架构"></a>DIN模型架构</h2><p>上面分析完了base模型的不足和改进思路之后，DIN模型的结构就呼之欲出了，首先，它依然是采用了基模型的结构，只不过是在这个的基础上加了一个注意力机制来学习用户兴趣与当前候选广告间的关联程度， 用论文里面的话是，引入了一个新的<code>local activation unit</code>， 这个东西用在了用户历史行为特征上面， <strong>能够根据用户历史行为特征和当前广告的相关性给用户历史行为特征embedding进行加权</strong>。我们先看一下它的结构，然后看一下这个加权公式。</p>
<p>这里改进的地方已经框出来了，这里会发现相比于base model， 这里加了一个local activation unit， 这里面是一个前馈神经网络，输入是用户历史行为商品和当前的候选商品， 输出是它俩之间的相关性， 这个相关性相当于每个历史商品的权重，把这个权重与原来的历史行为embedding相乘求和就得到了用户的兴趣表示𝑣𝑈(𝐴)<strong>v*</strong>U<em>(</em>A*), 这个东西的计算公式如下：</p>
<h1 id="第四章-推荐系统算法面经"><a href="#第四章-推荐系统算法面经" class="headerlink" title="第四章 推荐系统算法面经"></a>第四章 推荐系统算法面经</h1><h2 id="4-1ML与DL基础"><a href="#4-1ML与DL基础" class="headerlink" title="4.1ML与DL基础"></a>4.1ML与DL基础</h2><h2 id="机器学习"><a href="#机器学习" class="headerlink" title="机器学习"></a>机器学习</h2><ol>
<li>介绍一个最熟悉的机器学习算法</li>
</ol>
<h2 id="参考解析"><a href="#参考解析" class="headerlink" title="参考解析"></a>参考解析</h2><h3 id="机器学习-1"><a href="#机器学习-1" class="headerlink" title="机器学习"></a>机器学习</h3><ol>
<li><p>介绍一个最熟悉的机器学习算法</p>
<ul>
<li><p>LR：逻辑回归是假设数据服从<strong>伯努利分布</strong>，通过<strong>极大似然估计方法</strong>，使用<strong>梯度下降</strong>来求解参数，达到二分类目的的一个模型。我们在考虑把广义线性模型用于分类的时候，需要如何确定逻辑边界，感知机模型用的是阶跃函数，但是阶跃函数不可导，不能作为广义线性模型的联系函数。逻辑回归对数几率函数代替阶跃函数。因为对数几率函数是单调可微的一个函数，所以可以作为联系函数。所以逻辑回归本质上还是广义线性模型。</p>
</li>
<li><p>LR的优缺点：</p>
<ul>
<li>形式简单，可解释性好；</li>
<li>它直接对分类概率进行建模，不需要知道真实数据的分布，这和生成式模型相区别，避免了假设错误带来的问题；</li>
<li>不仅能够预测出类别，还能够预测出概率，能够用于很多场景，比如ctr排序中；</li>
<li>对数几率函数任意阶数可导，能够很容易优化；</li>
<li>可以获得特征权重，方便我们进行特征筛选；</li>
<li>训练速度快；</li>
<li>它对稀疏特征效果比较好，因为使用的是w1 w2 w3本质上的线性模型，稀疏数据能够筛选出不稀疏的重要特征。</li>
<li>模型表达能力有限；</li>
<li>样本不均衡很难处理；</li>
<li>在非线性可分数据集上性能有限；</li>
</ul>
</li>
<li><p>LR推导：</p>
<p><img src="https://s2.loli.net/2024/05/02/AH9h3XGcQL6nBFr.png" alt="image-20240502234225932"></p>
</li>
</ul>
</li>
<li><p>决策树怎么建树，基尼系数公式</p>
<ul>
<li><p>决策树建树算法有三种ID3、C4.5、CART，每个算法主要考虑的事情主要有三个问题：</p>
<ul>
<li>选什么特征来当条件？</li>
<li>条件判断的属性值是什么？</li>
<li>什么时候停止分裂，达到我们需要的决策？</li>
</ul>
</li>
<li><p>CART</p>
<ul>
<li><p>CART树采用基尼系数进行最优特征的选择，构造过程中假设有K类，则样本属于第K类的概率为pk，则定义样本分布的<strong>基尼系数</strong>为：</p>
<script type="math/tex; mode=display">
Gini(p)=\sum_{k=1}^m p_k(1-p_k)=1-\sum_{k=1}^K p_k^2</script><p>根据基尼系数定义，可以得到样本集合D的<strong>基尼指数</strong>，其中ck表述样本集合中第k类的子集：</p>
<script type="math/tex; mode=display">
Gini(D)=1-\sum_{k=1}^K(\frac{C_k}{D})^2</script><p>如果数据集D根据特征A在某一取值a上进行分割，得到D1,D2两部分后，那么在特征A下集合D的基尼系数如下所示。其中基尼系数Gini(D)表示集合D的不确定性，基尼系数Gini(D,A)表示A=a分割后集合D的不确定性。<strong>基尼指数越大，样本集合的不确定性越大</strong>。</p>
<script type="math/tex; mode=display">
Gain\_Gini(D,A)=\frac{D_1}{D}Gini(D_1)+\frac{D_2}{D}Gini(D_2)</script><blockquote>
<p>选什么特征作为最优特征分割：当我们计算完所有特征基尼指数后，选择其中最小所在特征的作为分裂特征；</p>
<p>条件判断的属性值是什么：判断特征属性是否为为此最指数来分裂；</p>
<p>什么时候停止分裂，达到我们需要的决策：分裂的最小收益小于我们的划定的阈值，或者树的深度达到我们的阈值。</p>
</blockquote>
</li>
</ul>
</li>
</ul>
</li>
<li><p>Adaboost拟合目标是什么</p>
<ul>
<li>Adaboost中每训练完一个弱分类器都就会调整权重，上一轮训练中被误分类的点的权重会增加，在本轮训练中，由于权重影响，本轮的弱分类器将更有可能把上一轮的误分类点分对，如果还是没有分对，那么分错的点的权重将继续增加，下一个弱分类器将更加关注这个点，这是adaboost目标。</li>
</ul>
</li>
<li><p>Adaboost介绍一下，每个基学习器的权重怎么得到的</p>
</li>
<li><p>介绍下GBDT</p>
</li>
<li><p>介绍XGBoost</p>
</li>
<li><p>介绍下LightGBM</p>
</li>
<li><p>LightGBM相对于XGBoost的改进</p>
</li>
<li><p>GBDT中的梯度是什么，怎么用</p>
</li>
<li><p>GBDT如何计算特征重要性</p>
</li>
<li><p>GBDT讲一下，GBDT拟合残差，是真实的误差嘛，在什么情况下看做是真实的误差</p>
</li>
</ol>
<hr>
<p>references:</p>
<ul>
<li>‘<a target="_blank" rel="noopener" href="https://datawhalechina.github.io/fun-rec/#/ch02/ch2.1/ch2.1.1/usercf">datawhale-FunRec)</a>‘</li>
<li><a target="_blank" rel="noopener" href="https://mp.weixin.qq.com/s/ZtnaQrVIpVOPJpqMdLWOcw">https://mp.weixin.qq.com/s/ZtnaQrVIpVOPJpqMdLWOcw</a></li>
<li><a target="_blank" rel="noopener" href="https://chenk.tech/posts/8ad63d9d.html">https://chenk.tech/posts/8ad63d9d.html</a></li>
<li>B站黑马推荐系统实战课程</li>
</ul>
<hr>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN,en,default">
    <link itemprop="mainEntityOfPage" href="http://humble2967738843.github.io/2024/03/23/tong-ji-xue-xi-fang-fa/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="院龙">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="院龙">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2024/03/23/tong-ji-xue-xi-fang-fa/" class="post-title-link" itemprop="url">未命名</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2024-03-23 12:07:25 / 修改时间：12:15:56" itemprop="dateCreated datePublished" datetime="2024-03-23T12:07:25+08:00">2024-03-23</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="10-1-提升方法AdaBoost算法"><a href="#10-1-提升方法AdaBoost算法" class="headerlink" title="10.1 提升方法AdaBoost算法"></a>10.1 提升方法AdaBoost算法</h1><p><img src="https://s2.loli.net/2024/03/23/qSx2yrfQHYu86ij.png" alt="image-20240323120941983"></p>
<p><img src="https://s2.loli.net/2024/03/23/hUxJme2paKIswVn.png" alt="image-20240323121244084"></p>
<p><img src="C:/Users/YL.YL.000/AppData/Roaming/Typora/typora-user-images/image-20240323121450856.png" alt="image-20240323121450856"></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN,en,default">
    <link itemprop="mainEntityOfPage" href="http://humble2967738843.github.io/2024/01/16/53-li-jie-dropout/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="院龙">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="院龙">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2024/01/16/53-li-jie-dropout/" class="post-title-link" itemprop="url">53理解Dropout</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2024-01-16 11:26:08" itemprop="dateCreated datePublished" datetime="2024-01-16T11:26:08+08:00">2024-01-16</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2024-05-26 13:16:44" itemprop="dateModified" datetime="2024-05-26T13:16:44+08:00">2024-05-26</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="53-理解-Dropout"><a href="#53-理解-Dropout" class="headerlink" title="53 理解 Dropout"></a>53 理解 Dropout</h1><h2 id="53-1Why-does-drop-out-work"><a href="#53-1Why-does-drop-out-work" class="headerlink" title="53.1Why does drop-out work?"></a>53.1Why does drop-out work?</h2><p><img src="C:/Users/YL.YL.000/AppData/Roaming/Typora/typora-user-images/image-20240113212243807.png" alt="image-20240113212243807"></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN,en,default">
    <link itemprop="mainEntityOfPage" href="http://humble2967738843.github.io/2024/01/15/xi-gua-shu/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="院龙">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="院龙">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2024/01/15/xi-gua-shu/" class="post-title-link" itemprop="url">西瓜书</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2024-01-15 22:04:27" itemprop="dateCreated datePublished" datetime="2024-01-15T22:04:27+08:00">2024-01-15</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2024-05-26 13:18:39" itemprop="dateModified" datetime="2024-05-26T13:18:39+08:00">2024-05-26</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="0204-K折交叉验证"><a href="#0204-K折交叉验证" class="headerlink" title="0204 K折交叉验证"></a>0204 K折交叉验证</h1><p><img src="C:/Users/YL.YL.000/AppData/Roaming/Typora/typora-user-images/image-20240115195857331.png" alt="image-20240115195857331"></p>
<h1 id="0205-测试集分割留出法"><a href="#0205-测试集分割留出法" class="headerlink" title="0205 测试集分割留出法"></a>0205 测试集分割留出法</h1><p><img src="https://s2.loli.net/2024/01/15/hGeNuPqEw2KdamL.png" alt="image-20240115200322730"></p>
<h1 id="0206-验证集"><a href="#0206-验证集" class="headerlink" title="0206 验证集"></a>0206 验证集</h1><p><img src="https://s2.loli.net/2024/01/15/abAQVWwImtsRSu8.png" alt="image-20240115200552460"></p>
<h1 id="0207-均方误差"><a href="#0207-均方误差" class="headerlink" title="0207 均方误差"></a>0207 均方误差</h1><p><img src="C:/Users/YL.YL.000/AppData/Roaming/Typora/typora-user-images/image-20240115200948281.png" alt="image-20240115200948281"></p>
<h1 id="0208错误率与精确度公式"><a href="#0208错误率与精确度公式" class="headerlink" title="0208错误率与精确度公式"></a>0208错误率与精确度公式</h1><p><img src="https://s2.loli.net/2024/01/15/v3huHoQinVwDErU.png" alt="image-20240115201639265"></p>
<p><img src="C:/Users/YL.YL.000/AppData/Roaming/Typora/typora-user-images/image-20240115201820121.png" alt="image-20240115201820121"></p>
<h1 id="0209查准率与查全率"><a href="#0209查准率与查全率" class="headerlink" title="0209查准率与查全率"></a>0209查准率与查全率</h1><p><img src="C:/Users/YL.YL.000/AppData/Roaming/Typora/typora-user-images/image-20240115202522563.png" alt="image-20240115202522563"></p>
<h1 id="0210P-R反向关系原理"><a href="#0210P-R反向关系原理" class="headerlink" title="0210P-R反向关系原理"></a>0210P-R反向关系原理</h1><p><img src="C:/Users/YL.YL.000/AppData/Roaming/Typora/typora-user-images/image-20240115204224315.png" alt="image-20240115204224315"></p>
<p><img src="C:/Users/YL.YL.000/AppData/Roaming/Typora/typora-user-images/image-20240115204442507.png" alt="image-20240115204442507"></p>
<p><img src="https://s2.loli.net/2024/01/15/EvtV3X8fITgd74y.png" alt="image-20240115205515474"></p>
<h1 id="0211-P-R反向关系图"><a href="#0211-P-R反向关系图" class="headerlink" title="0211 P-R反向关系图"></a>0211 P-R反向关系图</h1><p><img src="https://s2.loli.net/2024/01/15/cjlvL1oNHBwpfs5.png" alt="image-20240115205723559"></p>
<p><img src="https://s2.loli.net/2024/01/15/PzjYBU16kfqFIEw.png" alt="image-20240115205822764"></p>
<p><img src="https://s2.loli.net/2024/01/15/7Q1jGOzvUreRHb4.png" alt="image-20240115210156902"></p>
<h1 id="0212-P-R加权调和平均数"><a href="#0212-P-R加权调和平均数" class="headerlink" title="0212 P-R加权调和平均数"></a>0212 P-R加权调和平均数</h1><p><img src="C:/Users/YL.YL.000/AppData/Roaming/Typora/typora-user-images/image-20240115210607952.png" alt="image-20240115210607952"></p>
<h1 id="0213-macro-microP-R"><a href="#0213-macro-microP-R" class="headerlink" title="0213 macro_microP-R"></a>0213 macro_microP-R</h1><p><img src="https://s2.loli.net/2024/01/15/lk24DTPKhtnrVUb.png" alt="image-20240115211245502"></p>
<p><img src="https://s2.loli.net/2024/01/15/T7xLYHR9BIOsC5S.png" alt="image-20240115211401246"></p>
<h1 id="0214-使用P-R曲线比较不同模型"><a href="#0214-使用P-R曲线比较不同模型" class="headerlink" title="0214 使用P-R曲线比较不同模型"></a>0214 使用P-R曲线比较不同模型</h1><p><img src="C:/Users/YL.YL.000/AppData/Roaming/Typora/typora-user-images/image-20240115211805687.png" alt="image-20240115211805687"></p>
<h1 id="0215-ROC曲线"><a href="#0215-ROC曲线" class="headerlink" title="0215 ROC曲线"></a>0215 ROC曲线</h1><p><img src="https://s2.loli.net/2024/01/15/kVMrjETeX3gQJH6.png" alt="image-20240115212739034"></p>
<h1 id="0216-排序损失rank-loss"><a href="#0216-排序损失rank-loss" class="headerlink" title="0216 排序损失rank-loss"></a>0216 排序损失rank-loss</h1><p><img src="https://s2.loli.net/2024/01/20/YoFNSMmGAjLP42d.png" alt="image-20240120205407448"></p>
<h1 id="0217-AUC与rank-loss"><a href="#0217-AUC与rank-loss" class="headerlink" title="0217 AUC与rank-loss"></a>0217 AUC与rank-loss</h1><p><img src="https://s2.loli.net/2024/01/20/wEkmlAtCs8iM2fV.png" alt="image-20240120205845857"></p>
<h1 id="0403-熵的度量：一般分布"><a href="#0403-熵的度量：一般分布" class="headerlink" title="0403 熵的度量：一般分布"></a>0403 熵的度量：一般分布</h1><p><img src="https://s2.loli.net/2024/03/16/WkYKBxvsw5odjzF.png" alt="image-20240316115401863"></p>
<p><img src="https://s2.loli.net/2024/03/16/wDVlFmr1bvH9ZX3.png" alt="image-20240316115626781"></p>
<h1 id="0404-信息的度量：信息增益"><a href="#0404-信息的度量：信息增益" class="headerlink" title="0404 信息的度量：信息增益"></a>0404 信息的度量：信息增益</h1><p><img src="https://s2.loli.net/2024/03/16/ZtgFwHrE6BiU8Gq.png" alt="image-20240316120030435"></p>
<h1 id="0405-决策树ID3算法举例：好坏西瓜"><a href="#0405-决策树ID3算法举例：好坏西瓜" class="headerlink" title="0405 决策树ID3算法举例：好坏西瓜"></a>0405 决策树ID3算法举例：好坏西瓜</h1><p><img src="https://s2.loli.net/2024/03/16/ikfwX93A8WdqLOu.png" alt="image-20240316120857159"></p>
<h1 id="0406-好坏西瓜继续分叉"><a href="#0406-好坏西瓜继续分叉" class="headerlink" title="0406 好坏西瓜继续分叉"></a>0406 好坏西瓜继续分叉</h1><p><img src="https://s2.loli.net/2024/03/16/ayjosxPpOLZBDM1.png" alt="image-20240316121121335"></p>
<h1 id="0407-增益率简述"><a href="#0407-增益率简述" class="headerlink" title="0407 增益率简述"></a>0407 增益率简述</h1><p><img src="https://s2.loli.net/2024/03/16/1CFTHWcjtxLqeBh.png" alt="image-20240316121421026"></p>
<h1 id="0408-决策树CART基尼指数"><a href="#0408-决策树CART基尼指数" class="headerlink" title="0408 决策树CART基尼指数"></a>0408 决策树CART基尼指数</h1><p><img src="https://s2.loli.net/2024/03/16/27uWCaZ6Hwnx5jL.png" alt="image-20240316123131555"></p>
<h1 id="0409-基尼指数计算：第一次分叉"><a href="#0409-基尼指数计算：第一次分叉" class="headerlink" title="0409 基尼指数计算：第一次分叉"></a>0409 基尼指数计算：第一次分叉</h1><p><img src="https://s2.loli.net/2024/03/16/rKPUiZnYWVGwE1s.png" alt="image-20240316123340068"></p>
<h3 id="0410-基尼指数计算：第二次分叉"><a href="#0410-基尼指数计算：第二次分叉" class="headerlink" title="0410 基尼指数计算：第二次分叉"></a>0410 基尼指数计算：第二次分叉</h3><p><img src="https://s2.loli.net/2024/03/16/4bJHnsZlvGtdVWK.png" alt="image-20240316124037637"></p>
<h1 id="0411-基尼指数计算：第三次分叉与终止"><a href="#0411-基尼指数计算：第三次分叉与终止" class="headerlink" title="0411 基尼指数计算：第三次分叉与终止"></a>0411 基尼指数计算：第三次分叉与终止</h1><p><img src="https://s2.loli.net/2024/03/16/9lfWY4NLZvUBcui.png" alt="image-20240316124324228"></p>
<h1 id="0412-CART算法回归树"><a href="#0412-CART算法回归树" class="headerlink" title="0412 CART算法回归树"></a>0412 CART算法回归树</h1><p><img src="https://s2.loli.net/2024/03/18/eNxatXS5rqBE39U.png" alt="image-20240318113957124"></p>
<p>多个特征，第一步算法SSR最小值</p>
<p>防止过拟合：如设定小于20不再分割</p>
<h1 id="0413-剪枝处理"><a href="#0413-剪枝处理" class="headerlink" title="0413 剪枝处理"></a>0413 剪枝处理</h1><p>在训练集上使用信息增益得到决策树：</p>
<p><img src="https://s2.loli.net/2024/03/18/co9km6H3lQ2b54Z.png" alt="image-20240318114909636"></p>
<p><img src="https://s2.loli.net/2024/03/18/ZhTqdG27J4sujfF.png" alt="image-20240318115034225"></p>
<h1 id="0414-预剪枝一"><a href="#0414-预剪枝一" class="headerlink" title="0414 预剪枝一"></a>0414 预剪枝一</h1><p>预剪枝是一种贪心算法，能剪就剪</p>
<p><img src="https://s2.loli.net/2024/03/18/o5EzXv27wIGZmsL.png" alt="image-20240318115415202"></p>
<h1 id="0415-预剪枝二"><a href="#0415-预剪枝二" class="headerlink" title="0415 预剪枝二"></a>0415 预剪枝二</h1><p><img src="https://s2.loli.net/2024/03/18/FE1zPHNbGuDvVUp.png" alt="image-20240318115627704"></p>
<p><img src="https://s2.loli.net/2024/03/18/zg4tuD8vTmyO5aG.png" alt="image-20240318115804138"></p>
<p>最后这里，全部是坏瓜，所以也不再分</p>
<p><img src="https://s2.loli.net/2024/03/18/twrS8OxmLap2CAq.png" alt="image-20240318115943306"></p>
<p>经过预剪枝，得到的图是：</p>
<p><img src="https://s2.loli.net/2024/03/18/BJEtSwbKAFRufD3.png" alt="image-20240318120142603"></p>
<h1 id="0416-后剪枝"><a href="#0416-后剪枝" class="headerlink" title="0416 后剪枝"></a>0416 后剪枝</h1><p>后剪枝是能不剪就不剪</p>
<p><img src="https://s2.loli.net/2024/03/18/U3ECIQGNbHTmWR9.png" alt="image-20240318120723808"></p>
<p>最终是：</p>
<p><img src="https://s2.loli.net/2024/03/18/Jjtz6xIaLDCU2wP.png" alt="image-20240318120842391"></p>
<p><img src="https://s2.loli.net/2024/03/18/qxrvtnBDP2JECRh.png" alt="image-20240318120931067"></p>
<p>补充：Prune Regression trees</p>
<h1 id="0417-连续值C4-5二分法信息增益最大准则"><a href="#0417-连续值C4-5二分法信息增益最大准则" class="headerlink" title="0417 连续值C4.5二分法信息增益最大准则"></a>0417 连续值C4.5二分法信息增益最大准则</h1><p><img src="https://s2.loli.net/2024/03/18/3KZsylpXHYir4BP.png" alt="image-20240318121551534"></p>
<h1 id="0418-决策树缺失值处理"><a href="#0418-决策树缺失值处理" class="headerlink" title="0418 决策树缺失值处理"></a>0418 决策树缺失值处理</h1><p><img src="https://s2.loli.net/2024/03/18/RhO3V48bsLMAQrB.png" alt="image-20240318121853668"></p>
<p><img src="https://s2.loli.net/2024/03/18/O1cWYJFv5kG9UxR.png" alt="image-20240318122313628"></p>
<p><img src="https://s2.loli.net/2024/03/18/U2jf3RG9NWpca1h.png" alt="image-20240318122500736"></p>
<h1 id="0701-贝叶斯分类器综述"><a href="#0701-贝叶斯分类器综述" class="headerlink" title="0701 贝叶斯分类器综述"></a>0701 贝叶斯分类器综述</h1><h1 id="0702-贝叶斯定理—一个应用"><a href="#0702-贝叶斯定理—一个应用" class="headerlink" title="0702 贝叶斯定理—一个应用"></a>0702 贝叶斯定理—一个应用</h1><p><a target="_blank" rel="noopener" href="https://www.matongxue.com/madocs/279/">https://www.matongxue.com/madocs/279/</a></p>
<h1 id="0703-贝叶斯定理"><a href="#0703-贝叶斯定理" class="headerlink" title="0703 贝叶斯定理"></a>0703 贝叶斯定理</h1><p><a target="_blank" rel="noopener" href="https://www.matongxue.com/madocs/279/">https://www.matongxue.com/madocs/279/</a></p>
<p><img src="https://s2.loli.net/2024/03/20/gLJlm1QGrvekF6R.png" alt="image-20240320103534165"></p>
<h1 id="0704-预热——一个半朴素贝叶斯的例子"><a href="#0704-预热——一个半朴素贝叶斯的例子" class="headerlink" title="0704 预热——一个半朴素贝叶斯的例子"></a>0704 预热——一个半朴素贝叶斯的例子</h1><p><img src="https://s2.loli.net/2024/03/20/iA8wPYc4bJQoslH.png" alt="image-20240320103738904"></p>
<h1 id="0705贝叶斯决策论"><a href="#0705贝叶斯决策论" class="headerlink" title="0705贝叶斯决策论"></a>0705贝叶斯决策论</h1><p><img src="https://s2.loli.net/2024/03/20/BqdHfZ1A4DlPzRo.png" alt="image-20240320105552909"></p>
<p><img src="https://s2.loli.net/2024/03/20/Y9bVmeSwNrhBivu.png" alt="image-20240320105728365"></p>
<p><img src="https://s2.loli.net/2024/03/20/5XidpVSD9M7zQ6e.png" alt="image-20240320105910528"></p>
<p><img src="https://s2.loli.net/2024/03/20/LzFiwM98T725UeN.png" alt="image-20240320110043237"></p>
<h1 id="0706-最大似然估计"><a href="#0706-最大似然估计" class="headerlink" title="0706 最大似然估计"></a>0706 最大似然估计</h1><p><a target="_blank" rel="noopener" href="https://www.matongxue.com/madocs/447/">https://www.matongxue.com/madocs/447/</a></p>
<p><img src="https://s2.loli.net/2024/03/20/UHg3xOF4b7tMY2Q.png" alt="image-20240320111820618"></p>
<h1 id="0709-最大似然估计——公式与取对数"><a href="#0709-最大似然估计——公式与取对数" class="headerlink" title="0709 最大似然估计——公式与取对数"></a>0709 最大似然估计——公式与取对数</h1><p><img src="https://s2.loli.net/2024/03/20/hLnOxq91Ar4jcmP.png" alt="image-20240320112532737"></p>
<p><img src="https://s2.loli.net/2024/03/20/CdALSlsvrbkKijP.png" alt="image-20240320112722367"></p>
<p><img src="https://s2.loli.net/2024/03/20/DqNRfCL5w31vdQh.png" alt="image-20240320112804276"></p>
<p><img src="https://s2.loli.net/2024/03/20/QwRmU9nEOolZeGx.png" alt="image-20240320113055666"></p>
<p><img src="https://s2.loli.net/2024/03/20/GVnFRKD1loQWL3u.png" alt="image-20240320113151728"></p>
<p><img src="https://s2.loli.net/2024/03/20/fdSB7q1TzpjrDcv.png" alt="image-20240320113406505"></p>
<p><img src="https://s2.loli.net/2024/03/20/DNR3TLxzI2frEkP.png" alt="image-20240320113430304"></p>
<p><img src="https://s2.loli.net/2024/03/20/4rnolLUfD6X3vaS.png" alt="image-20240320113519442"></p>
<p><img src="https://s2.loli.net/2024/03/20/vBigtkzJE4A8pxO.png" alt="image-20240320113606279"></p>
<h1 id="0711-拉普拉斯修正"><a href="#0711-拉普拉斯修正" class="headerlink" title="0711 拉普拉斯修正"></a>0711 拉普拉斯修正</h1><p><img src="https://s2.loli.net/2024/03/21/tMfAJhVWTUBu519.png" alt="image-20240321111636788"></p>
<p><img src="https://s2.loli.net/2024/03/21/UDK5f4EznXNkLHM.png" alt="image-20240321111730575"></p>
<p>拉普拉斯修正避免了因为训练集不充分而导致的概率估计为0的情况</p>
<p>拉普拉斯修正实际上是假设了属性值与类别均匀分布，这是在朴素贝叶斯学习过程中额外引入的关于数据的先验</p>
<h1 id="0712-EM算法"><a href="#0712-EM算法" class="headerlink" title="0712 EM算法"></a>0712 EM算法</h1><p><img src="https://s2.loli.net/2024/03/21/xLAeNDMQyFCVrfK.png" alt="image-20240321112303279"></p>
<p><img src="https://s2.loli.net/2024/03/21/W7ry6RFZknJaDfI.png" alt="image-20240321112439716"></p>
<p><img src="https://s2.loli.net/2024/03/21/8IbfD4EXWZuUANM.png" alt="image-20240321112701578"></p>
<p><img src="https://s2.loli.net/2024/03/21/n5KQJAC4GcxWYjE.png" alt="image-20240321112826435"></p>
<h1 id="0802-集成学习的威力"><a href="#0802-集成学习的威力" class="headerlink" title="0802 集成学习的威力"></a>0802 集成学习的威力</h1><p><img src="C:/Users/YL.YL.000/AppData/Roaming/Typora/typora-user-images/image-20240321114406985.png" alt="image-20240321114406985"></p>
<h1 id="0803-AdaBoost原理看个例子包懂"><a href="#0803-AdaBoost原理看个例子包懂" class="headerlink" title="0803 AdaBoost原理看个例子包懂"></a>0803 AdaBoost原理看个例子包懂</h1><p><img src="https://s2.loli.net/2024/03/21/jITXMkENx7LohFU.png" alt="image-20240321114637180"></p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/27126737">https://zhuanlan.zhihu.com/p/27126737</a></p>
<h1 id="0804-AdaBoost算法实现"><a href="#0804-AdaBoost算法实现" class="headerlink" title="0804 AdaBoost算法实现"></a>0804 AdaBoost算法实现</h1><h1 id="0807-Gradient-Boosting简述"><a href="#0807-Gradient-Boosting简述" class="headerlink" title="0807 Gradient Boosting简述"></a>0807 Gradient Boosting简述</h1><p><img src="https://s2.loli.net/2024/03/23/yi3XNk2FuOUvGz9.png" alt="image-20240323114332358"></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN,en,default">
    <link itemprop="mainEntityOfPage" href="http://humble2967738843.github.io/2024/01/13/52-dropout-zheng-ze-hua/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="院龙">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="院龙">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2024/01/13/52-dropout-zheng-ze-hua/" class="post-title-link" itemprop="url">52Dropout正则化</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2024-01-13 21:09:22" itemprop="dateCreated datePublished" datetime="2024-01-13T21:09:22+08:00">2024-01-13</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2024-05-26 13:16:18" itemprop="dateModified" datetime="2024-05-26T13:16:18+08:00">2024-05-26</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="52-Dropout-正则化"><a href="#52-Dropout-正则化" class="headerlink" title="52 Dropout 正则化"></a>52 Dropout 正则化</h1><h2 id="52-1Dropout-regularization"><a href="#52-1Dropout-regularization" class="headerlink" title="52.1Dropout regularization"></a>52.1Dropout regularization</h2><p><img src="C:/Users/YL.YL.000/AppData/Roaming/Typora/typora-user-images/image-20240113203610466.png" alt="image-20240113203610466"></p>
<h2 id="52-2Implementing-dropout-“Inverted-dropout”"><a href="#52-2Implementing-dropout-“Inverted-dropout”" class="headerlink" title="52.2Implementing dropout(“Inverted dropout”)"></a>52.2Implementing dropout(“Inverted dropout”)</h2><p><img src="https://s2.loli.net/2024/01/13/ilLpst3SmYoHRuX.png" alt="image-20240113210330586"></p>
<h2 id="52-3Making-predictions-at-test-time"><a href="#52-3Making-predictions-at-test-time" class="headerlink" title="52.3Making predictions at test time"></a>52.3Making predictions at test time</h2><p><img src="https://s2.loli.net/2024/01/13/YDzrq1TguIACMRh.png" alt="image-20240113210850114"></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN,en,default">
    <link itemprop="mainEntityOfPage" href="http://humble2967738843.github.io/2024/01/06/51-wei-shi-me-zheng-ze-hua-ke-yi-jian-shao-guo-ni-he/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="院龙">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="院龙">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2024/01/06/51-wei-shi-me-zheng-ze-hua-ke-yi-jian-shao-guo-ni-he/" class="post-title-link" itemprop="url">为什么正则化可以减少过拟合？</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2024-01-06 23:03:06" itemprop="dateCreated datePublished" datetime="2024-01-06T23:03:06+08:00">2024-01-06</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2024-05-26 13:15:57" itemprop="dateModified" datetime="2024-05-26T13:15:57+08:00">2024-05-26</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="51-为什么正则化可以减少过拟合？"><a href="#51-为什么正则化可以减少过拟合？" class="headerlink" title="51 为什么正则化可以减少过拟合？"></a>51 为什么正则化可以减少过拟合？</h1><h2 id="51-1-How-does-regularization-prevent-overfitting"><a href="#51-1-How-does-regularization-prevent-overfitting" class="headerlink" title="51.1 How does regularization prevent overfitting?"></a>51.1 How does regularization prevent overfitting?</h2><p><img src="https://s2.loli.net/2024/01/06/eK9HgiZJAS7MwGz.png" alt="image-20240106230618834"></p>
<p><img src="https://s2.loli.net/2024/01/06/vMUnYbDhmiwR85j.png" alt="image-20240106231520169"></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN,en,default">
    <link itemprop="mainEntityOfPage" href="http://humble2967738843.github.io/2024/01/05/50-zheng-ze-hua/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="院龙">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="院龙">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2024/01/05/50-zheng-ze-hua/" class="post-title-link" itemprop="url">50正则化</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2024-01-05 12:43:38" itemprop="dateCreated datePublished" datetime="2024-01-05T12:43:38+08:00">2024-01-05</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2024-05-26 13:15:33" itemprop="dateModified" datetime="2024-05-26T13:15:33+08:00">2024-05-26</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="50-正则化"><a href="#50-正则化" class="headerlink" title="50 正则化"></a>50 正则化</h1><h2 id="logistic-regression"><a href="#logistic-regression" class="headerlink" title="logistic regression"></a>logistic regression</h2><p><img src="https://s2.loli.net/2024/01/05/heCbfkAP8I1O2cp.png" alt="image-20240105123556939"></p>
<h2 id="Neural-network"><a href="#Neural-network" class="headerlink" title="Neural network"></a>Neural network</h2><p><img src="https://s2.loli.net/2024/01/05/6K2UAiya5ILhCQn.png" alt="image-20240105124653826"></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN,en,default">
    <link itemprop="mainEntityOfPage" href="http://humble2967738843.github.io/2024/01/05/49-ji-qi-xue-xi-ji-chu/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="院龙">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="院龙">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2024/01/05/49-ji-qi-xue-xi-ji-chu/" class="post-title-link" itemprop="url">49机器学习基础</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2024-01-05 12:29:26" itemprop="dateCreated datePublished" datetime="2024-01-05T12:29:26+08:00">2024-01-05</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2024-05-26 13:15:17" itemprop="dateModified" datetime="2024-05-26T13:15:17+08:00">2024-05-26</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="49-机器学习基础"><a href="#49-机器学习基础" class="headerlink" title="49 机器学习基础"></a>49 机器学习基础</h1><h2 id="Basic-recipe-for-machine-learning"><a href="#Basic-recipe-for-machine-learning" class="headerlink" title="Basic recipe for machine learning"></a>Basic recipe for machine learning</h2><p><img src="https://s2.loli.net/2024/01/05/Owq1IAr3ZspX2jn.png" alt="image-20240105122804069"></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  


  
  <nav class="pagination">
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><span class="space">&hellip;</span><a class="page-number" href="/page/7/">7</a><a class="extend next" rel="next" href="/page/2/"><i class="fa fa-angle-right" aria-label="下一页"></i></a>
  </nav>



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">院龙</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">61</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
        <span class="site-state-item-count">11</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
        <span class="site-state-item-count">14</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2024</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">院龙</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a> 强力驱动
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  

<script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","pluginJsPath":"lib/","pluginModelPath":"assets/","tagMode":false,"log":false,"model":{"jsonPath":"live2d-widget-model-chitose"},"display":{"position":"right","width":150,"height":300},"mobile":{"show":true}});</script></body>
</html>
