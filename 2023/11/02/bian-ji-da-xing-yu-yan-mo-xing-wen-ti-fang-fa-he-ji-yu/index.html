<!DOCTYPE html>
<html lang="zh-CN,en,default">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 7.0.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">

<link rel="stylesheet" href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext">
<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"yuanllong.github.io","root":"/","scheme":"Gemini","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":true},"bookmark":{"enable":true,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta name="description" content="编辑大型语言模型：问题、方法和机遇Abstruct尽管有能力培养有能力的LLMs，但维持其相关性和纠正错误的方法仍然难以捉摸。为此，过去几年见证了LLMs编辑技术的激增，其目标是有效地改变特定领域内LLMs的行为，而不会对其他输入的性能产生负面影响。本文深入探讨了LLMs模型编辑相关的问题、方法和机遇。特别是，我们对任务定义和与模型编辑相关的挑战进行了详尽的概述，并对我们目前掌握的最先进的方法进行">
<meta property="og:type" content="article">
<meta property="og:title" content="编辑大型语言模型：问题、方法和机遇">
<meta property="og:url" content="http://yuanllong.github.io/2023/11/02/bian-ji-da-xing-yu-yan-mo-xing-wen-ti-fang-fa-he-ji-yu/index.html">
<meta property="og:site_name" content="院龙">
<meta property="og:description" content="编辑大型语言模型：问题、方法和机遇Abstruct尽管有能力培养有能力的LLMs，但维持其相关性和纠正错误的方法仍然难以捉摸。为此，过去几年见证了LLMs编辑技术的激增，其目标是有效地改变特定领域内LLMs的行为，而不会对其他输入的性能产生负面影响。本文深入探讨了LLMs模型编辑相关的问题、方法和机遇。特别是，我们对任务定义和与模型编辑相关的挑战进行了详尽的概述，并对我们目前掌握的最先进的方法进行">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://yuanllong.github.io/2023/11/02/imgs/$%7Bfiilename%7D/SWBFWKDL.png">
<meta property="og:image" content="http://yuanllong.github.io/2023/11/02/imgs/$%7Bfiilename%7D/YCQJYKA2.png">
<meta property="og:image" content="http://yuanllong.github.io/2023/11/02/imgs/$%7Bfiilename%7D/Q3XB8E8H.png">
<meta property="og:image" content="http://yuanllong.github.io/2023/11/02/imgs/$%7Bfiilename%7D/TSPLLYFE.png">
<meta property="og:image" content="http://yuanllong.github.io/2023/11/02/imgs/$%7Bfiilename%7D/88F6ZZTR.png">
<meta property="og:image" content="http://yuanllong.github.io/2023/11/02/imgs/$%7Bfiilename%7D/Y5M6Y7UX.png">
<meta property="og:image" content="http://yuanllong.github.io/2023/11/02/imgs/$%7Bfiilename%7D/B7WY2DBH.png">
<meta property="og:image" content="http://yuanllong.github.io/2023/11/02/imgs/$%7Bfiilename%7D/WVMGDVMU.png">
<meta property="og:image" content="http://yuanllong.github.io/2023/11/02/imgs/$%7Bfiilename%7D/AKCEUPGK.png">
<meta property="og:image" content="http://yuanllong.github.io/2023/11/02/imgs/$%7Bfiilename%7D/ND8H464M.png">
<meta property="og:image" content="http://yuanllong.github.io/2023/11/02/imgs/$%7Bfiilename%7D/EQ2WM2DI.png">
<meta property="og:image" content="http://yuanllong.github.io/2023/11/02/imgs/$%7Bfiilename%7D/K9ABI2KM.png">
<meta property="article:published_time" content="2023-11-02T04:21:08.000Z">
<meta property="article:modified_time" content="2024-05-26T09:32:27.676Z">
<meta property="article:author" content="院龙">
<meta property="article:tag" content="EMNLP2023">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://yuanllong.github.io/2023/11/02/imgs/$%7Bfiilename%7D/SWBFWKDL.png">

<link rel="canonical" href="http://yuanllong.github.io/2023/11/02/bian-ji-da-xing-yu-yan-mo-xing-wen-ti-fang-fa-he-ji-yu/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>

  <title>编辑大型语言模型：问题、方法和机遇 | 院龙</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

<!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">
<!-- hexo injector head_end end --><link rel="alternate" href="/atom.xml" title="院龙" type="application/atom+xml">
</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">院龙</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container"></div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div class="algolia-results">
  <div id="algolia-stats"></div>
  <div id="algolia-hits"></div>
  <div id="algolia-pagination" class="algolia-pagination"></div>
</div>

      
    </div>
  </div>

</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>
  <div class="reading-progress-bar"></div>
  <a role="button" class="book-mark-link book-mark-link-fixed"></a>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://yuanllong.github.io/2023/11/02/bian-ji-da-xing-yu-yan-mo-xing-wen-ti-fang-fa-he-ji-yu/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.png">
      <meta itemprop="name" content="院龙">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="院龙">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          编辑大型语言模型：问题、方法和机遇<a href="https://github.com/user-name/repo-name/tree/branch-name/subdirectory-name_posts/%E7%BC%96%E8%BE%91%E5%A4%A7%E5%9E%8B%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%EF%BC%9A%E9%97%AE%E9%A2%98%E3%80%81%E6%96%B9%E6%B3%95%E5%92%8C%E6%9C%BA%E9%81%87.md" class="post-edit-link" title="编辑" rel="noopener" target="_blank"><i class="fa fa-pencil-alt"></i></a>
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2023-11-02 12:21:08" itemprop="dateCreated datePublished" datetime="2023-11-02T12:21:08+08:00">2023-11-02</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2024-05-26 17:32:27" itemprop="dateModified" datetime="2024-05-26T17:32:27+08:00">2024-05-26</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E7%9F%A5%E8%AF%86%E7%BC%96%E8%BE%91/" itemprop="url" rel="index"><span itemprop="name">知识编辑</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h1 id="编辑大型语言模型：问题、方法和机遇"><a href="#编辑大型语言模型：问题、方法和机遇" class="headerlink" title="编辑大型语言模型：问题、方法和机遇"></a>编辑大型语言模型：问题、方法和机遇</h1><h2 id="Abstruct"><a href="#Abstruct" class="headerlink" title="Abstruct"></a>Abstruct</h2><p>尽管有能力培养有能力的LLMs，但维持其相关性和纠正错误的方法仍然难以捉摸。为此，过去几年见证了LLMs编辑技术的激增，其<span style="background-color: #2ea8e580">目标是有效地改变特定领域内LLMs的行为，而不会对其他输入的性能产生负面影响</span>。本文深入探讨了LLMs模型编辑相关的问题、方法和机遇。特别是，我们<span style="background-color: #ff666680">对任务定义和与模型编辑相关的挑战进行了详尽的概述，并对我们目前掌握的最先进的方法进行了深入的实证分析</span>。我们还<span style="background-color: #ff666680">构建了一个新的基准数据集，以促进更稳健的评估并查明现有技术固有的持久问题</span>。我们的目标是为每种编辑技术的有效性和可行性提供有价值的见解，从而帮助社区做出明智的决定，为特定任务或上下文选择最合适的方法。</p>
<h1 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1 Introduction"></a>1 Introduction</h1><p>大型语言模型（LLM）已经表现出理解和生成类人文本的非凡能力（Brown et al., 2020；OpenAI, 2023；Anil et al., 2023；Touvron et al., 2023；Qiao et al., 2022；赵等人，2023）。尽管LLMs的训练非常熟练，但确保其相关性和修复错误的策略仍不清楚。理想情况下，随着世界形势的发展，我们的目标是更新LLMs，避免与训练全新模型相关的计算负担。如图1所示，解决这个问题模型编辑的概念被提出</p>
<p><img src="../imgs/$%7Bfiilename%7D/SWBFWKDL.png" alt="\&lt;img alt=&quot;&quot; data-attachment-key=&quot;SWBFWKDL&quot; data-annotation=&quot;%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2Flocal%2FiUeV0SQs%2Fitems%2F9227ERUC%22%2C%22annotationKey%22%3A%226BF2NSVY%22%2C%22color%22%3A%22%23ffd400%22%2C%22pageLabel%22%3A%221%22%2C%22position%22%3A%7B%22pageIndex%22%3A0%2C%22rects%22%3A%5B%5B300.5%2C490.39%2C533%2C628.89%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2Flocal%2FiUeV0SQs%2Fitems%2FCU5SMQAC%22%5D%2C%22locator%22%3A%221%22%7D%7D&quot; width=&quot;388&quot; height=&quot;231&quot; src=&quot;attachments/SWBFWKDL.png&quot; ztype=&quot;zimage&quot;&gt;"></p>
<p>（Sinitsin 等人，2020；De Cao 等人，2021），<span style="background-color: #2ea8e580">能够对模型的行为进行数据有效的改变，特别是在指定的感兴趣领域内，同时确保不会对其他输入产生不利影响。</span>目前，大量关于LLMs模型编辑的工作（De Cao et al., 2021；Meng et al., 2022, 2023；Sinitsin et al., 2020；Huang et al., 2023)在各种编辑任务和设置方面取得了长足的进步。如图 2 所示，<span style="background-color: #2ea8e580">这些工作通过将辅助网络与原始未更改的模型集成或更改导致不良输出的模型参数来操纵特定情况下的模型输出。</span>尽管文献中存在广泛的模型编辑技术，但<span style="background-color: #5fb23680">明显缺乏在统一实验条件下评估这些方法的全面比较分析。缺乏直接比较会削弱我们辨别每种方法相对优缺点的能力，从而阻碍我们理解它们在不同问题领域的适应性。</span></p>
<pre><code> 为了解决这个问题，本研究致力于建立一个标准的问题定义，并对这些方法进行细致的评估（§2，§3）。我们在规定的条件下进行实验，促进对各自的优缺点进行公正的比较（§4）。&lt;span style=&quot;background-color: #2ea8e580&quot;&gt;我们最初使用两个流行的模型编辑数据集，ZsRE (Levy et al., 2017) 和 COUNTERFACT (Meng et al., 2022)，以及两个结构上的数据集不同的语言模型，T5（Raffel et al.，2020a）（编码器-解码器）和 GPT-J（Wang 和 Komatsuzaki，2021a）（仅解码器）作为我们的基础模型&lt;/span&gt;。我们还评估了较大模型 OPT-13B（Zhang 等人，2022a）和 GPT-NEOX20B（Black 等人，2022）的性能。除了基本编辑设置之外，我们还评估批量和顺序编辑的性能。虽然我们观察到当前的方法在事实模型编辑任务中表现出当大的能力，但&lt;span style=&quot;background-color: #2ea8e580&quot;&gt;我们重新考虑当前的评估并创建一个更具包容性的评估数据集（§5）：可移植性（强大的泛化能力）、局部性（副作用）和效率（时间）和内存使用情况）&lt;/span&gt;。我们发现当前的模型编辑方法在这些层面上有所限制，从而限制了它们的实际应用，未来值得更多的研究。通过系统评估，我们的目标是为每种模型编辑技术的有效性提供有价值的见解，帮助研究人员为特定任务选择合适的方法。
</code></pre><h2 id="2-Problem-Definition"><a href="#2-Problem-Definition" class="headerlink" title="2 Problem Definition"></a>2 Problem Definition</h2><p>模型编辑，由 Mitchell 等人阐明。 （2022b），<span style="background-color: #2ea8e580">旨在有效地调整特定编辑描述符（xe，ye）上的初始基础模型（fθ，θ表示模型的参数）行为，而不影响其他样本上的模型行为</span>。最终目标是创建一个编辑模型，表示为 fθe。具体来说，基本模型 fθ 由函数 f : X → Y 表示，该函数将输入 x 与其相应的预测 y 相关联。给定一个由编辑输入 xe 和编辑标签 ye 组成的编辑描述符，使得 fθ(xe) ̸= ye，后期编辑模型 fθe 被设计为产生预期输出，其中 fθe(xe) = ye。</p>
<pre><code> &lt;span style=&quot;background-color: #2ea8e580&quot;&gt;模型编辑过程通常会影响与编辑示例密切相关的大量输入的预测。这个输入集合称为编辑范围。&lt;/span&gt;成功的编辑应该调整编辑范围内示例的模型行为，同时保持范围外示例的性能不变：
</code></pre><p><img src="../imgs/$%7Bfiilename%7D/YCQJYKA2.png" alt="\&lt;img alt=&quot;&quot; data-attachment-key=&quot;YCQJYKA2&quot; data-annotation=&quot;%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2Flocal%2FiUeV0SQs%2Fitems%2F9227ERUC%22%2C%22annotationKey%22%3A%22WCMI8DSD%22%2C%22color%22%3A%22%23ffd400%22%2C%22pageLabel%22%3A%222%22%2C%22position%22%3A%7B%22pageIndex%22%3A1%2C%22rects%22%3A%5B%5B70.5%2C136.39%2C291.5%2C183.39%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2Flocal%2FiUeV0SQs%2Fitems%2FCU5SMQAC%22%5D%2C%22locator%22%3A%222%22%7D%7D&quot; width=&quot;368&quot; height=&quot;78&quot; src=&quot;attachments/YCQJYKA2.png&quot; ztype=&quot;zimage&quot;&gt;"></p>
<p>范围内 I(xe, ye) 通常包含 xe 及其等价邻域 N (xe, ye)，其中包括相关的输入/输出对。相反，超出范围的 O(xe, ye) 由与编辑示例无关的输入组成。模型fe应该满足以下三个属性：可靠性、泛化性和局部性。</p>
<p><strong>可靠性</strong> 先前的工作（Huang et al., 2023；De Cao et al., 2021；Meng et al., 2022）定义了当后期编辑模型 fθe 给出案例 (xe, ye) 的目标答案时的可靠编辑被编辑。可靠性以编辑案例的平均准确度来衡量：</p>
<p><img src="../imgs/$%7Bfiilename%7D/Q3XB8E8H.png" alt="\&lt;img alt=&quot;&quot; data-attachment-key=&quot;Q3XB8E8H&quot; data-annotation=&quot;%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2Flocal%2FiUeV0SQs%2Fitems%2F9227ERUC%22%2C%22annotationKey%22%3A%22WHB9Z2XR%22%2C%22color%22%3A%22%23ffd400%22%2C%22pageLabel%22%3A%222%22%2C%22position%22%3A%7B%22pageIndex%22%3A1%2C%22rects%22%3A%5B%5B302.5%2C616.89%2C530%2C650.39%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2Flocal%2FiUeV0SQs%2Fitems%2FCU5SMQAC%22%5D%2C%22locator%22%3A%222%22%7D%7D&quot; width=&quot;379&quot; height=&quot;56&quot; src=&quot;attachments/Q3XB8E8H.png&quot; ztype=&quot;zimage&quot;&gt;"></p>
<p><strong>泛化</strong> 编辑后模型 fθe 还应该编辑等效邻居 N (xe, ye)（例如改写的句子)。它是通过模型 fθe 在从等价邻域中均匀抽取的示例上的平均精度来评估的：</p>
<p><img src="../imgs/$%7Bfiilename%7D/TSPLLYFE.png" alt="\&lt;img alt=&quot;&quot; data-attachment-key=&quot;TSPLLYFE&quot; data-annotation=&quot;%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2Flocal%2FiUeV0SQs%2Fitems%2F9227ERUC%22%2C%22annotationKey%22%3A%22ICM3VKEP%22%2C%22color%22%3A%22%23ffd400%22%2C%22pageLabel%22%3A%222%22%2C%22position%22%3A%7B%22pageIndex%22%3A1%2C%22rects%22%3A%5B%5B305%2C503.39%2C527%2C538.89%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2Flocal%2FiUeV0SQs%2Fitems%2FCU5SMQAC%22%5D%2C%22locator%22%3A%222%22%7D%7D&quot; width=&quot;370&quot; height=&quot;59&quot; src=&quot;attachments/TSPLLYFE.png&quot; ztype=&quot;zimage&quot;&gt;"></p>
<p><strong>局部性 </strong>在一些工作中，也被称为特异性。编辑应该在本地实现，这意味着编辑后模型 fθe 不应更改范围外 O(xe, ye) 中不相关示例的输出。因此，局部性是通过编辑后模型 fθe 的预测与编辑前 fθ 模型相同的来评估的</p>
<p><img src="../imgs/$%7Bfiilename%7D/88F6ZZTR.png" alt="\&lt;img alt=&quot;&quot; data-attachment-key=&quot;88F6ZZTR&quot; data-annotation=&quot;%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2Flocal%2FiUeV0SQs%2Fitems%2F9227ERUC%22%2C%22annotationKey%22%3A%22KH34VUGC%22%2C%22color%22%3A%22%23ffd400%22%2C%22pageLabel%22%3A%222%22%2C%22position%22%3A%7B%22pageIndex%22%3A1%2C%22rects%22%3A%5B%5B306.5%2C364.39%2C527%2C401.39%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2Flocal%2FiUeV0SQs%2Fitems%2FCU5SMQAC%22%5D%2C%22locator%22%3A%222%22%7D%7D&quot; width=&quot;368&quot; height=&quot;62&quot; src=&quot;attachments/88F6ZZTR.png&quot; ztype=&quot;zimage&quot;&gt;"></p>
<h3 id="3-Current-Methods"><a href="#3-Current-Methods" class="headerlink" title="3 Current Methods"></a>3 Current Methods</h3><p>目前LLMs的模型编辑方法可以分为两种主要范式，如图2所示：<span style="background-color: #2ea8e580">修改模型参数或保留模型参数。</span>更多比较见表 6。</p>
<h3 id="3-1-Methods-for-Preserving-LLMs-Parameters"><a href="#3-1-Methods-for-Preserving-LLMs-Parameters" class="headerlink" title="3.1 Methods for Preserving LLMs Parameters"></a>3.1 Methods for Preserving LLMs Parameters</h3><p><strong>基于内存的模型 </strong><span style="background-color: #2ea8e580">这种方法将所有编辑示例显式存储在内存中，并使用检索器为每个新输入提取最相关的编辑事实，以指导模型生成编辑事实。 </span>SERAC（Mitchell 等人，2022b）提出了一种采用独特的反事实模型，同时保持原始模型不变的方法。具体来说，<span style="background-color: #2ea8e580">它采用范围分类器来计算新输入落入存储的编辑示例范围内的可能性。如果输入与内存中任何缓存的编辑相匹配，则反事实模型的预测将基于输入和最可能的编辑。否则，如果输入超出了所有编辑的范围，给出了原始模型的预测。</span>此外，<span style="background-color: #5fb23680">最近的研究表明LLMs拥有强大的情境学习能力。模型本身可以生成与所提供的知识相对应的输出，而不是求助于用新事实训练的额外模型，并给出精炼的知识上下文作为提示。</span><span style="background-color: #2ea8e580">这种方法通过用编辑后的事实提示模型并从编辑记忆中检索编辑演示来编辑语言模型</span>，包括以下工作：MemPrompt (Madaan et al., 2022)、IKE (Zheng et al., 2023) 和MeLLo（Zhong 等人，2023）。</p>
<p><img src="../imgs/$%7Bfiilename%7D/Y5M6Y7UX.png" alt="\&lt;img alt=&quot;&quot; data-attachment-key=&quot;Y5M6Y7UX&quot; data-annotation=&quot;%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2Flocal%2FiUeV0SQs%2Fitems%2F9227ERUC%22%2C%22annotationKey%22%3A%226XZYF4BM%22%2C%22color%22%3A%22%23ffd400%22%2C%22pageLabel%22%3A%223%22%2C%22position%22%3A%7B%22pageIndex%22%3A2%2C%22rects%22%3A%5B%5B79%2C517.39%2C515%2C782.89%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2Flocal%2FiUeV0SQs%2Fitems%2FCU5SMQAC%22%5D%2C%22locator%22%3A%223%22%7D%7D&quot; width=&quot;727&quot; height=&quot;443&quot; src=&quot;attachments/Y5M6Y7UX.png&quot; ztype=&quot;zimage&quot;&gt;"></p>
<p><strong>附加参数 </strong><span style="background-color: #2ea8e580">此范例在语言模型中引入了额外的可训练参数。这些参数在修改后的知识数据集上进行训练，而原始模型参数保持静态。</span> T-Patcher（Huang et al., 2023）在模型前馈网络（FFN）的最后一层针对一个错误集成了一个神经元（补丁），仅在遇到其对应错误时才生效。 CaliNET（Dong et al., 2022）整合了多个神经元以用于多个编辑案例。不同的是，GRACE（Hartvigsen et al., 2022）维护一个离散的密码本作为适配器，随着时间的推移添加和更新元素以编辑模型的预测。</p>
<h3 id="3-2-Methods-for-Modifying-LLMs-Paramete"><a href="#3-2-Methods-for-Modifying-LLMs-Paramete" class="headerlink" title="3.2 Methods for Modifying LLMs Paramete"></a>3.2 Methods for Modifying LLMs Paramete</h3><p>该范例将更新部分参数 θ，它应用更新 Δ 矩阵来编辑模型。</p>
<p><strong>定位然后编辑 </strong><span style="background-color: #2ea8e580">该范例首先识别与特定知识相对应的参数，并通过直接更新目标参数来修改它们。</span>知识神经元（KN）方法（Dai et al., 2022）<span style="background-color: #2ea8e580">引入了知识归因技术来精确定位体现知识的“知识神经元”（FFN 矩阵中的键值对），然后更新这些神经元。 </span>ROME（Meng et al., 2022）<span style="background-color: #2ea8e580">应用因果中介分析来定位编辑区域。 ROME 不是修改 FFN 中的知识神经元，而是改变整个矩阵。 ROME 将模型编辑视为具有线性等式约束的最小二乘法，并使用拉格朗日乘子来求解。</span>然而，<span style="background-color: #2ea8e580">KN 和 ROME 一次只能编辑一个事实关联。</span>为此，<span style="background-color: #2ea8e580">MEMIT（Meng et al., 2023）对ROME的设置进行了扩展，实现了多病例同步编辑的情况。</span>基于 MEMIT，PMET（Li et al., 2023a）涉及注意力值以获得更好的性能。</p>
<p><strong>元学习</strong> <span style="background-color: #2ea8e580">元学习方法采用超网络来学习编辑 LLM 所需的 Δ</span>。<span style="background-color: #2ea8e580">知识编辑器（KE）（De Cao et al., 2021）利用超网络（特别是双向 LSTM）来预测每个数据点的权重更新，从而能够在不干扰其他知识的情况下对编辑目标知识进行约束优化</span>。然而，这种方法在编辑LLMs方面存在不足。为了克服这个限制，<span style="background-color: #2ea8e580">模型编辑器网络梯度分解（MEND）（Mitchell et al., 2022a）学习通过采用梯度的低秩分解来变换微调语言模型的梯度，这可以应用于具有更好性能的LLM。</span></p>
<h2 id="4-Preliminary-Experiments"><a href="#4-Preliminary-Experiments" class="headerlink" title="4 Preliminary Experiments"></a>4 Preliminary Experiments</h2><p>考虑到大量以事实知识为中心的研究和数据集，我们将其用作主要比较基础。我们最初的对照实验使用两个著名的事实知识数据集（表 1）进行，促进了方法的直接比较，突出了它们独特的优势和局限性（Wang 等人，2023b）。</p>
<h3 id="4-1-Experiment-Setting"><a href="#4-1-Experiment-Setting" class="headerlink" title="4.1 Experiment Setting"></a>4.1 Experiment Setting</h3><p>我们使用两个著名的模型编辑数据集：ZsRE 和 COUNTERFACT，其详细信息请参见附录 B。以前的研究通常使用较小的语言模型 (&lt;1B)，并证明了当前编辑方法在 BERT 等较小模型上的有效性（Devlin 等人， 2019）。然而，这些方法是否适用于更大的模型仍有待探索。因此，考虑到编辑任务和未来的发展，我们专注于基于生成的模型并选择更大的模型：T5-XL（3B）和GPT-J（6B），代表编码器-解码器和仅解码器结构。</p>
<pre><code> 我们从每种方法类型中选择了有影响力的作品。除了现有的模型编辑技术之外，我们还检查了微调的结果，这是模型更新的基本方法。为了避免重新训练所有层的计算成本，我们采用了Meng等人提出的方法。 (2022)，由 ROME 识别的微调层，我们将其表示为 FT-L。该策略确保与其他直接编辑进行公平比较方法，增强我们分析的有效性。更多详细信息请参见附录 A。
</code></pre><h3 id="4-2-Experiment-Results"><a href="#4-2-Experiment-Results" class="headerlink" title="4.2 Experiment Results"></a>4.2 Experiment Results</h3><p>基本模型表 1 <span style="background-color: #ff666680">揭示了 SERAC 和 ROME 在 ZsRE 和 COUNTERFACT 数据集上的卓越性能，SERAC 在多个指标上超过 90%。虽然 MEMIT 缺乏通用性，但它在可靠性和局部性方面表现出色。 KE、CaliNET 和 KN 表现不佳，在较小的模型中表现尚可，但在较大的模型中表现平平。 MEND 在这两个数据集上表现良好，在 T5 上的结果达到了 80% 以上，尽管不如 ROME 和 SERAC 那样令人印象深刻。 T-Patcher 模型的性能因模型架构和大小的不同而有所不同。例如，它在 ZsRE 数据集的 T5-XL 上表现不佳，而在 GPT-J 上表现完美。在 COUNTERFACT 数据集的情况下，T-Patcher 在 T5 上实现了令人满意的可靠性和局部性，但缺乏泛化性。相反，在 GPT-J 上，该模型在可靠性和泛化性方面表现出色，但在局部性方面表现不佳。</span><span style="background-color: #5fb23680">这种不稳定性可归因于模型架构，因为 T-Patcher 在 T5 的最终解码器层添加了一个神经元；</span><span style="background-color: #ff666680">然而，编码器可能仍然保留原始知识。 FT-L 在 PLM 上的表现不如 ROME，即使修改相同的位置。它在 ZsRE 数据集上显示出令人印象深刻的性能，但在 GPT-J 上的 COUNTERFACT 数据集上与 ROME 的可靠性和泛化能力相当。然而，其较低的局部性得分表明对不相关知识领域的潜在影响。 IKE 表现出良好的可靠性，但在局部性方面遇到困难，因为预先设置的提示可能会影响不相关的输入。它的泛化能力也可以提高。情境学习</span></p>
<p><img src="../imgs/$%7Bfiilename%7D/B7WY2DBH.png" alt="\&lt;img alt=&quot;&quot; data-attachment-key=&quot;B7WY2DBH&quot; data-annotation=&quot;%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2Flocal%2FiUeV0SQs%2Fitems%2F9227ERUC%22%2C%22annotationKey%22%3A%22AYAJST64%22%2C%22color%22%3A%22%23ffd400%22%2C%22pageLabel%22%3A%225%22%2C%22position%22%3A%7B%22pageIndex%22%3A4%2C%22rects%22%3A%5B%5B67%2C654.39%2C531.5%2C770.39%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2Flocal%2FiUeV0SQs%2Fitems%2FCU5SMQAC%22%5D%2C%22locator%22%3A%225%22%7D%7D&quot; width=&quot;774&quot; height=&quot;193&quot; src=&quot;attachments/B7WY2DBH.png&quot; ztype=&quot;zimage&quot;&gt;"></p>
<p>该方法可能会遇到上下文调解失败的问题（Hernandez et al., 2023)，因为预先训练的语言模型可能无法始终生成与提示对齐的文本。</p>
<p><strong>模型缩放</strong> 由于计算限制，我们使用更大的模型进行实验，在 OPT-13B 和 GPT-NEOX-20B 上测试 IKE、ROME 和 MEMIT。结果（表 2）令人惊讶地<span style="background-color: #ff666680">显示 ROME 和 MEMIT 在 GPT-NEOX-20B 模型上表现良好，但在 OPT-13B 上表现不佳。这是由于这两种方法都依赖于矩阵求逆运算。然而，在 OPT-13B 模型中，矩阵是不可逆的。</span>我们甚至根据经验发现，<span style="background-color: #5fb23680">用最小二乘法逼近解会产生不令人满意的结果。我们认为这是 ROME 和 MEMIT 的局限性，因为它们不能应用于不同的模型。</span> <span style="background-color: #ff666680">MEMIT 由于依赖多层矩阵计算而表现较差，并且对于较大模型，其可靠性和泛化性比 ROME 下降得更多。 IKE 的性能受到模型本身的上下文学习能力的影响。 OPT的结果比GPT-J的结果还要差，这可能归因于OPT本身的上下文学习能力。</span><span style="background-color: #5fb23680">此外，随着模型大小的增加，其泛化和局部性的性能都会下降。</span></p>
<p><strong>批量编辑</strong> 鉴于许多研究通常将更新限制为几十个事实或仅关注单个编辑案例，我们进行了进一步的批量编辑分析。然而，通常需要同时修改具有多个知识片段的模型。我们重点关注支持批量编辑的方法（FT、SERAC、MEND 和 MEMIT），并在图 3 中展示了它们的性能。值得注意的是，<span style="background-color: #ff666680">MEMIT 支持LLMs的大规模知识编辑，允许以最少的时间和内存进行数百甚至数千个同时编辑成本。其在可靠性和泛化方面的性能在最多 1000 次编辑时仍然保持稳健，但局部性在此级别下降。而 FT-L、SERAC、和MEND还支持批量编辑，它们需要大量内存来处理更多情况，超出了我们当前的能力。因此，我们将测试限制为 100 次编辑。 SERAC 可以完美地进行最多 100 次编辑的批量编辑。 MEND 和 FT-L 在批量编辑中的性能并不那么强，随着编辑数量的增加，模型的性能迅速下降。</span></p>
<p><img src="../imgs/$%7Bfiilename%7D/WVMGDVMU.png" alt="\&lt;img alt=&quot;&quot; data-attachment-key=&quot;WVMGDVMU&quot; data-annotation=&quot;%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2Flocal%2FiUeV0SQs%2Fitems%2F9227ERUC%22%2C%22annotationKey%22%3A%22UTUV86Q8%22%2C%22color%22%3A%22%23ffd400%22%2C%22pageLabel%22%3A%225%22%2C%22position%22%3A%7B%22pageIndex%22%3A4%2C%22rects%22%3A%5B%5B303%2C516.89%2C528%2C639.39%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2Flocal%2FiUeV0SQs%2Fitems%2FCU5SMQAC%22%5D%2C%22locator%22%3A%225%22%7D%7D&quot; width=&quot;375&quot; height=&quot;204&quot; src=&quot;attachments/WVMGDVMU.png&quot; ztype=&quot;zimage&quot;&gt;"></p>
<p><strong>顺序编辑</strong> 请注意，默认评估过程是更新单个模型知识，评估新模型，然后回滚更新，然后对每个测试点重复该过程。在实际场景中，模型在进行新的编辑时应保留先前的更改。因此，进行连续编辑的能力是模型编辑的一个重要特征（Huang et al., 2023）。我们评估了具有强大的单编辑性能的顺序编辑方法，并在图 4 中报告了结果。<span style="background-color: #ff666680">冻结模型参数的方法（如 SERAC 和 T-Patcher)通常在顺序编辑中表现出稳定的性能。然而，那些改变模型参数的人却很困难</span>。 <span style="background-color: #ff666680">ROME 在 n = 10 之前表现良好，然后在 n = 100 时下降。MEMIT 的性能也会在超过 100 次编辑后下降，但不如 ROME 大幅下降。同样，MEND 在 n = 1 时表现良好，但在 n = 10 时表现明显下降。随着编辑过程的继续，这些模型越来越偏离其原始状态，导致性能次优。</span></p>
<h2 id="5-Comprehensive-Study"><a href="#5-Comprehensive-Study" class="headerlink" title="5 Comprehensive Study"></a>5 Comprehensive Study</h2><p>考虑到上述几点，我们认为以前的评估指标可能无法充分评估模型编辑能力。因此，我们提出对可移植性、局部性和效率进行更全面的评估。</p>
<h3 id="5-1-Portability-Robust-Generalization"><a href="#5-1-Portability-Robust-Generalization" class="headerlink" title="5.1 Portability - Robust Generalization"></a>5.1 Portability - Robust Generalization</h3><p>几项研究使用通过反向翻译生成的样本来评估泛化性（De Cao 等人，2021）。然而，这些释义的句子通常只涉及微小的措辞变化，并不能反映实质性的事实修改。正如 Jacques Thibodeau (2022) 中所述，验证这些方法是否能够处理编辑对实际应用程序的影响至关重要。因此，我们引入了一种称为可移植性的新评估指标，以衡量模型编辑在将知识转移到相关内容方面的有效性，称为鲁棒泛化。因此我们考虑三个方面：（1）<strong>主语替换</strong>：由于大多数改写的句子保留了主语描述，但更多地改写了关系，我们通过替换来测试泛化能力问题中的主题带有别名或同义词。这测试模型是否可以将编辑的属性推广到同一主题的其他描述。 (2)<strong>反向关系</strong>：当编辑主体和关系的目标时，目标实体的属性也发生变化。我们通过过滤合适的关系（例如一对一）并询问相反的问题来检查目标实体是否也更新来测试模型处理此问题的能力。 （3）<strong>一跳</strong>：修改后的知识应该可以被编辑后的语言模型用于下游任务。例如，如果我们更改“瓦茨·汉弗莱 (Watts Humphrey) 就读哪所大学？”这个问题的答案。从“三一学院”到“密歇根大学”，当被问到“Watts Humphrey 在大学学习期间住在哪个城市？”时，模型应该回答“密歇根州的安娜堡”而不是“爱尔兰的都柏林”。因此，我们构建了一个推理数据集来评估编辑后模型使用编辑知识的能力。</p>
<pre><code> 我们将一个新部分 P (xe, ye) 合并到现有数据集 ZsRE 中，可移植性计算为应用于 P (xe, ye) 中的推理示例时编辑模型 (fθe) 的平均准确度：
</code></pre><p><img src="../imgs/$%7Bfiilename%7D/AKCEUPGK.png" alt="\&lt;img alt=&quot;&quot; data-attachment-key=&quot;AKCEUPGK&quot; data-annotation=&quot;%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2Flocal%2FiUeV0SQs%2Fitems%2F9227ERUC%22%2C%22annotationKey%22%3A%22J5ARE9LF%22%2C%22color%22%3A%22%23ffd400%22%2C%22pageLabel%22%3A%226%22%2C%22position%22%3A%7B%22pageIndex%22%3A5%2C%22rects%22%3A%5B%5B306.5%2C154.39%2C526.5%2C190.39%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2Flocal%2FiUeV0SQs%2Fitems%2FCU5SMQAC%22%5D%2C%22locator%22%3A%226%22%7D%7D&quot; width=&quot;367&quot; height=&quot;60&quot; src=&quot;attachments/AKCEUPGK.png&quot; ztype=&quot;zimage&quot;&gt;"></p>
<p><strong>数据集构建 </strong>对于一跳数据集<span style="background-color: #ff666680">，在原始编辑中，我们将主题 s 的答案从 o 更改为 o<em>。然后，我们提示模型生成链接的三元组 (o</em>, r<em>, o′</em>)。随后，GPT-4 根据这个三元组和 s 创建一个问题和答案。尤其，如果模型可以回答这个新问题，意味着它具有三元组 (o<em>, r</em>, o′<em>) 的预先存在的知识。我们通过要求模型从 o</em> 和 r<em> 预测 o’</em> 来过滤未知的三元组。如果成功，则推断该模型具有先验知识。最后，人类评估者验证三元组的准确性和问题的流畅性</span>。其他详细信息，例如我们使用的演示和数据集构建的其他部分，可以在附录 B 中找到。</p>
<p><strong>结果 </strong>我们根据新提出的评估指标和数据集进行实验，结果如表3所示。如表所示，<span style="background-color: #ff666680">当前模型编辑方法在可移植性方面的性能有些欠佳。尽管 SERAC 在之前的指标上显示出无可挑剔的结果，但在所有三个可移植性方面的准确度均低于 20%。 SERAC的瓶颈在于分类器的准确性和附加模型的能力。对于主题替换场景，包括SERAC、MEND、ROME和MEMIT，只能适应特定的主题实体表达，而不能泛化到主题实体的概念。然而，FT-L、IKE 和 T-patcher 在面对替换主题时表现出了出色的性能。</span>关于反向关系，我们的结果表明，<span style="background-color: #ff666680">当前的编辑方法主要编辑单向关系，IKE 是一个明显的例外，在 GPT-J 和 GPT-NEOX-20B 上都达到了 90% 以上。其他方法改变主体实体的属性，同时保持客体实体不受影响</span>。在一跳推理环境中，<span style="background-color: #ff666680">大多数编辑方法都难以将改变的知识转移到相关事实。</span>出乎意料的是，<span style="background-color: #5fb23680">ROME、MEMIT和IKE在可移植性方面表现出相对值得称赞的表现（超过50%）。他们不仅能够编辑原始案件，而且能够在某些方面修改与案件相关的事实</span>。综上所述，在我们的评估中，IKE 在三个场景中都表现出了相对较好的性能。然而，很明显，当前的模型编辑技术在管理编辑的后果方面继续面临挑战，即确保知识的变化在相关上下文中连贯一致地反映。事实上，这一领域需要在未来的研究中进一步调查和创新。</p>
<h3 id="5-2-Locality-Side-Effect-of-Model-Editing"><a href="#5-2-Locality-Side-Effect-of-Model-Editing" class="headerlink" title="5.2 Locality - Side Effect of Model Editing"></a>5.2 Locality - Side Effect of Model Editing</h3><p>在上一节中，COUNTERFACT 和 ZsRE 从以下方面评估模型编辑的局部性：COUNTERFACT 使用与目标知识相同分布的三元组，而 ZsRE 使用来自不同自然问题数据集的问题。值得注意的是，一些方法（例如 T-Patcher）在这两个数据集上表现出不同的性能。这凸显出模型编辑对语言模型的影响是多方面的，需要进行彻底、全面的评估才能充分理解其效果。为了彻底检查模型编辑的潜在副作用，我们提出了三个不同层面的评估：（1）其他关系：尽管Meng等人。 (2022)引入了本质的概念，但他们没有明确评价它。我们认为，已更新的主题的其他属性在编辑后应保持不变。 (2)分散邻里的注意力：HoelscherObermaier等人。 （2023a）发现，如果我们将编辑后的案例连接在其他不相关的输入之前，模型往往会受到编辑后的事实的影响，并继续产生与编辑后的案例一致的结果。 (3) 其他任务：基于 Skill Neuron 的断言（Wang 等人，2022），即大语言模型（LLM）中的前馈网络拥有特定于任务的知识能力，我们引入了一个新的挑战来评估模型编辑是否可能对性能产生负面影响关于其他任务。数据集构建的详细信息请参见附录 B.3。</p>
<p><img src="../imgs/$%7Bfiilename%7D/ND8H464M.png" alt="\&lt;img alt=&quot;&quot; data-attachment-key=&quot;ND8H464M&quot; data-annotation=&quot;%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2Flocal%2FiUeV0SQs%2Fitems%2F9227ERUC%22%2C%22annotationKey%22%3A%22DX678K73%22%2C%22color%22%3A%22%23ffd400%22%2C%22pageLabel%22%3A%227%22%2C%22position%22%3A%7B%22pageIndex%22%3A6%2C%22rects%22%3A%5B%5B297.5%2C600.89%2C529%2C776.89%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2Flocal%2FiUeV0SQs%2Fitems%2FCU5SMQAC%22%5D%2C%22locator%22%3A%227%22%7D%7D&quot; width=&quot;386&quot; height=&quot;293&quot; src=&quot;attachments/ND8H464M.png&quot; ztype=&quot;zimage&quot;&gt;"></p>
<p><strong>结果</strong> 表 4 列出了我们的结果。值得注意的是，当前的编辑方法在其他属性方面表现出色，表明它们仅修改目标特征而不影响其他属性。<span style="background-color: #5fb23680">然而，它们在 Distract-Neighbor 设置中通常表现不佳，</span>如与表 1 中的结果相比性能下降所反映的那样。IKE 是一个例外，它的性能保持相对稳定，因为它继承了以下事实：</p>
<p><img src="../imgs/$%7Bfiilename%7D/EQ2WM2DI.png" alt="\&lt;img alt=&quot;&quot; data-attachment-key=&quot;EQ2WM2DI&quot; data-annotation=&quot;%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2Flocal%2FiUeV0SQs%2Fitems%2F9227ERUC%22%2C%22annotationKey%22%3A%22S55AHZG9%22%2C%22color%22%3A%22%23ffd400%22%2C%22pageLabel%22%3A%228%22%2C%22position%22%3A%7B%22pageIndex%22%3A7%2C%22rects%22%3A%5B%5B62.727%2C578.708%2C296.591%2C783.254%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2Flocal%2FiUeV0SQs%2Fitems%2FCU5SMQAC%22%5D%2C%22locator%22%3A%228%22%7D%7D&quot; width=&quot;390&quot; height=&quot;341&quot; src=&quot;attachments/EQ2WM2DI.png&quot; ztype=&quot;zimage&quot;&gt;"></p>
<p>完全需要在输入之前连接编辑后的事实。对于常识推理任务，参数保留方法在很大程度上保持了其在其他任务上的性能。相反，改变参数的方法往往会对性能产生负面影响，MEMIT 除外。尽管参数发生了变化，MEMIT 在常识性任务中仍然保持着强劲的性能，展示了其值得称赞的局部性。</p>
<h3 id="5-3-Efficiency"><a href="#5-3-Efficiency" class="headerlink" title="5.3 Efficiency"></a>5.3 Efficiency</h3><p>模型编辑应最大限度地减少进行编辑所需的时间和内存，而不影响模型的性能。</p>
<p>时间分析表5说明了不同模型编辑技术从提供编辑案例到获得发布后编辑模型所需的时间。我们观察到，一旦超网络经过训练，KE 和 MEND 就会以相当快的速度执行编辑过程。同样，SERAC 还可以快速编辑知识，在经过训练的分类器和反事实模型的情况下，在大约 5 秒内完成该过程。然而，这些方法需要数小时至数天的额外训练和额外的数据集。在我们的实验中，在 ZsRE 数据集上训练 MEND 需要超过 7 个小时，在 3× V100 上训练 SERAC 需要超过 36 个小时。另一方面，ROME 和 MEMIT 需要预先计算维基文本的协方差统计数据。然而，这种计算非常耗时，可能需要数小时至数天才能完成。相比之下，其他方法（例如 KN、CaliNET 和 T-Patcher）可能更快，因为它们不需要任何预计算或预训练。然而，KN 和 CaliNET 在较大模型上的性能</p>
<p><img src="../imgs/$%7Bfiilename%7D/K9ABI2KM.png" alt="\&lt;img alt=&quot;&quot; data-attachment-key=&quot;K9ABI2KM&quot; data-annotation=&quot;%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2Flocal%2FiUeV0SQs%2Fitems%2F9227ERUC%22%2C%22annotationKey%22%3A%22GC2LNVEQ%22%2C%22color%22%3A%22%23ffd400%22%2C%22pageLabel%22%3A%228%22%2C%22position%22%3A%7B%22pageIndex%22%3A7%2C%22rects%22%3A%5B%5B302.045%2C605.981%2C532.5%2C781.89%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2Flocal%2FiUeV0SQs%2Fitems%2FCU5SMQAC%22%5D%2C%22locator%22%3A%228%22%7D%7D&quot; width=&quot;384&quot; height=&quot;293&quot; src=&quot;attachments/K9ABI2KM.png&quot; ztype=&quot;zimage&quot;&gt;"></p>
<p>不能令人满意，T-Patcher 是最慢的，因为需要针对每个相应的错误进行单独的神经元训练。考虑到时间方面，需要一种更加省时的模型编辑方法。</p>
<p><strong>内存分析</strong> 图 5 显示了每种模型编辑方法的内存 VRAM 使用情况。从该图中，我们观察到大多数方法消耗的内存量相似，但 MEND 除外，它需要超过 60GB 的内存用于训练。引入额外训练的方法（例如 MEND 和 SERAC）会导致额外的计算开销，从而显着增加内存消耗。</p>
<h2 id="6-Relationship-with-Relevant-Works"><a href="#6-Relationship-with-Relevant-Works" class="headerlink" title="6 Relationship with Relevant Works"></a>6 Relationship with Relevant Works</h2><h3 id="6-1Knowledge-in-LLMs"><a href="#6-1Knowledge-in-LLMs" class="headerlink" title="6.1Knowledge in LLMs"></a>6.1Knowledge in LLMs</h3><p>多种模型编辑方法旨在了解 PLM 中存储的知识如何精确且直接地改变模型参数。现有工作研究了 PLM 如何存储知识的原则（Geva 等人，2021、2022；Haviv 等人，2023；Hao 等人，2021；Hernandez 等人，2023；Yao 等人， 2023；Cao et al., 2023；Lamparth and Reuel, 2023；Cheng et al., 2023；Li et al., 2023b；Chen et al., 2023；Ju and Zhang, 2023），这些都有助于模型编辑过程。此外，一些模型编辑技术与知识增强相似（Zhang et al., 2019；Lewis et al., 2020；Zhang et al., 2022b；Yasunaga et al., 2021；Yao et al., 2022；Pan et al. ., 2023）方法，因为更新模型的知识也可以被视为将知识灌输到模型中。</p>
<h3 id="6-2Lifelong-Learning-and-Unlearning"><a href="#6-2Lifelong-Learning-and-Unlearning" class="headerlink" title="6.2Lifelong Learning and Unlearning"></a>6.2Lifelong Learning and Unlearning</h3><p>模型编辑包括终身学习和忘却，允许自适应地添加、修改和删除知识。持续学习（Biesialska et al., 2020）可以提高模型跨任务和领域的适应性，已在 PLM 中的模型编辑中显示出有效性（Zhu et al., 2020）。此外，模型忘记敏感知识并与机器遗忘概念保持一致至关重要（Hase 等人，2023；Wu 等人，2022；Tarun 等人，2021；Gandikota 等人，2023）。</p>
<h3 id="6-3Security-and-Privacy-for-LLMs"><a href="#6-3Security-and-Privacy-for-LLMs" class="headerlink" title="6.3Security and Privacy for LLMs"></a>6.3Security and Privacy for LLMs</h3><p>过去的研究（Carlini 等人，2020；Shen 等人，2023）表明，LLMs可以根据某些提示生成不可靠或个人的样本。删除大型语言模型 (LLM) 中存储的潜在有害信息和隐私信息的任务对于增强基于 LLM 的应用程序的隐私和安全性至关重要（Sun 等人，2023）。模型编辑可以抑制有害语言的生成（Geva et al., 2022；Hu et al., 2023），可以帮助解决这些问题。</p>
<h2 id="7-Conclusion"><a href="#7-Conclusion" class="headerlink" title="7 Conclusion"></a>7 Conclusion</h2><p>我们系统地分析了编辑大语言模型（LLM）的方法。我们的目标是通过检查现有编辑技术的特征、优势和局限性，帮助研究人员更好地理解现有编辑技术。我们的分析显示了很大的改进空间，特别是在可移植性、局部性和效率方面。改进的LLMs编辑可以帮助他们更好地适应用户不断变化的需求和价值观。我们希望我们的工作能够促进开放问题和进一步研究的进展。</p>

    </div>

    
    
    
      
  <div class="popular-posts-header">Custom header, leave empty to use the default one</div>
  <ul class="popular-posts">
    <li class="popular-posts-item">
      <div class="popular-posts-title"><a href="\2023\11\02\duo-yu-yan-mo-xing-zhong-shi-shi-zhi-shi-de-kua-yu-yan-yi-zhi-xing\" rel="bookmark">多语言模型中事实知识的跨语言一致性</a></div>
    </li>
    <li class="popular-posts-item">
      <div class="popular-posts-title"><a href="\2023\11\06\wang-diao-ni-xiang-wang-diao-de-dong-xi-llms-de-gao-xiao-wang-que\" rel="bookmark">忘掉你想忘掉的东西：LLMs的高效忘却</a></div>
    </li>
    <li class="popular-posts-item">
      <div class="popular-posts-title"><a href="\2023\11\02\qing-jing-xue-xi-chuang-jian-ren-wu-xiang-liang\" rel="bookmark">情境学习创建任务向量</a></div>
    </li>
    <li class="popular-posts-item">
      <div class="popular-posts-title"><a href="\2023\11\02\wo-men-ke-yi-tong-guo-qing-jing-xue-xi-lai-bian-ji-shi-shi-zhi-shi-ma\" rel="bookmark">我们可以通过情景学习来编辑事实知识吗？</a></div>
    </li>
    <li class="popular-posts-item">
      <div class="popular-posts-title"><a href="\2023\11\02\wo-men-ke-yi-bian-ji-duo-mo-tai-da-xing-yu-yan-mo-xing-ma\" rel="bookmark">我们可以编辑多模态大型语言模型吗？</a></div>
    </li>
  </ul>

        <div class="reward-container">
  <div></div>
  <button onclick="var qr = document.getElementById('qr'); qr.style.display = (qr.style.display === 'none') ? 'block' : 'none';">
    打赏
  </button>
  <div id="qr" style="display: none;">

  </div>
</div>


      <footer class="post-footer">
          
          <div class="post-tags">
              <a href="/tags/EMNLP2023/" rel="tag"><i class="fa fa-tag"></i> EMNLP2023</a>
          </div>

        
  <div class="post-widgets">
    <div class="wp_rating">
      <div id="wpac-rating"></div>
    </div>
  </div>


        
    <div class="post-nav">
      <div class="post-nav-item"></div>
      <div class="post-nav-item">
    <a href="/2023/11/02/suan-fa-bi-ji-v2.0/" rel="next" title="算法笔记V2.0">
      算法笔记V2.0 <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          
    <div class="comments" id="gitalk-container"></div>

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#%E7%BC%96%E8%BE%91%E5%A4%A7%E5%9E%8B%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%EF%BC%9A%E9%97%AE%E9%A2%98%E3%80%81%E6%96%B9%E6%B3%95%E5%92%8C%E6%9C%BA%E9%81%87"><span class="nav-number">1.</span> <span class="nav-text">编辑大型语言模型：问题、方法和机遇</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Abstruct"><span class="nav-number">1.1.</span> <span class="nav-text">Abstruct</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#1-Introduction"><span class="nav-number">2.</span> <span class="nav-text">1 Introduction</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#2-Problem-Definition"><span class="nav-number">2.1.</span> <span class="nav-text">2 Problem Definition</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#3-Current-Methods"><span class="nav-number">2.1.1.</span> <span class="nav-text">3 Current Methods</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-1-Methods-for-Preserving-LLMs-Parameters"><span class="nav-number">2.1.2.</span> <span class="nav-text">3.1 Methods for Preserving LLMs Parameters</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-2-Methods-for-Modifying-LLMs-Paramete"><span class="nav-number">2.1.3.</span> <span class="nav-text">3.2 Methods for Modifying LLMs Paramete</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#4-Preliminary-Experiments"><span class="nav-number">2.2.</span> <span class="nav-text">4 Preliminary Experiments</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#4-1-Experiment-Setting"><span class="nav-number">2.2.1.</span> <span class="nav-text">4.1 Experiment Setting</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-2-Experiment-Results"><span class="nav-number">2.2.2.</span> <span class="nav-text">4.2 Experiment Results</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#5-Comprehensive-Study"><span class="nav-number">2.3.</span> <span class="nav-text">5 Comprehensive Study</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#5-1-Portability-Robust-Generalization"><span class="nav-number">2.3.1.</span> <span class="nav-text">5.1 Portability - Robust Generalization</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-2-Locality-Side-Effect-of-Model-Editing"><span class="nav-number">2.3.2.</span> <span class="nav-text">5.2 Locality - Side Effect of Model Editing</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-3-Efficiency"><span class="nav-number">2.3.3.</span> <span class="nav-text">5.3 Efficiency</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#6-Relationship-with-Relevant-Works"><span class="nav-number">2.4.</span> <span class="nav-text">6 Relationship with Relevant Works</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#6-1Knowledge-in-LLMs"><span class="nav-number">2.4.1.</span> <span class="nav-text">6.1Knowledge in LLMs</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#6-2Lifelong-Learning-and-Unlearning"><span class="nav-number">2.4.2.</span> <span class="nav-text">6.2Lifelong Learning and Unlearning</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#6-3Security-and-Privacy-for-LLMs"><span class="nav-number">2.4.3.</span> <span class="nav-text">6.3Security and Privacy for LLMs</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#7-Conclusion"><span class="nav-number">2.5.</span> <span class="nav-text">7 Conclusion</span></a></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="院龙"
      src="/images/avatar.png">
  <p class="site-author-name" itemprop="name">院龙</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">61</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">11</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">14</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="sidebar-button motion-element"><i class="fa fa-comment"></i>
    Chat
  </a>
  </div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/yuanllong" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;yuanllong" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:wtirtyzxc@gmail.com" title="E-Mail → mailto:wtirtyzxc@gmail.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2024</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">院龙</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a> 强力驱动
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>

<script src="/js/bookmark.js"></script>




  



  <script>
  if (CONFIG.page.isPost) {
    wpac_init = window.wpac_init || [];
    wpac_init.push({
      widget: 'Rating',
      id    : ,
      el    : 'wpac-rating',
      color : 'fc6423'
    });
    (function() {
      if ('WIDGETPACK_LOADED' in window) return;
      WIDGETPACK_LOADED = true;
      var mc = document.createElement('script');
      mc.type = 'text/javascript';
      mc.async = true;
      mc.src = '//embed.widgetpack.com/widget.js';
      var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(mc, s.nextSibling);
    })();
  }
  </script>

  
<script src="//cdn.jsdelivr.net/npm/algoliasearch@4/dist/algoliasearch-lite.umd.js"></script>
<script src="//cdn.jsdelivr.net/npm/instantsearch.js@4/dist/instantsearch.production.min.js"></script>
<script src="/js/algolia-search.js"></script>














  

  

  

<link rel="stylesheet" href="//cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.css">

<script>
NexT.utils.loadComments(document.querySelector('#gitalk-container'), () => {
  NexT.utils.getScript('//cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.js', () => {
    var gitalk = new Gitalk({
      clientID    : '',
      clientSecret: '',
      repo        : '',
      owner       : '',
      admin       : [''],
      id          : 'edc472d0f281da7fea848ceea63518fb',
        language: '',
      distractionFreeMode: true
    });
    gitalk.render('gitalk-container');
  }, window.Gitalk);
});
</script>

<script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","pluginJsPath":"lib/","pluginModelPath":"assets/","tagMode":false,"log":false,"model":{"jsonPath":"live2d-widget-model-chitose"},"display":{"position":"right","width":150,"height":300},"mobile":{"show":true}});</script></body>
</html>
